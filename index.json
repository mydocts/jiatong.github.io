{"categories":[{"link":"/categories/code-highlighting/","name":"Code-Highlighting","slug":"Code-Highlighting"},{"link":"/categories/github/","name":"Github","slug":"Github"},{"link":"/categories/image-rendering/","name":"Image-Rendering","slug":"Image-Rendering"},{"link":"/categories/test/","name":"Test","slug":"Test"},{"link":"/categories/%E5%88%86%E7%B1%BB1/","name":"分类1","slug":"分类1"},{"link":"/categories/%E5%88%86%E7%B1%BB2/","name":"分类2","slug":"分类2"},{"link":"/categories/%E5%8D%9A%E5%AE%A2/","name":"博客","slug":"博客"}],"pages":[],"posts":[{"link":"/posts/grid/","text":"GRID —— 一个易于使用、灵活高效的开源平台，用于快速原型化基于 SID 的 GR 方法。\n1.前言 GR 利用生成模型的进步，实现两种主要方式：\n直接生成用户感兴趣物品的文本内容 或者从预训练模型中提取语义表示（semantic representations），以编码开放世界的知识，本文中的模型是后者。 2.语义ID（Semantic IDs） 2.1语义ID是什么 促成 GR（Generative Recommender） 成功的关键因素之一是 语义 ID（SID, Semantic ID）。它将连续的语义表示（例如来自大型语言模型的向量表示）转换为离散的 ID 序列。 2.2传统ID与语义ID对比 传统的ID特征是为用户/物品分配独一无二，但不提供信息的ID，这些ID被映射到嵌入向量中，主要用于捕获协同过滤信号 语义ID将连续的语义表示(例如，来自大模型语言)转换为离散的ID序列；这样能够同时利用预训练基础模型中编码的语义知识以及用户-物品交互历史中的编码的协同信号。 当两个物品的 SID 有重叠时，这种重叠在原理上反映了它们的语义相似性；同时，下一步的监督学习（next-item supervision）使模型能够学习跨 SID 的协同过滤信号 2.3怎么得到语义ID 首先通过模态编码器（modality encoder，例如大型语言模型 LLMs）提取语义表示； 然后使用量化器（quantizer）将连续嵌入压缩为离散稀疏 ID。 常见的基于量化的标记器包括： RQ-VAE ，RVQ ，Residual K-Means 。 2.4如何将语义ID用于生成式推荐 TIGER 首次将 Transformer 应用于推荐任务， 用于预测物品的语义 ID（SID），并借鉴了文档检索（document retrieval）中生成式推荐的思想 。后续研究在多个方向上改进了 SID 的训练：\n通过协同过滤信号增强学习 ； 引入分布式平衡机制（distributional balancing）； 使用更强大的大型语言模型或多模态编码器（multimodal encoders）。 3.GRID架构 我们考虑一个用户集合 $\\mathcal{U}$，每个用户与一个物品集合 $\\mathcal{I}$ 中的项目交互。每个物品 $i \\in \\mathcal{I}$ 都有相关的语义特征 $f_i$，包括但不限于文本和图像。\n每个用户 $u \\in \\mathcal{U}$ 都有一个交互序列，其长度为 $L_u$，记作 $S_u = [i_u^1, i_u^2, \\dots, i_u^{L_u}]$。\n一般性地，模态编码器（modality encoder，例如 LLM 或 VLM）将特征 $f_i$ 转换为 $d$ 维的表示 $h_i \\in \\mathbb{R}^d$。\n生成式推荐（GR）的目标是解决序列推荐问题：给定一个用户的交互序列 $S_u$，生成一个候选物品集合，使得这些物品是用户下一步最可能交互的内容（即 $i_u^{L_u+1}$）。\n3.1架构：先标记化（Tokenization）后生成（Generation）** GRID 将基于语义ID的生成式推荐划分为两个阶段：\n标记化阶段（Tokenization）\n将物品特征映射成嵌入向量（即 $h_i$），再量化为语义ID（SID）。\n生成阶段（Generation）\n使用所有物品的SID，探索不同的生成模型结构（例如 Transformer-based 模型），以生成下一个物品的SID。\nGRID 在每个阶段都提供了灵活的可配置实现。\n3.2语义ID标记化（Semantic ID Tokenization） SID 标记化首先通过预训练的模态编码器 $E(\\cdot)$ 将物品的语义特征映射到嵌入空间，然后通过分层结构将这些嵌入量化为稀疏的语义ID。\n这种层次化组织的SID可以通过不同层级的前缀（prefix）灵活控制粒度。\n形式化地，给定 $h_i$，量化器（Tokenizer）$Tokenizer(\\cdot): \\mathbb{R}^d \\to {0, 1, \\dots, V}^L$ 将嵌入 $h_i$ 映射为ID序列：\n$SID_i=Tokenizer(h_i)=[SID_i^1,SID_i^2,…,SID_i^L]$ 其中 $V$ 表示每层的词汇大小，$L$ 表示层数。\nGRID 提供了可插拔的模块化设计，方便用户自定义模型或直接使用 HuggingFace 上的模型。\n目前支持三种主要量化器：\nResidual Mini-Batch K-Means (RK-Means) Residual Vector Quantization (R-VQ) Residual Quantized Variational Autoencoder (RQ-VAE) 3.3下一项生成（Next Item Generation） 当所有物品都生成了SID后，针对每个用户序列，GR 框架会使用序列模型生成用户最可能交互的候选SID。\n在 GRID 中，支持多种生成结构：\nEncoder-Decoder 模型； Decoder-only（仅解码器） 模型； 可灵活配置的 Transformer 层数、头数、MoE 专家结构等。 用户可以导入 HuggingFace 上公开的架构或自定义自己的结构。\n默认训练目标是基于 下一token预测（Next-token prediction） 的生成任务，并采用 **滑动窗口增强（sliding window augmentation）。 推理阶段使用带 KV-cache 的 beam search，支持调节 beam width，确保生成的SID是合法的。\nGRID 还内置了多种技巧，比如：\n用户token机制（user token）； 去重（de-duplication）； 避免SID冲突。 4.GRID 实验结果总结 总体设置 数据集：Amazon 5-core（Beauty, Sports, Toys） 评估方式：最后一个交互用于测试，倒数第二个用于验证，其余用于训练。 模型：Flan-T5-Large / XL / XXL 提取语义嵌入。 标记化算法：RK-Means、R-VQ、RQ-VAE。 生成模型：采用 Transformer 架构（共8层，encoder 4层 + decoder 4层）。 评估指标：Recall@K 与 NDCG@K（K = 5, 10）。 4.1 语义ID标记化（Semantic ID Tokenization） 标记化算法（Tokenizer Algorithm） 比较 RQ-VAE、RK-Means、R-VQ 三者。 结果： -**RQ-VAE 虽常被采用，但需同时训练自编码器与量化器，复杂度高。 -RK-Means 与 R-VQ 表现更优，即使在计算量更低的情况下也能获得更好推荐结果。 - 说明复杂的 RQ-VAE 性价比不高。 语义编码器规模（Semantic Encoder Size）\n测试 Flan-T5-Large (780M) → XL (3B) → XXL (11B)。 结果： 模型参数扩大14倍，性能提升却非常有限。 结论： 当前 GR with SID 体系未充分利用更大LLM的语义知识，性能主要受其它模块影响。 标记器层数与维度（SID Tokenizer Dimension） 调整残差层数 L 与每层token数量 W。默认配置 (L, W) = (3, 256)。 结果： 默认配置效果最佳； 层数过多会降低性能——更多层虽可传递更多语义信息，但会造成SID序列不稳定。 存在“语义信息量 vs. 序列可学习性”的权衡。 4.2 生成式推荐（Generative Recommendation） 实验从五个角度研究了 GR with SID 的生成阶段设计。\n用户token数量（User Tokens） 每个用户的SID序列前可加入一个用户token。 结果： 更大的用户词汇表并不提升性能； 完全去掉用户token（即不加个性化标识）反而效果最佳。 结论： 当前 GR with SID 框架中的用户token设计未实现个性化目标。 模型架构：Encoder-Decoder vs. Decoder-only 对比 Transformer 编解码器与仅解码器架构。 结果： Decoder-only 明显性能更差。 Encoder-Decoder 模型更能捕捉用户长期行为的上下文信息。 说明 Encoder 层的上下文建模对生成式推荐至关重要。 数据增强（Data Augmentation） 使用滑动窗口（sliding window）生成多样化序列样本。 结果： 适当的数据增强显著提高泛化能力、减少过拟合； 模型能更好预测多样化和稀疏样本中的下一交互项。 结论： 数据增强是提升 GR 性能的关键手段。 SID去重（De-duplication） 比较两种去重策略： TIGER策略： 在SID末尾加数字； 随机选择策略： 当碰撞时随机选一个。 结果： 两种方式效果接近，TIGER略好但计算代价更高。 TIGER方法需全局SID分布信息，不适合大规模数据。 结论： 简单随机去重在实际场景更高效。 束搜索（Beam Search）策略 对比 约束式 与 非约束式 beam search。 结果： 性能差距极小； 非约束式搜索更快、计算更省； 说明SID生成任务本身的模式约束足以保证生成质量，无需显式约束。 5. 结论（Conclusion） 主要发现： 过去被认为“关键”的组件（如复杂量化器、RQ-VAE、自定义LLM、大量用户token）其实可以被更简单、高效的设计替代； 相反，一些被忽视的因素（如 Encoder-Decoder结构 与 数据增强）却是性能提升的关键； 这些结果为理解 GR with SID 的真正性能驱动因素提供了新的视角。 GRID 的价值： 通过开源的、统一的实验平台，系统揭示了 GR with SID 的核心设计权衡； GRID 框架 能作为标准基准（benchmark）和实验工具，加速后续研究与验证。 ","title":"GRID"},{"link":"/posts/custom-url/","text":" KL 散度（Kullback–Leibler Divergence） $$ KL(P | Q) = \\sum_{x \\in X} P(x) \\log \\frac{P(x)}{Q(x)} $$\n含义 P 分布相对于 Q 分布的相似程度。\n性质 KL 散度的值 大于等于 0。 P 和 Q 越相似，KL 散度越接近 0。 如果 P 和 Q 分布完全一致，则 KL 散度 = 0。 注意：\n$$ KL(P | Q) \\neq KL(Q | P) $$ KL 散度大于等于 0 的直观理解 直观理解：KL 散度是一个非负数，因为我们在比较两个分布时， 只有在完全一致时，它们之间的“差异”才为 0。\n$$ KL(P | Q) = \\sum_{x \\in X} P(x) \\log \\frac{P(x)}{Q(x)} $$\n示例 变量 P(x) Q(x) x = 0 0.2 0.8 x = 1 0.8 0.2 计算过程 $$ KL(P | Q) = 0.2 \\times \\log \\frac{0.2}{0.8} + 0.8 \\times \\log \\frac{0.8}{0.2} $$\n直观理解 KL 散度衡量 P 分布相对于 Q 分布的差异程度。 由于对数函数的凸性，KL 散度始终 大于等于 0。 当 P 和 Q 分布完全一致时，KL 散度 等于 0。 若 P 与 Q 差异越大，KL 散度值越大。 Bradley–Terry 模型（成对比较模型） 基本思想 用于描述多个选手（或元素）之间的成对胜负概率。\n假设每个元素 $i$ 都有一个“实力参数” $\\alpha_i \u0026gt; 0$。\n定义： $$ P(i \u0026gt; j) = \\frac{\\alpha_i}{\\alpha_i + \\alpha_j} $$ 其中：\n$\\alpha_i$：第 ( i ) 个元素的真实实力； $P(i \u0026gt; j)$：第 $i$ 个元素战胜第 $j$ 个元素的概率。 对数似然函数（Maximum Likelihood Estimation） 模型给出每次对战的概率：\n$$P(i\u0026gt;j)= \\frac{\\alpha_i}{\\alpha_i + \\alpha_j}$$​​\n我们可以计算出“在当前参数 $\\alpha$下，所有比赛结果同时发生的概率： $$ L(\\boldsymbol{\\alpha}) = \\prod_{\\text{所有比赛}} P(\\text{胜方} \u0026gt; \\text{负方}) $$ 这就是所谓的 似然函数（Likelihood Function）。\n它衡量了：\n给定参数 $\\alpha$，观测到这些比赛结果的可能性有多大”。\n根据观测数据构建似然函数并取对数：\n$$ \\ln L = 8 \\ln\\left(\\frac{\\alpha_A}{\\alpha_A + \\alpha_B}\\right)+ 4 \\ln\\left(\\frac{\\alpha_B}{\\alpha_A + \\alpha_B}\\right)+ 3 \\ln\\left(\\frac{\\alpha_A}{\\alpha_A + \\alpha_C}\\right)+ 5 \\ln\\left(\\frac{\\alpha_C}{\\alpha_A + \\alpha_C}\\right) $$\n通过最大化对数似然，可以求得各选手的相对实力参数。\n参数估计结果 $$ \\alpha_A = 1, \\quad \\alpha_B = \\frac{1}{2}, \\quad \\alpha_C = \\frac{5}{3} $$ 指定$\\alpha_A$ 的值，可的得到剩下两个的值\n推导新的对战概率 例如计算 ( P(B \u0026gt; C) )：\n$$ P(B \u0026gt; C) = \\frac{\\alpha_B}{\\alpha_B + \\alpha_C} = \\frac{1/2}{1/2 + 5/3} \\approx 0.23 $$\n直观理解：\n模型通过比较胜负记录估计出每个选手的“潜在实力”； 实力越大，战胜其他人的概率越高； 可推广用于比赛预测、排序、推荐系统等任务。 一般的 Loss 函数（Bradley–Terry 模型） 模型目标是最大化对数似然，对应的最小化损失函数（负对数似然）为：\n$$ Loss = - \\mathbb{E}_{(a_x, a_y) \\sim D} \\left[ \\ln \\frac{a_x}{a_x + a_y} \\right] $$\n强化学习中的比较建模（Pairwise Preference in RLHF） 在强化学习中：\n大模型的输入是 prompt，记作 $x$； 模型的输出（回答）是 response，记作 $y$； 回答 $y$ 的好坏（即“得分”或“实力”）由 Reward 模型 $r(x, y)$ 进行评估。 比较两个回答的优劣概率 对于同一个输入 $x$，有两个不同回答 $y_1$ 和 $y_2$。\n原始形式为： $$ P(y_1 \\succ y_2) = \\frac{r(x, y_1)}{r(x, y_1) + r(x, y_2)} $$\n由于 $r(x, y)$ 可能返回负数，因此引入指数函数，使概率始终为正：\n$$ P(y_1 \\succ y_2) = \\frac{\\exp(r(x, y_1))}{\\exp(r(x, y_1)) + \\exp(r(x, y_2))} $$\n说明 $r(x, y)$：Reward 模型给出的得分（越大表示回答越好）； $P(y_1 \\succ y_2)$：模型预测回答 $y_1$ 优于 $y_2$ 的概率； 该形式本质上是 Bradley–Terry 模型 在强化学习（RLHF）中的应用。 偏好概率建模 对于同一输入 $x$，Reward 模型给出两个回答 $y_w$（获胜回答）和 $y_l$（失败回答）的得分： $$ P(y_w \\succ y_l) = \\frac{\\exp(r(x, y_w))}{\\exp(r(x, y_w)) + \\exp(r(x, y_l))} $$\nSigmoid 形式表示 Sigmoid 函数定义为： $$ \\sigma(x) = \\frac{1}{1 + \\exp(-x)} $$\n损失函数（Loss Function） Reward 模型的目标是最小化负对数似然（Negative Log-Likelihood）：\n$$ Loss = -\\mathbb{E}_{(x, y_w, y_l) \\sim D} \\left[ \\ln \\frac{\\exp(r(x, y_w))}{\\exp(r(x, y_w)) + \\exp(r(x, y_l))} \\right] $$\n等价地：\n$$ Loss = - \\mathbb{E}_{(x, y_w, y_l) \\sim D} \\left[ \\ln \\frac{1}{1 + \\exp(r(x, y_l) - r(x, y_w))} \\right] $$\n最终可写为简洁的 Sigmoid 形式： $$ Loss = - \\mathbb{E}_{(x, y_w, y_l) \\sim D} \\left[ \\ln \\sigma(r(x, y_w) - r(x, y_l)) \\right] $$\nDPO 的训练目标（Direct Preference Optimization） 基本定义 奖励函数：$r(x, y)$，其中 $x$ 为 prompt，$y$ 为 response 基准模型（reference model）：$\\pi_{\\text{ref}}(y|x)$ 训练模型（policy model）：$\\pi(y|x)$ DPO 的目标是在获得高奖励的同时，使新模型不偏离参考模型：\n$$ \\max_{\\pi} \\; \\mathbb{E}_{x\\sim D, y\\sim \\pi(y|x)}[r(x,y)] -\\beta \\mathbb{D}_{\\mathrm{KL}}\\big(\\pi(y|x)|\\pi_{\\mathrm{ref}}(y|x)\\big) $$\n第一项：希望得到尽可能多的奖励； 第二项：限制新模型与基准模型的分布差距； $\\beta$：超参数，用于平衡两者。 目标函数的等价形式推导 将 KL 散度展开：\n$$ \\max_{\\pi} \\mathbb{E}_{x, y \\sim \\pi} [r(x, y)] - \\beta , \\mathbb{E}_{x, y \\sim \\pi} \\left[\\log \\frac{\\pi(y|x)}{\\pi_{\\text{ref}}(y|x)} \\right] $$\n等价地（加负号取最小值且两项同时除以$\\beta$）：\n$$ \\min_{\\pi} \\mathbb{E}_{x, y \\sim \\pi} \\left[\\log \\frac{\\pi(y|x)}{\\pi_{\\text{ref}}(y|x)} - \\frac{1}{\\beta} r(x, y)\\right] $$\n将减法项写成对数形式\n$$ = \\min_{\\pi} ; \\mathbb{E}_{x, y \\sim \\pi} \\left[ \\log \\frac{\\pi(y|x)}{\\pi_{\\text{ref}}(y|x)} - \\log \\exp!\\left(\\frac{1}{\\beta} r(x, y)\\right) \\right] $$ 合并为单个对数项\n$$ = \\min_{\\pi} ; \\mathbb{E}_{x, y \\sim \\pi} \\left[ \\log \\frac{\\pi(y|x)} {\\pi_{\\text{ref}}(y|x) \\exp!\\left(\\frac{1}{\\beta} r(x, y)\\right)} \\right] $$\n引入归一化常数 $Z(x)$\n$$ = \\min_{\\pi} ; \\mathbb{E}_{x, y \\sim \\pi} \\left[ \\log \\frac{\\pi(y|x)} {\\pi_{\\text{ref}}(y|x) \\exp!\\left(\\frac{1}{\\beta} r(x, y)\\right) \\frac{1}{Z(x)} Z(x)} \\right] $$\n拆出常数项\n$$ =\\min_{\\pi} ; \\mathbb{E}_{x, y \\sim \\pi} \\left[ \\log \\frac{\\pi(y|x)}{\\frac{1}{Z(x)} \\pi_{\\text{ref}}(y|x) \\exp!\\left(\\frac{1}{\\beta} r(x, y)\\right)} -\\log Z(x) \\right] $$\n求解最优策略分布 定义：\n$$ \\pi^*(y|x) = \\frac{1}{Z(x)} , \\pi_{\\text{ref}}(y|x) \\exp!\\left(\\frac{1}{\\beta} r(x, y)\\right) $$\n其中：\n$$ Z(x) = \\sum_y \\pi_{\\text{ref}}(y|x) \\exp!\\left(\\frac{1}{\\beta} r(x, y)\\right) $$\n表示归一化常数，是我们自己定义的，这么定义的原因是为了能和后面的式子消去，简化式子。\n解释：最优策略 $\\pi^*(y|x)$ 是在参考模型分布上，通过奖励函数加权后的归一化分布。\n代入 $\\pi^*(y|x)$，目标可写为：\n$$ \\min_{\\pi} \\mathbb{E}_{x \\sim D} \\left[ D_{KL}(\\pi(y|x) | \\pi^*(y|x)) \\right] $$\n因此最优时： $$ \\pi(y|x) = \\pi^*(y|x) = \\frac{1}{Z(x)} \\pi_{\\text{ref}}(y|x) \\exp\\left(\\frac{1}{\\beta} r(x, y)\\right) $$\n奖励函数反求形式 由上式反推奖励： 由 DPO 的最优策略定义：\n$$ \\pi^*(y|x) = \\frac{1}{Z(x)} \\pi_{\\text{ref}}(y|x) \\exp \\left(\\frac{1}{\\beta} r(x, y)\\right) $$\n可得：\n$$ \\exp\\left(\\frac{1}{\\beta} r(x, y)\\right) = \\frac{\\pi(y|x)}{\\pi_{\\text{ref}}(y|x)} Z(x) $$\n两边取对数并乘以 $\\beta$，得到：\n$$ r(x, y) = \\beta \\ln!\\left( \\frac{\\pi(y|x)}{\\pi_{\\text{ref}}(y|x)} Z(x) \\right) $$ 即：\n$$ r(x, y) = \\beta \\ln \\frac{\\pi(y|x)}{\\pi_{\\text{ref}}(y|x)} + \\beta \\ln Z(x) $$\n解释：\n奖励函数可以理解为模型输出相对参考模型的“对数优势”。 它衡量了当前模型相对参考模型在特定回答上的改进幅度\n与偏好建模的联系 在成对偏好比较中，假设有更优回答 $y_w$ 与较差回答 $y_l$，则：\n$$ -\\ln \\sigma(r(x, y_w) - r(x, y_l)) = - \\ln \\sigma!\\left( \\beta \\ln \\frac{\\pi(y_w|x)}{\\pi_{\\text{ref}}(y_w|x)} - \\beta \\ln \\frac{\\pi(y_l|x)}{\\pi_{\\text{ref}}(y_l|x)} \\right) $$\n这表明 DPO 的优化目标实际上是将 Reward 模型的比较学习 转换为对 策略分布比值 的直接优化。\n总结 DPO的核心洞察在于原始强化学习问题存在解析最优解，表明最优策略与奖励函数存在一一映射关系。DPO将此关系反解后代入Bradley-Terry偏好模型，将对奖励函数的似然最大化，等价地转化为直接对策略的似然最大化。因此，优化DPO损失函数即是在直接寻找那个能同时最大化人类偏好概率且满足最优解形式的策略，避免了先用偏好数据拟合奖励模型再进行强化学习过程寻找最优策略\n","title":"LLM-RL-DPO"},{"link":"/posts/custom-url/","text":" 基础概念I 如果你不熟悉强化学习，学习ppo了解这些基础知识就足够了 Action Space: 可选择的动作，比如 {left, up, right}\nPolicy: 策略函数，输入 State，输出 Action 的概率分布。一般用 π 表示。\n$\\pi(left|s_t) = 0.1$,在状态$s_t$下，采取的动作为left的概率为0.1 $\\pi(up|s_t) = 0.2$ , $\\pi(right|s_t) = 0.7$, 这个状态下的 所有动作 概率之和 为1 Trajectory: 轨迹，用$τ$表示，一连串状态和动作的序列。又称Episode, Rollout。 ${s0,a0,s1,a1,…}$\n$s_{t+1}=f(st,at)$ 确定状态转移\n$s_{t+1}=P(⋅|st,at)$随机状态转移\nReturn: 回报，从当前时间点到游戏结束的 Reward 的累积和。\n期望：每个可能结果的概率与其结果值的乘积之和\n$\\mathbb{E}(x)_{x \\sim p(x)}=\\sum x \\cdotp(x)\\approx \\frac{1}{n} \\sum_{i=1}^n x, \\quad x \\sim p(x)$\n$\\mathbb{E}[f(\\tau)] \\approx \\frac{1}{N} \\sum_{n=1}^N f(\\tau^n)$\nNote 运用的是蒙特卡洛思想，从分布p(x)中随机采样n次求平均，n趋于无穷时样本平均会趋近于期望值/大数定律，期望可以用样本平均近似\n轨迹概率：一条轨迹 $\\tau = (s_0, a_0, s_1, a_1, \\dots, s_T, a_T)$的概率是： $P_\\theta(\\tau) = P(s_0, a_0, s_1, a_1, \\dots, s_T, a_T)$ 概率论基本公式： $P(x_1, x_2, \\dots, x_n) = \\prod_{i=1}^n P(x_i \\mid x_1, \\dots, x_{i-1})$ 把它用在轨迹上，就可以逐步展开： $P_\\theta(\\tau) = P(s_0) \\cdot P(a_0 \\mid s_0) \\cdot P(s_1 \\mid s_0, a_0) \\cdot P(a_1 \\mid s_1,a_0,s_0) \\cdot \\dots$ 在强化学习里，环境通常假设是 马尔可夫决策过程 (MDP)，即：\n下一状态只依赖于当前状态和动作： $P(s_{t+1} \\mid s_0, a_0, \\dots, s_t, a_t) = P(s_{t+1} \\mid s_t, a_t)$ 动作只依赖于当前状态： $P(a_t \\mid s_0, a_0, \\dots, s_t) = \\pi_\\theta(a_t \\mid s_t)$ 于是上面的展开可以简化为： $\\ P_\\theta(\\tau) = P(s_0) \\prod_{t=0}^T \\pi_\\theta(a_t \\mid s_t) , P(s_{t+1} \\mid s_t, a_t)$ $\\pi_\\theta(a_n^t | s_n^t)$：策略在状态 $s_n^t$下选择动作 $a_n^t$ 的概率（依赖参数 $\\theta$）。 $P(s_{n}^{t+1}|s_n^t, a_n^t)$：环境的转移概率（不依赖 $\\theta$）。 Important 目标： 训练一个Policy神经网络 $π_{\\theta}$ ,在所有的状态S下，给出相应的Action，得到的Return的期望最大 or 训练一个Policy神经网络 $\\pi_{\\theta}$ ,在所有Trajectory中，得到的Return的期望最大\nPPO原理-part1 我们将刚才的目标翻译成数学表达式，就是希望 $E(R(\\tau))_{\\tau \\sim P_\\theta(\\tau)} = \\sum_{\\tau} R(\\tau) P_\\theta(\\tau)$ 越大越好\n$P_\\theta(\\tau)$ :在策略$\\pi_\\theta$下，采样到轨迹 $\\tau$ 的概率。 $R(\\tau)$：轨迹 $\\tau$ 的总回报（Return）。 Note 为什么$R(\\tau)$不受$\\theta$影响？ $R(τ)=∑_{t=0}^{T−1}​γ^{t}r(s_t​,a_t​)$ 因为一旦一条轨迹被确定下来，这条轨迹的回报R是由每个状态下的每个动作的得分之和决定的，又因为越远的动作影响对当前状态影响越小，所以要乘上一个衰减因子$\\gamma$\n我们只能改变神经网络里的参数 θ ,不能改变Reward，所以对 θ求梯度\n$$ \\nabla E(R(\\tau))_{\\tau \\sim P_{\\theta}(\\tau)} $$\n$$ = \\nabla \\sum_{\\tau} R(\\tau) P_\\theta(\\tau)$$ $$= \\sum_{\\tau} R(\\tau) \\nabla P_\\theta(\\tau) \\ $$$$= \\sum_{\\tau} R(\\tau) \\nabla P_\\theta(\\tau) \\frac{P_\\theta(\\tau)}{P_\\theta(\\tau)} \\ $$ $$ = \\sum_{\\tau} P_\\theta(\\tau) R(\\tau) \\frac{\\nabla P_\\theta(\\tau)}{P_\\theta(\\tau)} $$ $$\\approx \\frac{1}{N} \\sum_{n=1}^N R(\\tau^n) \\frac{\\nabla P_\\theta(\\tau^n)}{P_\\theta(\\tau^n)} $$\nTip $\\tau^n$表示第n条轨迹（trajectory）;多次采样取平均\n$$ = \\frac{1}{N} \\sum_{n=1}^N R(\\tau^n) \\nabla \\log P_\\theta(\\tau^n) \\ \\ $$\nTip 因为$\\nabla \\log f(x) = \\frac{\\nabla f(x)}{f(x)}$\n$$= \\frac{1}{N} \\sum_{n=1}^N R(\\tau^n) \\nabla \\log \\prod_{t=1}^{T_n} P_\\theta(a_n^t | s_n^t)$$\nTip 这里是轨迹分解，因为对 $\\theta$ 求梯度只关注 $P_\\theta$ ,就是前面基础知识中的 $\\pi_\\theta$,环境转移概率不包含 $\\theta$ ,故当作常数，求梯度就没了\n$$=\\frac{1}{N} \\sum_{n=1}^N R(\\tau^n) \\sum_{t=1}^{T_n} \\nabla \\log P_\\theta(a_n^t | s_n^t)\\ = \\frac{1}{N} \\sum_{n=1}^N \\sum_{t=1}^{T_n} R(\\tau^n),\\nabla \\log P_\\theta(a_n^t | s_n^t)$$\n到这里，我们求出了对于所有可能的Trajectory，期望的最大梯度，用个梯度去更新神经网络参数，就是 Policy gradient梯度策略算法\nPPO原理-part2 $$ \\frac{1}{N} \\sum_{n=1}^N \\sum_{t=1}^{T_n} R(\\tau^n),\\nabla \\log P_\\theta(a_n^t | s_n^t) $$ 刚刚我们得到的这个式子，很明显有两点可以改进\n应该看在$S_n$状态下采取动作$a_n$之后的Reward，而不是整个Trajectory（轨迹）的Reward，因为动作action只能影响后面的 action是只会影响后面的动作，但是影响会逐步衰减 由此我们修改Reward公式为$$R(\\tau^n) \\to \\sum_{t\u0026rsquo;=t}^{T_n} \\gamma^{t\u0026rsquo;-t}r_{t\u0026rsquo;}^n = R_t^n$$ 表示从当前t时刻开始到最后的累积Reward， $\\gamma$ 是衰减因子，表示离当前动作越远，在当前状态下采取当前动作的Reward越小，核心就是想突出当前状态下采取当前动作对于Reward的影响.\n另一个值得注意的点是局势也会影响算法的稳定性，比如说在好的局势下，采取什么动作都会得分（顺风局当赢的感觉），但是这样就偏离我们的初衷——我们想知道在某种状态下采取哪些动作更好，模型需要明确区分哪个动作“更好”\n因此我们希望让相对好的action的Reward得分增加，相对差的action的Reward减少，这样会加快训练速度。\n由此我们减去一个Baseline\n$$ \\frac{1}{N} \\sum_{n=1}^N \\sum_{t=1}^{T_n} \\bigl(R_t^n - B(s_n^t)\\bigr) \\nabla \\log P_\\theta(a_n^t \\mid s_n^t)$$\n基础概念II 强化学习中有几个重要的定义，可以简化上述式子\n$\\textbf{Action-Value Function}(动作价值函数)$ $Q_\\theta(s,a) 在 state \\ \\ s 下，做出 Action \\ \\ a的回报的期望$\n$\\textbf{State-Value Function} (状态价值函数)$ $V_\\theta(s) 在 state \\ \\ s 下，回报的期望$。\n$\\textbf{Advantage Function}(优势函数)$ $A_\\theta(s,a) = Q_\\theta(s,a) - V_\\theta(s) \\quad$ $在 state \\ \\ s 下，做出 Action \\ \\ a，比其他动作能带来多少优势。$于是原式被转化为： $$\\frac{1}{N} \\sum_{n=1}^{N} \\sum_{t=1}^{T_n} A_\\theta(s_n^t, a_n^t) \\nabla \\log P_\\theta(a_n^t | s_n^t)$$ 现在我们得到了理论上表达式，但是如何表示实际采样的呢？\n一次采样：\n$Q_\\theta(s_t, a) = r_t + \\gamma \\cdot V_\\theta(s_{t+1})$\n$A_\\theta(s_t, a) = r_t + \\gamma \\cdot V_\\theta(s_{t+1}) - V_\\theta(s_t)$\n$V_\\theta(s_{t+1}) \\approx r_{t+1} + \\gamma \\cdot V_\\theta(s_{t+2})$\nTip 上面我们分别对动作价值函数和状态价值函数进行了一次采样，对于动作价值函数Q来说，采用的 动作是固定的，因此可以用等号来表示，对于状态价值函数，采取的动作a还没有固定，因此用约等于表示一次采样\n多次采样，虽然会增大方差，但是会减少偏差 $$ A_\\theta^1(s_t, a) = r_t + \\gamma V_\\theta(s_{t+1}) - V_\\theta(s_t) $$\n$$ A_\\theta^2(s_t, a) = r_t + \\gamma r_{t+1} + \\gamma^2 V_\\theta(s_{t+2}) - V_\\theta(s_t) $$\n$$ A_\\theta^3(s_t, a) = r_t + \\gamma r_{t+1} + \\gamma^2 r_{t+2} + \\gamma^3 V_\\theta(s_{t+3}) - V_\\theta(s_t) $$\n$$ \\vdots $$\n$$ A_\\theta^T(s_t, a) = r_t + \\gamma r_{t+1} + \\gamma^2 r_{t+2} + \\gamma^3 r_{t+3} + \\cdots + \\gamma^T r_T - V_\\theta(s_t) $$ 定义新函数，简化表示 $$ \\delta_t^V = r_t + \\gamma V_\\theta(s_{t+1}) - V_\\theta(s_t) $$\n$$ \\delta_{t+1}^V = r_{t+1} + \\gamma V_\\theta(s_{t+2}) - V_\\theta(s_{t+1}) $$\n$$ A_\\theta^1(s_t, a) = \\delta_t^V $$\n$$ A_\\theta^2(s_t, a) = \\delta_t^V + \\gamma \\delta_{t+1}^V $$\n$$ A_\\theta^3(s_t, a) = \\delta_t^V + \\gamma \\delta_{t+1}^V + \\gamma^2 \\delta_{t+2}^V $$\n$$ \\vdots $$ 所以算法（比如 GAE）会综合使用多个步长 k 的估计，构建出一个平衡版本（平衡方差和偏差）： 多次采样，并乘上对应衰减权重 $$ A_\\theta^{GAE}(s_t, a) = (1 - \\lambda)\\big(A_\\theta^1 + \\lambda A_\\theta^2 + \\lambda^2 A_\\theta^3 + \\cdots \\big) $$\n$$ \\lambda = 0.9: \\quad A_\\theta^{GAE} = 0.1 A_\\theta^1 + 0.09 A_\\theta^2 + 0.081 A_\\theta^3 + \\cdots $$\n$$ = (1 - \\lambda)\\big(\\delta_t^V + \\lambda(\\delta_t^V + \\gamma \\delta_{t+1}^V) + \\lambda^2(\\delta_t^V + \\gamma \\delta_{t+1}^V + \\gamma^2 \\delta_{t+2}^V) + \\cdots \\big) $$\n$$ = (1 - \\lambda)\\big(\\delta_t^V(1 + \\lambda + \\lambda^2 + \\cdots) + \\gamma \\delta_{t+1}^V (\\lambda + \\lambda^2 + \\cdots) + \\cdots \\big) $$\n$$ = (1 - \\lambda)\\big(\\delta_t^V \\tfrac{1}{1-\\lambda} + \\gamma \\delta_{t+1}^V \\tfrac{\\lambda}{1-\\lambda} + \\cdots \\big) $$\n$$ = \\sum_{b=0}^{\\infty} (\\gamma \\lambda)^b , \\delta_{t+b}^V $$ 最后得到这仨表达式 $$\\delta_t^V = r_t + \\gamma V_\\theta(s_{t+1}) - V_\\theta(s_t) $$$$A_\\theta^{GAE}(s_t, a) = \\sum_{b=0}^{\\infty} (\\gamma \\lambda)^b , \\delta_{t+b}^V $$ $$\\frac{1}{N} \\sum_{n=1}^N \\sum_{t=1}^{T_n}A_\\theta^{GAE}(s_n^t, a_n^t) \\nabla \\log P_\\theta(a_n^t \\mid s_n^t)$$\nPPO原理-part3 通过运行模型来采集数据，这样就会导致采集数据时间过长，这也是ppo需要解决的问题,对于此我们采用重要性采样来解决\n重要性采样 $$ \\mathbb{E}(f(x))_{x \\sim p(x)} = \\sum_x f(x) p(x) $$\n$$ = \\sum_x f(x) p(x) \\frac{q(x)}{q(x)} $$\n$$ = \\sum_x f(x) \\frac{p(x)}{q(x)} q(x) $$\n$$ = \\mathbb{E}!\\left(f(x) \\frac{p(x)}{q(x)}\\right)_{x \\sim q(x)} $$\n$$ \\approx \\frac{1}{N} \\sum_{n=1}^N f(x) \\frac{p(x)}{q(x)}, \\quad x \\sim q(x) $$\n由此我们可以变换我们的公式，由On-Policy 转向Off-Policy\n$$ \\frac{1}{N} \\sum_{n=1}^N \\sum_{t=1}^{T_n} A_{\\theta}^{GAE}(s_n^t, a_n^t) \\nabla \\log P_\\theta(a_n^t \\mid s_n^t) $$\n$$ = \\frac{1}{N} \\sum_{n=1}^N \\sum_{t=1}^{T_n} A_{\\theta’}^{GAE}(s_n^t, a_n^t) \\frac{P_\\theta(a_n^t \\mid s_n^t)}{P_{\\theta’}(a_n^t \\mid s_n^t)} \\nabla \\log P_\\theta(a_n^t \\mid s_n^t) $$\n$$ = \\frac{1}{N} \\sum_{n=1}^N \\sum_{t=1}^{T_n} A_{\\theta’}^{GAE}(s_n^t, a_n^t) \\frac{\\nabla P_\\theta(a_n^t \\mid s_n^t)}{P_{\\theta’}(a_n^t \\mid s_n^t)} $$\n这里我们的做法是用旧策略(off-policy)的优势函数乘一个比例系数去模拟新的策略函数(on-policy)\n最后去掉梯度我们可以得到最终的loss函数（中间对$\\theta$求梯度是为了消去不影响$theta$的状态转移概率） $$\\text{Loss} = -\\frac{1}{N} \\sum_{n=1}^N \\sum_{t=1}^{T_n} A_{\\theta’}^{GAE}(s_n^t, a_n^t) \\frac{P_\\theta(a_n^t \\mid s_n^t)}{P_{\\theta’}(a_n^t \\mid s_n^t)}$$ 注意：旧策略与新策略之间的分布不能差距过大，否则很难学到有用的经验\n可以采用KL散度进行约束或者clip函数（限定那个新旧两种策略之间的比例在1左右，不能相差过大）\n$$\\text{Loss}_{ppo_1} = -\\frac{1}{N} \\sum_{n=1}^N \\sum_{t=1}^{T_n} A_{\\theta\u0026rsquo;}^{GAE}(s_n^t, a_n^t) \\frac{P_\\theta(a_n^t \\mid s_n^t)}{P_{\\theta\u0026rsquo;}(a_n^t \\mid s_n^t)} + \\beta , KL(P_\\theta, P_{\\theta\u0026rsquo;})$$\n$$\\text{Loss}_{ppo_2} = -\\frac{1}{N} \\sum_{n=1}^N \\sum_{t=1}^{T_n} \\min \\Bigg( A_{\\theta\u0026rsquo;}^{GAE}(s_n^t, a_n^t) \\frac{P_\\theta(a_n^t \\mid s_n^t)}{P_{\\theta\u0026rsquo;}(a_n^t \\mid s_n^t)} , \\text{clip}!\\left( \\frac{P_\\theta(a_n^t \\mid s_n^t)}{P_{\\theta\u0026rsquo;}(a_n^t \\mid s_n^t)}, 1-\\epsilon,1+\\epsilon \\right) A_{\\theta\u0026rsquo;}^{GAE}(s_n^t, a_n^t) \\Bigg)$$\n","title":"LLM-RL-PPO"},{"link":"/posts/test-alert/","text":"GitHub Style Alert Testing This article is used to test the new GitHub-style Alert feature and folding functionality.\nAlert Syntax Note Alert Note This is a note alert box. Used to display useful information that users should be aware of, even when quickly browsing the content.\nTip Alert Tip This is a tip alert box. Provides suggestions that help complete tasks better or more easily.\nImportant Alert Important This is an important alert box. Displays critical information users need to know to achieve their goals.\nWarning Alert Warning This is a warning box. Urgent information that requires immediate user attention to avoid problems.\nCaution Alert Caution This is a caution alert box. Advises users to be aware of the risks or negative consequences of certain behaviors.\nExtended Syntax - Custom Titles Note with Custom Title Custom Title This is a note alert box with a custom title.\nWarning with Custom Title Radiation Hazard Do not approach or handle without protective equipment.\nFolding Feature Expanded Foldable Alert by Default Click to Collapse This is an expanded foldable alert box by default. Click the title to collapse the content.\nSupports multi-line content:\nList item 1 List item 2 List item 3 Collapsed Alert by Default Important Information (Collapsed by Default) This is an important information box collapsed by default. Click the title to expand and view the content.\nCan include:\nOrdered list Bold text Italic text Code snippet Foldable Alert with Complex Content Complex Content Example This foldable box contains complex Markdown content:\nSubheading This is a paragraph containing a link and other formatting.\nJAVASCRIPT Collapse Copy // Code block example function hello() { console.log(\u0026#34;Hello, World!\u0026#34;); } Click to expand and view more Table Example Row1 Data1 Row2 Data2 Regular Blockquote This is a regular blockquote, not an Alert:\nThis is a standard blockquote. It won\u0026rsquo;t be rendered as an Alert but will use the standard blockquote styling.\nSupports multi-line content and formatted text.\nMultilingual Support Alerts support multiple languages, and titles will automatically display in the current language:\nNote In a Chinese environment, this title will display as \u0026ldquo;注意\u0026rdquo; (Note).\nTip In a Chinese environment, this title will display as \u0026ldquo;提示\u0026rdquo; (Tip).\nNested Content Test Nested Content Test This Alert contains nested content:\nThis is a nested blockquote\nList item Nested list item Another nested item Ordered list Nested ordered list Another nested item ","title":"GitHub Style Alert Test"},{"link":"/posts/katex-and-mermaid-test/","text":"KaTeX and Mermaid Test This article is used to test the KaTeX and Mermaid features.\nConfiguration Frontmatter Configuration YAML Collapse Copy --- katex: true mermaid: true --- Click to expand and view more Global Configuration YAML Collapse Copy # hugo.yaml katex: enabled: true delimiters: - left: \u0026#34;$$\u0026#34; right: \u0026#34;$$\u0026#34; display: true - left: \u0026#34;$\u0026#34; right: \u0026#34;$\u0026#34; display: false mermaid: enabled: true Click to expand and view more KaTeX Test Inline Formula This is an inline formula: $E = mc^2$, Einstein\u0026rsquo;s mass-energy equivalence formula.\nAnother example：When $a \\neq 0$, the solutions to the quadratic equation $ax^2 + bx + c = 0$ are $x = \\frac{-b \\pm \\sqrt{b^2-4ac}}{2a}$.\nBlock Formula Quadratic Formula $$x = \\frac{-b \\pm \\sqrt{b^2-4ac}}{2a}$$\nEuler\u0026rsquo;s Formula $$e^{i\\pi} + 1 = 0$$\nIntegral Formula $$\\int_{-\\infty}^{\\infty} e^{-x^2} dx = \\sqrt{\\pi}$$\nMatrix Representation $$\\begin{pmatrix} a \u0026amp; b \\\\ c \u0026amp; d \\end{pmatrix} \\begin{pmatrix} x \\\\ y \\end{pmatrix} = \\begin{pmatrix} ax + by \\\\ cx + dy \\end{pmatrix}$$\nSummation Formula $$\\sum_{n=1}^{\\infty} \\frac{1}{n^2} = \\frac{\\pi^2}{6}$$\nCommon Mathematical Symbols Test Using predefined macros: $\\RR$, $\\NN$, $\\ZZ$, $\\QQ$, $\\CC$\n","title":"KaTeX and Mermaid Test"},{"link":"/posts/image-rendering-test/","text":"Image Rendering Test The Demo comes from LightGallery All images are from Unsplash Single Image Photo by - Daniel Leone Justified Gallery Photo by - Tobias Rademacher Photo by - Massimiliano Morosinotto Photo by - Sascha Bosshard Photo by - Yusuf Evli Photo by - Jay Mantri Photo by - Florian van Duyn Photo by - Juan Davila Photo by - David Marcu Masonry by shortcodes ","title":"Image Rendering Test"},{"link":"/posts/code-highlighting-test/","text":"Code Highlighting Test This article is used to test the new code highlighting feature, including syntax highlighting, copy button, language display, etc.\nJavaScript JAVASCRIPT Collapse Copy function fibonacci(n) { if (n \u0026lt;= 1) return n; return fibonacci(n - 1) + fibonacci(n - 2); } const result = fibonacci(10); console.log(`The 10th Fibonacci number is: ${result}`); // Async/Await const asyncFunction = async () =\u0026gt; { try { const response = await fetch(\u0026#39;/api/data\u0026#39;); const data = await response.json(); return data; } catch (error) { console.error(\u0026#39;Error fetching data:\u0026#39;, error); } }; Click to expand and view more Codeblock with Line Numbers PYTHON Collapse Copy 1# Python with line numbers 2import asyncio 3from typing import List, Optional 4 5class DataProcessor: 6 def __init__(self, data: List[dict]): 7 self.data = data 8 9 def process(self) -\u0026gt; Optional[dict]: 10 \u0026#34;\u0026#34;\u0026#34;Process the data and return the result\u0026#34;\u0026#34;\u0026#34; 11 if not self.data: 12 return None 13 14 result = { 15 \u0026#39;total\u0026#39;: len(self.data), 16 \u0026#39;processed\u0026#39;: [] 17 } 18 19 for item in self.data: 20 if self.validate_item(item): 21 result[\u0026#39;processed\u0026#39;].append(item) 22 23 return result Click to expand and view more Highlighting Specific Lines GO Collapse Copy 1package main 2 3import \u0026#34;fmt\u0026#34; // This line will be highlighted 4 5func main() { 6 message := \u0026#34;Hello, World!\u0026#34; // This line will also be highlighted 7 8 fmt.Println(message) // This line will also be highlighted 9 10 for i := 0; i \u0026lt; 3; i++ { 11 fmt.Printf(\u0026#34;Count: %d\\n\u0026#34;, i) 12 } 13} Click to expand and view more Codeblock with Filename api.ts Collapse Copy // TypeScript API interface ApiResponse\u0026lt;T\u0026gt; { data: T; status: number; message: string; } interface User { id: number; name: string; email: string; avatar?: string; } class ApiClient { private baseURL: string; private headers: Record\u0026lt;string, string\u0026gt;; constructor(baseURL: string, apiKey?: string) { this.baseURL = baseURL; this.headers = { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, ...(apiKey \u0026amp;\u0026amp; { \u0026#39;Authorization\u0026#39;: `Bearer ${apiKey}` }) }; } async get\u0026lt;T\u0026gt;(endpoint: string): Promise\u0026lt;ApiResponse\u0026lt;T\u0026gt;\u0026gt; { const response = await fetch(`${this.baseURL}${endpoint}`, { method: \u0026#39;GET\u0026#39;, headers: this.headers, }); if (!response.ok) { throw new Error(`HTTP error! status: ${response.status}`); } return response.json(); } async post\u0026lt;T\u0026gt;(endpoint: string, data: any): Promise\u0026lt;ApiResponse\u0026lt;T\u0026gt;\u0026gt; { const response = await fetch(`${this.baseURL}${endpoint}`, { method: \u0026#39;POST\u0026#39;, headers: this.headers, body: JSON.stringify(data), }); return response.json(); } } const client = new ApiClient(\u0026#39;https://api.example.com\u0026#39;, \u0026#39;your-api-key\u0026#39;); async function getUsers(): Promise\u0026lt;User[]\u0026gt; { try { const response = await client.get\u0026lt;User[]\u0026gt;(\u0026#39;/users\u0026#39;); return response.data; } catch (error) { console.error(\u0026#39;Error fetching users:\u0026#39;, error); return []; } } Click to expand and view more Plain Text Codeblock PLAINTEXT Collapse Copy This is a plain text codeblock. It should not have syntax highlighting. You can test the copy functionality here. function test() { console.log(\u0026#34;This is a test.\u0026#34;); } Click to expand and view more Inline Code This is an inline code example：const x = 42; and npm install and git commit -m \u0026quot;update\u0026quot;.\n","title":"Code Highlighting Test"},{"link":"/posts/markdown-syntax-test-document/","text":"Heading 1 This is a paragraph under a level 1 heading.\nHeading 2 This is a paragraph under a level 2 heading.\nHeading 3 This is a paragraph under a level 3 heading.\nHeading 4 This is a paragraph under a level 4 heading.\nHeading 5 This is a paragraph under a level 5 heading.\nHeading 6 This is a paragraph under a level 6 heading.\nParagraphs and Text Formatting This is a normal paragraph. It can contain bold text, italic text, bold italic text, strikethrough, inline code, and link text .\nThis is another paragraph to test spacing between paragraphs.\nBlockquotes This is a simple blockquote.\nBlockquotes can contain multiple paragraphs.\nThis is an example of a nested blockquote:\nThis is nested quote content.\nMultiple levels of nesting are possible.\nLists Unordered List First item Second item Nested item 1 Nested item 2 Even deeper nested item Third item Ordered List First item Second item Nested ordered item 1 Nested ordered item 2 Even deeper nested item Third item Task List (Checkbox) Completed task Incomplete task Another completed task Nested task list Subtask 1 (done) Subtask 2 (not done) Subtask 3 (done) Definition List Term 1 This is the definition for term 1. Term 2 This is the definition for term 2. Terms can have multiple definitions. Code Inline Code This is a paragraph with console.log('Hello World') inside.\nCode Blocks JAVASCRIPT Collapse Copy function greet(name) { console.log(`Hello, ${name}!`); } greet(\u0026#39;World\u0026#39;); Click to expand and view more PYTHON Collapse Copy def fibonacci(n): if n \u0026lt;= 1: return n return fibonacci(n-1) + fibonacci(n-2) print(fibonacci(10)) Click to expand and view more CSS Collapse Copy .prose { max-width: none; color: var(--tw-prose-body); } .prose h1 { font-size: 2.25rem; font-weight: 700; } Click to expand and view more Tables Left Align Center Align Right Align Content 1 Content 2 Content 3 Longer content Medium Short Data A Data B Data C Horizontal Rule Images Sample Image Links This is a regular link .\nThis is a link with title .\nThis is a reference-style link: Reference Link Footnotes This is a paragraph with a footnote1.\nHere is another footnote2.\nHighlighted Text This is a paragraph with highlighted text.\nSuperscript and Subscript H~2~O is the chemical formula for water.\nE = mc^2^ is Einstein\u0026rsquo;s mass-energy equation.\nKeyboard Keys Press Ctrl + C to copy text.\nAbbreviations HTML is the abbreviation for HyperText Markup Language.\n*[HTML]: HyperText Markup Language\nMath Formula (if KaTeX supported) Inline formula: $E = mc^2$\nBlock formula:\n$$ \\int_{-\\infty}^{\\infty} e^{-x^2} dx = \\sqrt{\\pi} $$\nAdmonitions (if supported) Note This is a note.\nTip This is a tip.\nImportant This is important information.\nWarning This is a warning.\nCaution This is a caution.\nDetails (if supported) Click to expand details This is the collapsed detailed content.\nYou can include any Markdown syntax here:\nList item Bold text Code Mixed Content Test This paragraph contains multiple formats: bold, italic, code, link , strikethrough, highlight.\nComplex List First item with bold text Nested item with code Another nested item with link Second item with italic text Ordered nested item Another ordered nested item Third item with strikethrough text Complex Table Feature Status Description Bold ✅ Supports bold text Italic ✅ Supports italic Code ✅ Supports inline code Link ✅ Supports links Strikethrough ❌ Needs testing This test document covers most common Markdown syntax and can be used to verify the completeness and aesthetics of prose styles.\nThis is the content of the first footnote.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nThis is the content of a named footnote.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","title":"Markdown Syntax Test Document"}],"tags":[{"link":"/tags/alert/","name":"Alert","slug":"Alert"},{"link":"/tags/code/","name":"Code","slug":"Code"},{"link":"/tags/image/","name":"Image","slug":"Image"},{"link":"/tags/markdown/","name":"Markdown","slug":"Markdown"},{"link":"/tags/math/","name":"Math","slug":"Math"},{"link":"/tags/mermaid/","name":"Mermaid","slug":"Mermaid"},{"link":"/tags/prose/","name":"Prose","slug":"Prose"},{"link":"/tags/rendering/","name":"Rendering","slug":"Rendering"},{"link":"/tags/style/","name":"Style","slug":"Style"},{"link":"/tags/syntax-highlighting/","name":"Syntax-Highlighting","slug":"Syntax-Highlighting"},{"link":"/tags/test/","name":"Test","slug":"Test"},{"link":"/tags/%E5%A4%A7%E6%A8%A1%E5%9E%8B/","name":"大模型","slug":"大模型"},{"link":"/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/","name":"强化学习","slug":"强化学习"},{"link":"/tags/%E7%94%9F%E6%88%90%E5%BC%8F%E6%8E%A8%E8%8D%90/","name":"生成式推荐","slug":"生成式推荐"}]}