{"categories":[{"link":"/categories/code-highlighting/","name":"Code-Highlighting","slug":"Code-Highlighting"},{"link":"/categories/github/","name":"Github","slug":"Github"},{"link":"/categories/image-rendering/","name":"Image-Rendering","slug":"Image-Rendering"},{"link":"/categories/test/","name":"Test","slug":"Test"},{"link":"/categories/%E5%88%86%E7%B1%BB1/","name":"分类1","slug":"分类1"},{"link":"/categories/%E5%88%86%E7%B1%BB2/","name":"分类2","slug":"分类2"},{"link":"/categories/%E5%8D%9A%E5%AE%A2/","name":"博客","slug":"博客"}],"pages":[],"posts":[{"link":"/posts/hstu/","text":"DLRM（深度学习推荐系统）： 一个经典的，标准的，符合工业界趋势的深度学习推荐系统模型应该如何构建？\nBottom MLP：处理连续数值特征。 EmbeddingLookup：将离散类别特征映射到向量空间。 Feature Interaction Layer: 融合 dense 与 sparse 特征。 Top MLP：进行最终预测（如点击率）。 DLRM 成为推荐系统工业界（如 Meta、Tencent、ByteDance）广泛采用的标准架构之一，因为它兼顾：\n高效的稀疏特征处理能力； 强大的特征交互建模能力； 在分布式训练与推理环境下的可扩展性。 过去的十年间，在DLRM框架内不断地迭代、发展新模型 Feature interactions (FMs, DCN, Autolnt, DHEN/Wukong, MaskNet, \u0026hellip;) Multi-task learning (MMoE, ESMM, PLE, \u0026hellip;) Sequential (sub-)modules (one-stage DIN, BST, hybrid UBM, SIM, \u0026hellip;) Debiasing (off-policy correction / REINFORCE, IPW / CLRec, \u0026hellip;) Beyond two-tower settings (multi-interest / MIND, beam search / \u0026ldquo;generative retrieval\u0026rdquo; / TDM, OTM, DR, learned similarities / MoL, \u0026hellip;) . But 生成式模型的出现打破了传统的模型发展思路… Many explored use cases in RecSys:\nIn-context Learning (e.g., LLMRank, \u0026hellip;) Instruction Tuning (e.g., M6-Rec, TALLRec, \u0026hellip;) Transfer Learning utilizing World Knowledge (e.g., NoteLLM, \u0026hellip;) . 现有问题和挑战 推荐系统中的特征缺乏明确的结构。 十亿级词汇动态系统 vs 100K级自然语言静态系统 计算成本是实现大规模序列模型的主要”卡脖子”问题。 GR-24已经直追LLaMa-2的运算规模。 DLRMs + Generative Models: How do we get the best of both worlds? Classical recommendation models - DLRMs - vs LLMs Pros of LLMs\nReplace feature engineering, to the extent capable by language; World knowledge benefits cold-start scenarios; Scale with compute. Pros of DLRMs\nLeverage vast number of human-engineered features; Concise representations — efficient and support very long context sizes; Scale with (in-domain recommendation) data. 解决方案一重新构建工业化的推荐系统 - 将用户行为视为首要模态 将点击/跳过/停留/搜索等动作记为token;\n与图像/视频/文本/元数据融合而不丢失信息。\n- 规范化特征空间与编码 在sequence-to-sequence这种序列化视图下统一检索与排序\n- 消除架构中一些不好掌控的冗余 -工业级就绪性与可扩展性 随数据与计算资源扩展；\n对动态词汇表[持续新增项]及长历史数据具备泛化能力和鲁邦性。\n本文的核心贡献 统一的生成式推荐器 [GR]。部署在核心产品上；一个序列模型同时完成召回和排序，取代了长期以来依赖大量特征的推荐管线。 HSTU 编码器。针对高基数、非平稳数据流的新架构；在质量上超过 Transformer，并且在序列长度 L = 8192 时训练速度比 FlashAttention2 快 15.2 倍。 M-FALCON 算法。在成千上万候选项之间共享计算 [约 700 倍摊销]，即使模型大了 285 倍，也能实现约 2.48 倍的吞吐率提升。 公开数据集实验。在 MovieLens / Amazon Reviews 上，相比强序列模型基线（如 SASRec），NDCG@10 指标提升约 20% - 66% 线上A/B测试。端到端〔召回+排序〕提升最高 +18.6% [排序 +12.4%, 召回 +6.2%]。 大规模训练。训练了 1.5 万亿参数 [Trillion-Parameter] 的 GR；年计算量提升约 1000 倍，可与 GPT-3/LLaMA-2 相媲美；首次在推荐系统中观察到类似 LLM 的 scaling law。 DLRMs + Generative Models =\u0026gt; Generative Recommenders 从DLRM到GR 用户和商品的交互序列当作主线\n地理位置为辅助序列1，慢变化属性，辅助变量\n用户加入的购物社区2，慢变化属性，辅助变量\n像点击率这种快速变化,不能真实地反应用户兴趣，不再显示输入，而是让模型自己从历史时序序列中[生成]相同的信息\n作者将所有的正负反馈行为组织成序列，这个序列中既包含item_id, user_id,也包含稀疏特征，交互行为类型等，而摒弃了数值型特征，构造了生成式建模所需要的统一输入格式，这也印证了actions speak louder\n​\n简单说：GR里面，排序就是一个预测动作，检索就是预测内容，而这两个都是通过同一个序列模型来完成，在检索中，输入是动作和内容的交互序列 检索: 如果动作是正反馈，输出的候选内容加入候选集；如果是负反馈，则输出为空，所以检索的本质是根据用户的历史行为生成新的候选内容 排序：输入的是历史的内容和用户的动作序列，最后一位是内容，叫做内容位，模型在内容位预测用户会采取什么动作，比如点击，喜欢等等。所以排序的本质是在已有的候选内容上预测用户的排序分布 监督策略：只对正反馈监督，负反馈标记为空，只保留有意义的交互，减少冗余\nGR在同一条交互里序列完成检索，也就是动作位生成内容，再加上排序，也就是内容位生成内容，并且保持最新的动作和内容能够与历史快速交互，GR并不是每次曝光就发送一条样本，而是在会话末或者关键发送一条样本，在样本减少的情况下还能保存丰富的信息量。在传统的DLRM，每曝光一个item，就会生成一个独立的训练样本，这样的问题是样本特别多，而且很多计算是重复，因为用户的历史行为在不同样本会被反复算一遍。GR不会在每次曝光时就立刻生成样本，而是在一个关键时刻，比如一次推荐对话时间结束，把前面的交互序列打包成一个训练样本，这样在减少算力消耗的同时，每个样本的监督信号更密集，更干净， 同时通过更密集/更精确的每轮监督以及候选模型与历史数据间的目标感知交互，保持甚至提升质量。由公式可以看到，计算量下降了一个量级 $$O(N^3 d + N^2 d^2) t \\ \\ -\u0026gt; \\ \\ O(N^{2}d + Nd^{2})$$\nHSTU(Hierarchical Sequential Transduction Unit) 为了让GR模型在工业界大规模推荐系统中实现高可扩展性处理海量非稳态的词表和数据\nDLRM中的三个主要阶段：特征提取、特征交互作用以及表示的转换。 HSTU每个层包含三个主要的子层：\npointwise投影层：信息压缩与重构 Pointwise Projection Layer 在传统的 Q, K, V 投影之外引入了额外的 U 投影，用于建模用户长期历史（long-term user representation）。 它将用户的长序列行为压缩成较低维的语义表示，从而在与目标（target item）交互时，能更有效地筛选和增强关键信息。 pointwise空间聚合层: 局部信息聚合 Pointwise Aggregation Layer 使用“聚合注意力（aggregation attention）”取代传统 softmax 注意力，不再进行概率归一化，从而避免了 softmax 的稀释效应。 这种改进能更充分地保留输入信息，使模型在面对动态变化或长尾分布的行为词表时更加稳定。 pointwise转换层: 非线性变换与偏置调整 Pointwise Transformation Layer 在非线性变换（如 ReLU/GELU）中引入了偏置项 $r_{ab}$，以对 token 之间的关系进行细粒度建模。 该层增强了模型的表达灵活性，使其能更好地捕捉行为序列中的复杂交互模式。 工程优化 稀疏性优化 高效注意力内核 [GPU内核] 将注意力机制的计算重构为一组分组矩阵乘法（Grouped GEMM），并通过融合内核（Fused Kernel）实现一次 GPU 调用完成所有步骤（QKᵀ、Softmax、乘 V），从而减少显存访问和内核启动开销，使模型在 GPU 上的计算吞吐量提升约 2–5 倍。\n算法增强稀疏性：随机长度 [SL] 用户行为在多时间尺度上呈现重复性\n通过从完整历史记录中提取子序列进行训练，人工增强数据稀疏性 ​\n内存是工业级scaling非常重要的瓶颈! 精简层级与融合操作： HSTU将非注意力线性层从6层削减至2层\n使每层内存消耗降至约14d [bf16]\n该设计可构建深度提升2倍以上的模型。\n嵌入/优化器内存： 针对十亿级词汇表，采用行向AdamW算法，使嵌入参数的 HBM占用从约12B缩减至约2B。\n对于十亿级 embedding 表，普通 AdamW 会因维护元素级动量状态导致显存占用极高。 采用“行向 AdamW”后，将状态按行聚合，大幅压缩优化器状态存储，使嵌入参数在 GPU 高速显存（HBM）中的占用从约 12B 降至 2B，提高了训练可扩展性和内存效率 并行优化 通过成本摊销实现推理扩展 在排序阶段，我们面临数万个候选项目。作者提出名为M-FALCON[微批量快速注意力缓存操作]的算法，可在输入序列长度为n时，对m个候选项执行单次推理。\nM-FALCON 的共享计算原理：通过将用户历史序列的中间特征（K,V）缓存起来，让所有候选 item 复用这份表示，仅计算目标 item 的交叉注意力，并采用微批并行方式统一处理，从而实现“计算一次，多次使用”的推理加速。 他们成功部署了复杂度提升285倍的目标感知交叉注意力模型，在相同推理计算能力下实现了1.5倍的吞吐量提升。\nMicrobatched-Fast Attention Leveraging Cachable Operations 数据集 MovieLens\n该非商业性数据集由明尼苏达大学计算机科学与工程学院的GroupLens项目团队创建，旨在支持研究工作。数据集包含多个电影评分数据子集，已成为推荐系统领域的经典数据集。\nAmazon Book Reviews\n亚马逊提供的产品数据集包含商品评论及元数据，涵盖1996年5月至2014年7月期间的1.428亿条评论。\n工程优化的有效性 随机序列长度[SL]的有效性：长度为4096的序列可以减少80%的token。即使 $\\alpha$ 变大，可有效减少的幅度也不会下降很多。同时NE指标并没有因此变差，变差幅度不超过0.2%\n相比FlashAttention优化的 Transformers，HSTU在训练和推理阶段效率能分别提升15.2x、5.6x倍。\n此外，由于内存的各种优化和节省，相比于Transformers，HSTU可以再叠深2层网络。\nNE指标下降明显，意味着线上指标效果提升明显 Generative Recommenders vs DLRMs 传统DLRM里用了大量的特征，如果把DLRM的特征做消融，仅仅保留GR里用到的那些，那么模型效果会大打折扣，看离线指标降了很多。一方面说明传统DLRM没有特征不行，另一方面表明了新架构的优势。\n在GR中只考虑item的属性(content-based)，召回效果奇差，比baseline也低不少，说明用户行为中蕴含的高基数、高阶信息建模的重要性。\n即使模型复杂度比基线高出285倍，当候选序列为1024时，GR仍能实现1.5倍吞吐量；选序列为16384时，吞吐量提升至2.48倍。\n在大型工业环境中，DLRM模型在特定计算和参数配置条件下会达到质量饱和点。 而GRs体现出来了更强劲的、在大模型领域出现过的scaling能力。\n推荐系统中的幂律扩展趋势则无法显现算力越大，效果越好这种趋势\n总结 作者提出一种统一的生成式推荐模型 [GR]，以实践证明“行动胜于言辞”。性能表现：在传统公开测试集和行业真实流数据集上均取得显著提升，其NDCG@10指标较经典SASRec模型提升 20.3% - 65.8% 。相较于经多次迭代优化的DLRMs基准模型，该模型在线召回率与精确度实现 18.6% 的相对提升。首次在核心产品线中取代传统深度推荐模型——而这些基于海量异构特征的模型曾主导推荐领域近十年。\n长远的意义 通过降低推荐、搜索和广告系统对海量异构特征的依赖，这些系统既能提升用户体验，又能更注重隐私保护。 传统推荐系统常基于用户短期行为和偏好生成推荐，这可能导致用户接触到与其长期目标不符的内容。 该方法还使平台激励机制与用户价值更趋一致，能够真正识别用户潜在需求而非平台推广导向。 ","title":"HSTU"},{"link":"/posts/tiger/","text":"TIGER 推荐系统深度学习基本流程 现有深度学习架构：\n输入特征 → 文本嵌入 → 隐藏层（RNN，深度学习网络）\n得到更精准的预测表征 → 预测层（通过查询与物品的相似性度量、点击，对相关物品进行排序，得到预测） 检索 / 召回一系列可行的候选对象，然后用排序模型对其进行排序。 检索阶段 通过矩阵分解，在同一空间学习查询和候选的嵌入。\n为了更好地捕获数据中的非线性关系，近年来采用 内积查询和候选嵌入到同一空间的双塔编码架构（一个塔用于查询(用户)，另一个塔用于候选(物品)）成为主流。\n为了在推理期间使用这些模型，使用候选塔创建一个存储所有物品嵌入的索引。\n对于给定查询，通过在同一个空间内嵌入查询和候选项来执行大规模检索，\n然后使用**近似最邻近搜索（ANN）**来选择给定查询嵌入的最佳候选项。 然后筛选到的物品进行内积计算排序\n推荐系统存在的局限与挑战 召回 → 排序 → 重排，流程复杂，需要分别优化。 对召回阶段的依赖（庞大候选集快速筛选相关物品） 如果召回都没召回到用户感兴趣的东西，排序更不可能找到。 对排序阶段的局限（基于学习的排序模型 / 神经网络模型 / 对候选物品排序）。 对反馈循环的影响：基于用户的历史交互行为进行预测，会产生反馈循环。 冷启动推荐：新商品被反馈循环影响。 推荐的“长尾”现象：热门更热，冷门更冷。 本文工作：提出全新的推荐系统框架 TIGER 基于生成式检索范式并应用到序列推荐中：\n创建语义上有意义的 token 元组，作为每个物品的语义 ID。\n通过生成的用户交互序列中物品的语义 ID，训练基于 Transformer 的序列到序列模型来“生成”用户将与之交互的下一个物品的语义 ID。 提出 TIGER 推荐模型在各种数据集上的性能显著优于当前 SOTA 模型。 展现两个关键能力： 冷启动推荐能力：能够推荐此前从未出现过的物品； 推荐多样性控制能力：可通过调节生成参数实现推荐内容的丰富性。 生成式推荐新范式，为构建更具泛化能力、可解释性和灵活性的推荐系统开辟新方向。 TIGER 的生成式推荐范式 提出一种构建序列推荐生成式检索模型的新范式。\n与传统的检索-排序方法不同，\n我们的方法使用 直接预测候选物品 ID 的端到端生成模型。\n我们提出利用 Transformer 内存（参数）作为推荐系统中检索的端到端索引引擎。\n我们将提出的方法称为 Transformer Index for GEnerative Recommenders（TIGER）。\n图 2 展示了 TIGER 框架的整体工作流程，说明了如何将序列推荐任务转化为一个生成式检索任务。\n语义 ID（Semantic ID）简介 语义 ID 示例： (2, 3, 55)，每一个数字都代表着一个稠密向量。\n将物品 (item) 表示为语义 ID 序列具有许多优点：\n语义共享与泛化\n在具有语义意义的数据上训练 Transformer 允许在相似的物品之间共享知识，\n避免在推荐模型中使用原子、随机的物品 ID 作为特征，\n从而支持知识迁移和泛化。\n缓解反馈循环\n使用物品的语义 ID 能缓解推荐系统固有的反馈循环，\n减少系统对热门物品的依赖，\n并允许模型泛化到语料库中的新物品，更好地支持新物品推荐。\n存储与扩展性\n通常物品数量可达数十亿级。\n使用有限数量的代码词组合生成语义 ID 来表示物品，\n可显著降低物品表示的存储成本和参数规模，\n提升系统的可扩展性。\n本工作的主要贡献 提出新的基于生成检索的推荐框架 TIGER：\n为每个物品分配语义 ID，并训练模型预测生成给定用户可能交互的物品语义 ID。 通过召回率和 NDCG 指标，证明 TIGER 在多个数据集上优于现有 SOTA 推荐系统。 发现生成式检索新范式在序列推荐系统中带来了两个额外能力： 推荐新的和冷门的物品，改进冷启动； 可用可调参数生成多样的推荐结果。 语义ID详解 定义与构成 语义ID是长度为(m)的token元组，每个token来自不同的codebook（代码簿），可唯一表示的项数等于每层codebook大小的乘积。 示例：语义ID([10,21,35])与([10,21,40])的相似度高于([10,23,32])，因前两个token重叠，语义更接近。 关键属性 相似性：相似项（内容特征相似或语义嵌入接近）需具有重叠的语义ID。 离散化语义表示：不依赖具体物品ID，基于内容语义生成，而非物品实体本身。 可控可复用：组成单位（codewords）来自固定大小的codebook，大小可控且可重复利用。 组合式表达：如每个ID由3个token组成，每个token选自1000个候选，可表达(1000^3=10^9)个组合，能覆盖大规模推荐系统的物品空间。 与大语言模型Tokenizer的区别 对比维度 语义ID 大语言模型Tokenizer 依赖对象 不依赖具体物品ID，基于内容语义 依赖文本序列，与物品语义无直接关联 组成来源 固定大小codebook的codewords 文本语料库中的子词（如BPE分词结果） 表达能力 组合式，覆盖物品空间能力强 序列式，聚焦文本语义表达 Tip 通过语义ID的设计，使得推荐系统能够像语言模型那样“输出”目标物品，但又规避了推荐领域中 item ID/token 空间过大、实体变化频繁等根本难题。可以说，语义ID是 TIGER 成功的核心支柱之一。\n语义ID生成方法 核心方法：基于RQ-VAE（残差量化变分自动编码器） RQ-VAE 首先通过编码器 $\\mathcal{E}$ 对输入 $x$ 进行编码，以学习潜在表示：$z := \\mathcal{E}(x)$\n在第 0 个残差两化层（$d=0$）时，初始残差被定义为：$r_0 := z$ 在每一层 $d$，都有一个码本（codebook）：$C_d :=$ { $e_k$ } $_{k=1}^K$ 其中 $K$ 是码本的大小。（可以理解为有K个物品）\n接着，$r_0$ 被量化为该层码本中最近的嵌入向量。 距离最近的嵌入为 $e_{c_d}$，其索引为：$c_0 = \\arg\\min_k | r_0 - e_k |$ 这表示第 0 个 codeword。 在下一层（$d=1$）中，残差定义为：$r_1 := r_0 - e_{c_0}$ 然后与第 0 层类似，第 1 层的 codeword 通过在码本中寻找最接近 $r_1$ 的嵌入得到。 该过程递归重复 $m$ 次，以获得 $m$ 个 codeword 组成的元组： $$ (c_0, c_1, \\ldots, c_{m-1}) $$ 该元组即表示语义 ID（Semantic ID）。\n重构与损失函数 一旦得到语义 ID $(c_0, \\ldots, c_{m-1})$，量化后的潜在表示计算为： $$ \\hat{z} := \\sum_{d=0}^{m-1} e_{c_d} $$\n然后将 $\\hat{z}$ 传入解码器，以重建输入 $x$。\nRQ-VAE 的总损失定义为： $$ \\mathcal{L}(x) := \\mathcal{L}_{recon} + \\mathcal{L}_{rqvae} $$\n其中： $$ \\mathcal{L}_{recon} := || x - \\hat{x} ||^2, $$ $$ \\mathcal{L}_{rqvae} := \\sum_{d=0}^{m-1} \\big( | sg[r_i] - e_{c_i} |^2 + \\beta || r_i - sg[e_{c_i}] ||^2 \\big) $$\n这里 $\\hat{x}$ 是解码器输出，$sg$ 表示停止梯度（stop-gradient）操作。\n该损失函数联合训练编码器、解码器和码本。\n避免 Codebook Collapse 为了防止 RQ-VAE 出现 codebook collapse（即大部分输入仅映射到极少数码本向量），\n采用基于 k-means 聚类 的码本初始化方法。\n具体做法是在首个训练批次上应用 k-means 算法，并使用聚类中心作为初始码本向量。\n生成示例 若经 3 层量化得到嵌入索引 $e_{c_0}=7$、$e_{c_1}=1$、$e_{c_2}=4$， 则语义 ID 为 $(7, 1, 4)$。\n其他量化方法对比（消融实验证实RQ-VAE最优） 方法 特点 不足 LSH（局部敏感哈希） 离散化速度快 精度低，语义保留差 VQ-VAE（矢量量化变分自动编码器） 基础量化能力 无语义层次粒度，无法体现粗/细分类别 基于k-means层次聚类 可实现层次划分 丢失ID间的语义关系，相似项可能无重叠ID 冲突处理 附加标识位：增加1位token表示语义重复，区分相同语义ID的不同物品。 查找表：维护“语义ID→物品ID”映射表，避免模型误判无对应物品的语义ID；仅需训练后处理1次，不影响训练效率。 基于语义ID的生成式检索（TIGER框架核心） 核心思路 将推荐任务转化为“序列生成”任务：通过用户历史交互物品的语义ID序列，预测下一个物品的语义ID，实现生成式检索。 流程步骤 （1）用户序列构建 按时间顺序排序用户历史交互物品，形成物品序列$(item_1, \u0026hellip;, item_n)$。 （2）序列转换 设$(item_i)$的语义ID为$(c_{i,0}, \u0026hellip;, c_{i,m-1})$，将物品序列转换为语义ID序列：$(c_{1,0}, \u0026hellip;, c_{1,m-1}, c_{2,0}, \u0026hellip;, c_{2,m-1}, \u0026hellip;, c_{n,0}, \u0026hellip;, c_{n,m-1})$。 （3）模型训练与预测 模型结构： 任务目标：训练序列到序列模型，预测下一个物品的语义ID$(c_{n+1,0}, \u0026hellip;, c_{n+1,m-1})$。 特殊情况：解码器生成的语义ID可能与推荐语料库物品不匹配，但概率极低。 通过语义ID的设计，TIGER框架成功地将推荐系统引入了一个生成式检索的新范式，使得系统能够像语言模型那样直接“输出”目标物品。 实验设计与结果 1. 实验基础设置 基座模型与数据集 2. 核心实验结果 （1）序列推荐性能对比（TIGER最优） （2）消融实验：不同ID生成方式（RQ-VAE SID最优） （3）冷启动推荐性能（TIGER优于KNN） （4）推荐多样性（温度系数调控） （5）模型层数与用户信息影响 （6）Invalid Semantic ID 模型成本与优势 成本分析 （1）内存成本 查找表：维护“物品ID→语义ID”和“语义ID→物品ID”两个哈希表，每个语义ID为4整数元组，大小约64N位（N为物品数）。 嵌入表：仅存储codebook中每个codeword的嵌入，远小于传统推荐系统的“物品-嵌入”表（传统需为每个物品存储嵌入）。 （2）推理成本 挑战：自回归解码+beam search，推理成本高于基于ANN的模型。 优化方向：探索更小模型结构或高效推理方法。 核心优势 嵌入表规模可控：嵌入表基数不随物品空间线性增长，避免传统模型的大型嵌入表问题。 泛化能力强：支持冷启动推荐，可泛化到未见过的新物品。 生成式范式创新：将推荐从“检索匹配”转为“生成预测”，开辟新研究方向。 再次总结 模型核心思路 本文提出了一种新的推荐范式，称为 TIGER，使用生成检索模型在序列推荐中“生成”下一个可能的交互对象。 支撑该方法的是一种新的 物品语义 ID 表示，它在内容嵌入上使用多层量化器（RQ-VAE）来生成语义 ID。 基于 Transformer 的序列生成模型：将用户的历史行为表示为语义 ID 序列，使用 Encoder–Decoder 结构的 Transformer 模型学习用户偏好，并生成下一个物品的语义 ID。 嵌入表的基数不会随着物品空间的线性增长而增加，这使得训练过程中无需为每个单独物品建立大型嵌入表或索引系统，效率更高。 在三个数据集上的实验表明，该模型能达到与 SOTA 检索模型相当的性能，同时具备泛化能力，可生成全新的或未见过的物品。 不足 论文中的语义 ID 建模是静态的，没有与 user–item 交互相关联，也未融合协同信息。 后续的生成式模型在动态交互建模和个性化生成方面仍有较大改进空间。 ","title":"TIGER \u0026 Semantic ID"},{"link":"/posts/grpo/","text":"GRPO是针对LLM的一种改进PPO算法\n回顾： Policy Gradient $$ \\nabla_\\theta J(\\theta) \\approx \\frac{1}{N} \\sum_{n=1}^{N} \\sum_{t=1}^{T_n} R(\\tau^{(n)}) \\nabla_\\theta \\log P_\\theta(a_t^{(n)} \\mid s_t^{(n)}) $$\n符号解释 符号 含义 $\\nabla_\\theta J(\\theta)$ 目标函数（期望回报）关于参数$\\theta$的梯度，即策略更新的方向。 $N$ 采样的轨迹数量（episodes），即从环境中采样的完整回合数。 $T_n$ 第$n$条轨迹的时间步数（episode的长度）。 $\\tau^{(n)}$ 第$n$条轨迹（trajectory）：\n$\\tau^{(n)} = (s_1^{(n)}, a_1^{(n)}, r_1^{(n)}, \\dots, s_{T_n}^{(n)}, a_{T_n}^{(n)}, r_{T_n}^{(n)})$。 $R(\\tau^{(n)})$ 整条轨迹的总回报（return），通常为折扣累计奖励：\n$R(\\tau^{(n)}) = \\sum_{t=1}^{T_n}\\gamma^{t-1}r_t^{(n)}$。 $\\nabla_\\theta \\log P_\\theta(a_t^{(n)}\\mid s_t^{(n)})$ 策略梯度项，表示策略在状态$s_t^{(n)}$下选择动作$a_t^{(n)}$的对数概率的梯度，用于指导参数更新。 $P_\\theta(a_t^{(n)}\\mid s_t^{(n)})$ 参数化策略（policy），给定状态$s_t^{(n)}$时采取动作$a_t^{(n)}$的概率，由参数$\\theta$控制。 直观理解\n这个公式的含义是：如果一条轨迹带来的总奖励$R(\\tau)$很高，那么模型应该调整参数$\\theta$，让在这条轨迹中采取的动作$a_t$的概率更高；反之则降低这些动作的概率。\n考虑动作影响时效 Discounted Return（折扣回报） $$ R_t^{(n)} = \\sum_{t\u0026rsquo;=t}^{T_n} \\gamma^{,t\u0026rsquo;-t} r_{t\u0026rsquo;}^{(n)} $$ 符号解释 符号 含义 $t$、$t'$ 时间步索引；$t\u0026rsquo;$ 是求和变量，从当前时间步 $t$ 开始一直到 $T_n$。 $\\gamma$ 折扣因子（discount factor），$0 \u0026lt; \\gamma \\le 1$。它控制未来奖励的重要性：越小表示越“短视”。 $r_{t\u0026rsquo;}^{(n)}$ 第 $n$ 条轨迹在时间步 $t\u0026rsquo;$ 获得的即时奖励（immediate reward）。 直观理解\n这个公式计算的是从某个时间步 $t$ 开始，到轨迹结束的所有未来奖励的加权和。\n靠近当前的奖励权重大（因为$\\gamma^{0}=1$），越往后的奖励会被折扣（因为$\\gamma^{t\u0026rsquo;-t}$会变小）。\n与策略梯度公式的关系\n在策略梯度中，$R_t^{(n)}$ 通常替代 $R(\\tau^{(n)})$ 用作加权项，使得每个时间步的梯度更新都考虑该时刻之后的未来回报： $$ \\nabla_\\theta J(\\theta) \\approx \\frac{1}{N} \\sum_{n=1}^{N} \\sum_{t=1}^{T_n} R_t^{(n)} \\nabla_\\theta \\log P_\\theta(a_t^{(n)} \\mid s_t^{(n)}) $$\n考虑到动作的相对优势 Policy Gradient with Baseline $$ \\nabla_\\theta J(\\theta) \\approx \\frac{1}{N} \\sum_{n=1}^{N} \\sum_{t=1}^{T_n} \\big(R_t^{(n)} - B(s_t^{(n)})\\big) \\nabla_\\theta \\log P_\\theta(a_t^{(n)} \\mid s_t^{(n)}) $$\n符号解释 符号 含义 $B(s_t^{(n)})$ 基线函数（baseline function），通常是状态价值函数$V_\\pi(s_t^{(n)})$，用于减小方差。 $\\nabla_\\theta \\log P_\\theta(a_t^{(n)}\\mid s_t^{(n)})$ 策略梯度项，表示在状态$s_t^{(n)}$下采取动作$a_t^{(n)}$的对数概率的梯度。 $P_\\theta(a_t^{(n)}\\mid s_t^{(n)})$ 参数化策略（policy），给定状态$s_t^{(n)}$时采取动作$a_t^{(n)}$的概率，由参数$\\theta$控制。 直观理解\n与原始 REINFORCE 相比，这个版本在计算梯度时引入了一个基线项 $B(s_t^{(n)})$，用于减少方差但不引入偏差。\n它衡量“当前动作的回报比期望高多少”：\n如果 $R_t^{(n)} \u0026gt; B(s_t^{(n)})$，说明该动作比平均表现好 → 提高它的概率； 如果 $R_t^{(n)} \u0026lt; B(s_t^{(n)})$，说明该动作表现差 → 降低它的概率。 备注 若将 $A_t^{(n)} = R_t^{(n)} - B(s_t^{(n)})$ 记作优势函数（advantage function），\n公式可简写为：\n$$ \\nabla_\\theta J(\\theta) \\approx \\frac{1}{N} \\sum_{n=1}^{N} \\sum_{t=1}^{T_n} A_t^{(n)} \\nabla_\\theta \\log P_\\theta(a_t^{(n)} \\mid s_t^{(n)}) $$\n引入优势函数 优势函数定义 $$ A_\\theta(s, a) = Q_\\theta(s, a) - V_\\theta(s) $$ 在状态 $s$ 下，执行动作 $a$，相比平均水平（状态价值）能带来多少额外优势。\n各符号解释 符号 含义 $A_\\theta(s, a)$ 优势函数（Advantage Function），衡量在状态 $s$ 下执行动作 $a$ 比平均表现（$V_\\theta(s)$）好多少。 $Q_\\theta(s, a)$ 动作价值函数（Action-Value Function）：在状态 $s$ 下执行动作 $a$ 后，期望获得的累计回报。 $V_\\theta(s)$ 状态价值函数（State-Value Function）：在状态 $s$ 下，依据当前策略期望的回报。 $\\theta$ 策略参数，控制策略 $\\pi_\\theta$ 或价值函数的参数化形式。 直观理解 $Q_\\theta(s,a)$ 衡量“执行某动作后能得到的回报”； $V_\\theta(s)$ 衡量“在该状态下平均能得到的回报”； 二者之差 $A_\\theta(s,a)$ 衡量“这个动作比平均动作好多少”。 当 $A_\\theta(s,a) \u0026gt; 0$ 时，该动作优于平均水平，应提升其概率； 当 $A_\\theta(s,a) \u0026lt; 0$ 时，该动作劣于平均，应降低其概率。\nAdvantage Policy Gradient 公式 $$ \\nabla_\\theta J(\\theta) \\approx \\frac{1}{N} \\sum_{n=1}^{N} \\sum_{t=1}^{T_n} A_\\theta(s_t^{(n)}, a_t^{(n)}) \\nabla_\\theta \\log P_\\theta(a_t^{(n)} \\mid s_t^{(n)}) $$\n符号解释 符号 含义 $\\nabla_\\theta J(\\theta)$ 期望回报关于参数 $\\theta$ 的梯度，即策略改进方向。 $A_\\theta(s_t^{(n)}, a_t^{(n)})$ 第 $n$ 条轨迹第 $t$ 步的优势值，衡量该动作相对于平均水平的好坏。 $\\log P_\\theta(a_t^{(n)} \\mid s_t^{(n)})$ 策略函数在状态 $s_t^{(n)}$ 下选择动作 $a_t^{(n)}$ 的对数概率。 $N$ 采样轨迹数量（episodes）。 $T_n$ 第 $n$ 条轨迹的时间步数。 GAE 优势函数（Generalized Advantage Estimation） $$ A_\\theta(s_t, a_t) = Q_\\theta(s_t, a_t) - V_\\theta(s_t) $$\n$$ Q_\\theta(s_t, a_t) = r_t + \\gamma V_\\theta(s_{t+1}) $$\n$$ A_\\theta(s_t, a_t) = r_t + \\gamma V_\\theta(s_{t+1}) - V_\\theta(s_t) $$\n$$ V_\\theta(s_{t+1}) \\approx r_{t+1} + \\gamma V_\\theta(s_{t+2}) $$\n$$ \\begin{aligned} A_\\theta^{1}(s_t, a_t) \u0026amp;= r_t + \\gamma V_\\theta(s_{t+1}) - V_\\theta(s_t) \\ A_\\theta^{2}(s_t, a_t) \u0026amp;= r_t + \\gamma r_{t+1} + \\gamma^2 V_\\theta(s_{t+2}) - V_\\theta(s_t) \\ A_\\theta^{3}(s_t, a_t) \u0026amp;= r_t + \\gamma r_{t+1} + \\gamma^2 r_{t+2} + \\gamma^3 V_\\theta(s_{t+3}) - V_\\theta(s_t) \\ A_\\theta^{T}(s_t, a_t) \u0026amp;= r_t + \\gamma r_{t+1} + \\gamma^2 r_{t+2} + \\gamma^3 r_{t+3} + \\cdots + \\gamma^{T} r_T - V_\\theta(s_t) \\end{aligned} $$\n$$ A_\\theta^{GAE}(s_t, a_t) = (1 - \\lambda)(A_\\theta^{1} + \\lambda A_\\theta^{2} + \\lambda^2 A_\\theta^{3} + \\cdots) $$\nGRPO算法计算每个Token的Advantage GRPO：Group Relative Policy Optimization\n中文名称：群体相对策略优化\n示例输入（Prompt及生成结果） 同一个问题，模型会输出三个结果\n什么 是 数据库 ? → 数据库 用于 存储 数据 。 什么 是 数据库 ? → 数据库 是 一个 有组织的 数据集合 什么 是 数据库 ? → 数据库 是 用来 高效 存取 数据 的 软件 奖励（Reward） 对每个句子打一个分 $$ r_1 = 3.8, \\quad r_2 = 5.2, \\quad r_3 = 6.1 $$\n标准化奖励（Standardized Reward） $$ \\tilde{r}_i = \\frac{r_i - \\text{mean}(r)}{\\text{std}(r)} $$\n$$ \\tilde{r}_1 = -1.06, \\quad \\tilde{r}_2 = 0.14, \\quad \\tilde{r}_3 = 0.92 $$\n每个Token的Advantage表 Token序列 Advantage值 数据库 用于 存储 数据 。 -1.06 数据库 是 一个 有组织的 数据集合 0.14 数据库 是 用来 高效 存取 数据 的 软件 0.92 这里代表着每一个token都是Advantage值，例如‘数据库’ ’用于‘ ’存储‘ ’数据‘ 的token都是 -1.\n目标函数 $$ J_{GRPO} = \\frac{1}{N} \\sum_{n=1}^{N} \\sum_{t=1}^{T_n} \\min \\Bigg( \\textcolor{red}{A_{\\theta\u0026rsquo;}^{GRPO}(s_n^t, a_n^t) \\frac{P_\\theta(a_n^t|s_n^t)}{P_{\\theta\u0026rsquo;}(a_n^t|s_n^t)}}, clip\\left( \\frac{P_\\theta(a_n^t|s_n^t)}{P_{\\theta\u0026rsquo;}(a_n^t|s_n^t)}, 1-\\varepsilon, 1+\\varepsilon \\right) A_{\\theta\u0026rsquo;}^{GRPO}(s_n^t, a_n^t) \\Bigg) - \\beta KL(P_\\theta, P_{\\theta\u0026rsquo;}) $$\n点评 PPO在计算token的优势的时候，Reward模型只把得分到最后一个token里，其他得分都为0，然后用KL散度乘以一个系数加上刚才的得分 得到Reward对整体句子中每一个token的打分，但是对于大模型回答来说，我们评判的是整体的回答，而不是像游戏中那样在某个场景下只关注当下的动作，因此用KL散度乘系数再加上Reward对最后一个token打分这种计算方式，对于评估大模型生成的回答质量实在有些牵强\n简洁来说，PPO只关注最后一个token的优势值，而我们想关注的是一句话作为整体的每个token的优势值。\nGRPO提供了一种不需要训练状态价值网络，就可以估算每个token优势值的方法，而且这个方法更适合大模型生成强化学习这个场景\n实际代码中，Clip和KL散度用一个就行了。\n","title":"LLM-RL-GRPO"},{"link":"/posts/grid/","text":"1.前言 GR 利用生成模型的进步，实现两种主要方式：\n直接生成用户感兴趣物品的文本内容 或者从预训练模型中提取语义表示（semantic representations），以编码开放世界的知识，本文中的模型是后者。 2.语义ID（Semantic IDs） 2.1语义ID是什么 促成 GR（Generative Recommender） 成功的关键因素之一是 语义 ID（SID, Semantic ID）。它将连续的语义表示（例如来自大型语言模型的向量表示）转换为离散的 ID 序列。 2.2传统ID与语义ID对比 传统的ID特征是为用户/物品分配独一无二，但不提供信息的ID，这些ID被映射到嵌入向量中，主要用于捕获协同过滤信号 语义ID将连续的语义表示(例如，来自大模型语言)转换为离散的ID序列；这样能够同时利用预训练基础模型中编码的语义知识以及用户-物品交互历史中的编码的协同信号。 当两个物品的 SID 有重叠时，这种重叠在原理上反映了它们的语义相似性；同时，下一步的监督学习（next-item supervision）使模型能够学习跨 SID 的协同过滤信号 2.3怎么得到语义ID 首先通过模态编码器（modality encoder，例如大型语言模型 LLMs）提取语义表示； 然后使用量化器（quantizer）将连续嵌入压缩为离散稀疏 ID。 常见的基于量化的标记器包括： RQ-VAE ，RVQ ，Residual K-Means 。 2.4如何将语义ID用于生成式推荐 TIGER 首次将 Transformer 应用于推荐任务， 用于预测物品的语义 ID（SID），并借鉴了文档检索（document retrieval）中生成式推荐的思想 。后续研究在多个方向上改进了 SID 的训练：\n通过协同过滤信号增强学习 ； 引入分布式平衡机制（distributional balancing）； 使用更强大的大型语言模型或多模态编码器（multimodal encoders）。 3.GRID架构 我们考虑一个用户集合 $\\mathcal{U}$，每个用户与一个物品集合 $\\mathcal{I}$ 中的项目交互。每个物品 $i \\in \\mathcal{I}$ 都有相关的语义特征 $f_i$，包括但不限于文本和图像。\n每个用户 $u \\in \\mathcal{U}$ 都有一个交互序列，其长度为 $L_u$，记作 $S_u = [i_u^1, i_u^2, \\dots, i_u^{L_u}]$。\n一般性地，模态编码器（modality encoder，例如 LLM 或 VLM）将特征 $f_i$ 转换为 $d$ 维的表示 $h_i \\in \\mathbb{R}^d$。\n生成式推荐（GR）的目标是解决序列推荐问题：给定一个用户的交互序列 $S_u$，生成一个候选物品集合，使得这些物品是用户下一步最可能交互的内容（即 $i_u^{L_u+1}$）。\n3.1架构：先标记化（Tokenization）后生成（Generation）** GRID 将基于语义ID的生成式推荐划分为两个阶段：\n标记化阶段（Tokenization）\n将物品特征映射成嵌入向量（即 $h_i$），再量化为语义ID（SID）。\n生成阶段（Generation）\n使用所有物品的SID，探索不同的生成模型结构（例如 Transformer-based 模型），以生成下一个物品的SID。\nGRID 在每个阶段都提供了灵活的可配置实现。\n3.2语义ID标记化（Semantic ID Tokenization） SID 标记化首先通过预训练的模态编码器 $E(\\cdot)$ 将物品的语义特征映射到嵌入空间，然后通过分层结构将这些嵌入量化为稀疏的语义ID。\n这种层次化组织的SID可以通过不同层级的前缀（prefix）灵活控制粒度。\n形式化地，给定 $h_i$，量化器（Tokenizer）$Tokenizer(\\cdot): \\mathbb{R}^d \\to {0, 1, \\dots, V}^L$ 将嵌入 $h_i$ 映射为ID序列：\n$SID_i=Tokenizer(h_i)=[SID_i^1,SID_i^2,…,SID_i^L]$ 其中 $V$ 表示每层的词汇大小，$L$ 表示层数。\nGRID 提供了可插拔的模块化设计，方便用户自定义模型或直接使用 HuggingFace 上的模型。\n目前支持三种主要量化器：\nResidual Mini-Batch K-Means (RK-Means) Residual Vector Quantization (R-VQ) Residual Quantized Variational Autoencoder (RQ-VAE) 3.3下一项生成（Next Item Generation） 当所有物品都生成了SID后，针对每个用户序列，GR 框架会使用序列模型生成用户最可能交互的候选SID。\n在 GRID 中，支持多种生成结构：\nEncoder-Decoder 模型； Decoder-only（仅解码器） 模型； 可灵活配置的 Transformer 层数、头数、MoE 专家结构等。 用户可以导入 HuggingFace 上公开的架构或自定义自己的结构。\n默认训练目标是基于 下一token预测（Next-token prediction） 的生成任务，并采用 **滑动窗口增强（sliding window augmentation）。 推理阶段使用带 KV-cache 的 beam search，支持调节 beam width，确保生成的SID是合法的。\nGRID 还内置了多种技巧，比如：\n用户token机制（user token）； 去重（de-duplication）； 避免SID冲突。 4.GRID 实验结果总结 总体设置 数据集：Amazon 5-core（Beauty, Sports, Toys） 评估方式：最后一个交互用于测试，倒数第二个用于验证，其余用于训练。 模型：Flan-T5-Large / XL / XXL 提取语义嵌入。 标记化算法：RK-Means、R-VQ、RQ-VAE。 生成模型：采用 Transformer 架构（共8层，encoder 4层 + decoder 4层）。 评估指标：Recall@K 与 NDCG@K（K = 5, 10）。 4.1 语义ID标记化（Semantic ID Tokenization） 标记化算法（Tokenizer Algorithm） 比较 RQ-VAE、RK-Means、R-VQ 三者。 结果： -**RQ-VAE 虽常被采用，但需同时训练自编码器与量化器，复杂度高。 -RK-Means 与 R-VQ 表现更优，即使在计算量更低的情况下也能获得更好推荐结果。 - 说明复杂的 RQ-VAE 性价比不高。 语义编码器规模（Semantic Encoder Size）\n测试 Flan-T5-Large (780M) → XL (3B) → XXL (11B)。 结果： 模型参数扩大14倍，性能提升却非常有限。 结论： 当前 GR with SID 体系未充分利用更大LLM的语义知识，性能主要受其它模块影响。 标记器层数与维度（SID Tokenizer Dimension） 调整残差层数 L 与每层token数量 W。默认配置 (L, W) = (3, 256)。 结果： 默认配置效果最佳； 层数过多会降低性能——更多层虽可传递更多语义信息，但会造成SID序列不稳定。 存在“语义信息量 vs. 序列可学习性”的权衡。 4.2 生成式推荐（Generative Recommendation） 实验从五个角度研究了 GR with SID 的生成阶段设计。\n用户token数量（User Tokens） 每个用户的SID序列前可加入一个用户token。 结果： 更大的用户词汇表并不提升性能； 完全去掉用户token（即不加个性化标识）反而效果最佳。 结论： 当前 GR with SID 框架中的用户token设计未实现个性化目标。 模型架构：Encoder-Decoder vs. Decoder-only 对比 Transformer 编解码器与仅解码器架构。 结果： Decoder-only 明显性能更差。 Encoder-Decoder 模型更能捕捉用户长期行为的上下文信息。 说明 Encoder 层的上下文建模对生成式推荐至关重要。 数据增强（Data Augmentation） 使用滑动窗口（sliding window）生成多样化序列样本。 结果： 适当的数据增强显著提高泛化能力、减少过拟合； 模型能更好预测多样化和稀疏样本中的下一交互项。 结论： 数据增强是提升 GR 性能的关键手段。 SID去重（De-duplication） 比较两种去重策略： TIGER策略： 在SID末尾加数字； 随机选择策略： 当碰撞时随机选一个。 结果： 两种方式效果接近，TIGER略好但计算代价更高。 TIGER方法需全局SID分布信息，不适合大规模数据。 结论： 简单随机去重在实际场景更高效。 束搜索（Beam Search）策略 对比 约束式 与 非约束式 beam search。 结果： 性能差距极小； 非约束式搜索更快、计算更省； 说明SID生成任务本身的模式约束足以保证生成质量，无需显式约束。 5. 结论（Conclusion） 主要发现： 过去被认为“关键”的组件（如复杂量化器、RQ-VAE、自定义LLM、大量用户token）其实可以被更简单、高效的设计替代； 相反，一些被忽视的因素（如 Encoder-Decoder结构 与 数据增强）却是性能提升的关键； 这些结果为理解 GR with SID 的真正性能驱动因素提供了新的视角。 GRID 的价值： 通过开源的、统一的实验平台，系统揭示了 GR with SID 的核心设计权衡； GRID 框架 能作为标准基准（benchmark）和实验工具，加速后续研究与验证。 ","title":"GRID"},{"link":"/posts/dpo/","text":" KL 散度（Kullback–Leibler Divergence） $$ KL(P | Q) = \\sum_{x \\in X} P(x) \\log \\frac{P(x)}{Q(x)} $$\n含义 P 分布相对于 Q 分布的相似程度。\n性质 KL 散度的值 大于等于 0。 P 和 Q 越相似，KL 散度越接近 0。 如果 P 和 Q 分布完全一致，则 KL 散度 = 0。 注意：\n$$ KL(P | Q) \\neq KL(Q | P) $$ KL 散度大于等于 0 的直观理解 直观理解：KL 散度是一个非负数，因为我们在比较两个分布时， 只有在完全一致时，它们之间的“差异”才为 0。\n$$ KL(P | Q) = \\sum_{x \\in X} P(x) \\log \\frac{P(x)}{Q(x)} $$\n示例 变量 P(x) Q(x) x = 0 0.2 0.8 x = 1 0.8 0.2 计算过程 $$ KL(P | Q) = 0.2 \\times \\log \\frac{0.2}{0.8} + 0.8 \\times \\log \\frac{0.8}{0.2} $$\n直观理解 KL 散度衡量 P 分布相对于 Q 分布的差异程度。 由于对数函数的凸性，KL 散度始终 大于等于 0。 当 P 和 Q 分布完全一致时，KL 散度 等于 0。 若 P 与 Q 差异越大，KL 散度值越大。 Bradley–Terry 模型（成对比较模型） 基本思想 用于描述多个选手（或元素）之间的成对胜负概率。\n假设每个元素 $i$ 都有一个“实力参数” $\\alpha_i \u0026gt; 0$。\n定义： $$ P(i \u0026gt; j) = \\frac{\\alpha_i}{\\alpha_i + \\alpha_j} $$ 其中：\n$\\alpha_i$：第 ( i ) 个元素的真实实力； $P(i \u0026gt; j)$：第 $i$ 个元素战胜第 $j$ 个元素的概率。 对数似然函数（Maximum Likelihood Estimation） 模型给出每次对战的概率：\n$$P(i\u0026gt;j)= \\frac{\\alpha_i}{\\alpha_i + \\alpha_j}$$​​\n我们可以计算出“在当前参数 $\\alpha$下，所有比赛结果同时发生的概率： $$ L(\\boldsymbol{\\alpha}) = \\prod_{\\text{所有比赛}} P(\\text{胜方} \u0026gt; \\text{负方}) $$ 这就是所谓的 似然函数（Likelihood Function）。\n它衡量了：\n给定参数 $\\alpha$，观测到这些比赛结果的可能性有多大”。\n根据观测数据构建似然函数并取对数：\n$$ \\ln L = 8 \\ln\\left(\\frac{\\alpha_A}{\\alpha_A + \\alpha_B}\\right)+ 4 \\ln\\left(\\frac{\\alpha_B}{\\alpha_A + \\alpha_B}\\right)+ 3 \\ln\\left(\\frac{\\alpha_A}{\\alpha_A + \\alpha_C}\\right)+ 5 \\ln\\left(\\frac{\\alpha_C}{\\alpha_A + \\alpha_C}\\right) $$\n通过最大化对数似然，可以求得各选手的相对实力参数。\n参数估计结果 $$ \\alpha_A = 1, \\quad \\alpha_B = \\frac{1}{2}, \\quad \\alpha_C = \\frac{5}{3} $$ 指定$\\alpha_A$ 的值，可的得到剩下两个的值\n推导新的对战概率 例如计算 ( P(B \u0026gt; C) )：\n$$ P(B \u0026gt; C) = \\frac{\\alpha_B}{\\alpha_B + \\alpha_C} = \\frac{1/2}{1/2 + 5/3} \\approx 0.23 $$\n直观理解：\n模型通过比较胜负记录估计出每个选手的“潜在实力”； 实力越大，战胜其他人的概率越高； 可推广用于比赛预测、排序、推荐系统等任务。 一般的 Loss 函数（Bradley–Terry 模型） 模型目标是最大化对数似然，对应的最小化损失函数（负对数似然）为：\n$$ Loss = - \\mathbb{E}_{(a_x, a_y) \\sim D} \\left[ \\ln \\frac{a_x}{a_x + a_y} \\right] $$\n强化学习中的比较建模（Pairwise Preference in RLHF） 在强化学习中：\n大模型的输入是 prompt，记作 $x$； 模型的输出（回答）是 response，记作 $y$； 回答 $y$ 的好坏（即“得分”或“实力”）由 Reward 模型 $r(x, y)$ 进行评估。 比较两个回答的优劣概率 对于同一个输入 $x$，有两个不同回答 $y_1$ 和 $y_2$。\n原始形式为： $$ P(y_1 \\succ y_2) = \\frac{r(x, y_1)}{r(x, y_1) + r(x, y_2)} $$\n由于 $r(x, y)$ 可能返回负数，因此引入指数函数，使概率始终为正：\n$$ P(y_1 \\succ y_2) = \\frac{\\exp(r(x, y_1))}{\\exp(r(x, y_1)) + \\exp(r(x, y_2))} $$\n说明 $r(x, y)$：Reward 模型给出的得分（越大表示回答越好）； $P(y_1 \\succ y_2)$：模型预测回答 $y_1$ 优于 $y_2$ 的概率； 该形式本质上是 Bradley–Terry 模型 在强化学习（RLHF）中的应用。 偏好概率建模 对于同一输入 $x$，Reward 模型给出两个回答 $y_w$（获胜回答）和 $y_l$（失败回答）的得分： $$ P(y_w \\succ y_l) = \\frac{\\exp(r(x, y_w))}{\\exp(r(x, y_w)) + \\exp(r(x, y_l))} $$\nSigmoid 形式表示 Sigmoid 函数定义为： $$ \\sigma(x) = \\frac{1}{1 + \\exp(-x)} $$\n损失函数（Loss Function） Reward 模型的目标是最小化负对数似然（Negative Log-Likelihood）：\n$$ Loss = -\\mathbb{E}_{(x, y_w, y_l) \\sim D} \\left[ \\ln \\frac{\\exp(r(x, y_w))}{\\exp(r(x, y_w)) + \\exp(r(x, y_l))} \\right] $$\n等价地：\n$$ Loss = - \\mathbb{E}_{(x, y_w, y_l) \\sim D} \\left[ \\ln \\frac{1}{1 + \\exp(r(x, y_l) - r(x, y_w))} \\right] $$\n最终可写为简洁的 Sigmoid 形式： $$ Loss = - \\mathbb{E}_{(x, y_w, y_l) \\sim D} \\left[ \\ln \\sigma(r(x, y_w) - r(x, y_l)) \\right] $$\nDPO 的训练目标（Direct Preference Optimization） 基本定义 奖励函数：$r(x, y)$，其中 $x$ 为 prompt，$y$ 为 response 基准模型（reference model）：$\\pi_{\\text{ref}}(y|x)$ 训练模型（policy model）：$\\pi(y|x)$ DPO 的目标是在获得高奖励的同时，使新模型不偏离参考模型：\n$$ \\max_{\\pi} \\; \\mathbb{E}_{x\\sim D, y\\sim \\pi(y|x)}[r(x,y)] -\\beta \\mathbb{D}_{\\mathrm{KL}}\\big(\\pi(y|x)|\\pi_{\\mathrm{ref}}(y|x)\\big) $$\n第一项：希望得到尽可能多的奖励； 第二项：限制新模型与基准模型的分布差距； $\\beta$：超参数，用于平衡两者。 目标函数的等价形式推导 将 KL 散度展开：\n$$ \\max_{\\pi} \\mathbb{E}_{x, y \\sim \\pi} [r(x, y)] - \\beta , \\mathbb{E}_{x, y \\sim \\pi} \\left[\\log \\frac{\\pi(y|x)}{\\pi_{\\text{ref}}(y|x)} \\right] $$\n等价地（加负号取最小值且两项同时除以$\\beta$）：\n$$ \\min_{\\pi} \\mathbb{E}_{x, y \\sim \\pi} \\left[\\log \\frac{\\pi(y|x)}{\\pi_{\\text{ref}}(y|x)} - \\frac{1}{\\beta} r(x, y)\\right] $$\n将减法项写成对数形式\n$$ = \\min_{\\pi} ; \\mathbb{E}_{x, y \\sim \\pi} \\left[ \\log \\frac{\\pi(y|x)}{\\pi_{\\text{ref}}(y|x)} - \\log \\exp!\\left(\\frac{1}{\\beta} r(x, y)\\right) \\right] $$ 合并为单个对数项\n$$ = \\min_{\\pi} ; \\mathbb{E}_{x, y \\sim \\pi} \\left[ \\log \\frac{\\pi(y|x)} {\\pi_{\\text{ref}}(y|x) \\exp!\\left(\\frac{1}{\\beta} r(x, y)\\right)} \\right] $$\n引入归一化常数 $Z(x)$\n$$ = \\min_{\\pi} ; \\mathbb{E}_{x, y \\sim \\pi} \\left[ \\log \\frac{\\pi(y|x)} {\\pi_{\\text{ref}}(y|x) \\exp!\\left(\\frac{1}{\\beta} r(x, y)\\right) \\frac{1}{Z(x)} Z(x)} \\right] $$\n拆出常数项\n$$ =\\min_{\\pi} ; \\mathbb{E}_{x, y \\sim \\pi} \\left[ \\log \\frac{\\pi(y|x)}{\\frac{1}{Z(x)} \\pi_{\\text{ref}}(y|x) \\exp!\\left(\\frac{1}{\\beta} r(x, y)\\right)} -\\log Z(x) \\right] $$\n求解最优策略分布 定义：\n$$ \\pi^*(y|x) = \\frac{1}{Z(x)} , \\pi_{\\text{ref}}(y|x) \\exp!\\left(\\frac{1}{\\beta} r(x, y)\\right) $$\n其中：\n$$ Z(x) = \\sum_y \\pi_{\\text{ref}}(y|x) \\exp!\\left(\\frac{1}{\\beta} r(x, y)\\right) $$\n表示归一化常数，是我们自己定义的，这么定义的原因是为了能和后面的式子消去，简化式子。\n解释：最优策略 $\\pi^*(y|x)$ 是在参考模型分布上，通过奖励函数加权后的归一化分布。\n代入 $\\pi^*(y|x)$，目标可写为：\n$$ \\min_{\\pi} \\mathbb{E}_{x \\sim D} \\left[ D_{KL}(\\pi(y|x) | \\pi^*(y|x)) \\right] $$\n因此最优时： $$ \\pi(y|x) = \\pi^*(y|x) = \\frac{1}{Z(x)} \\pi_{\\text{ref}}(y|x) \\exp\\left(\\frac{1}{\\beta} r(x, y)\\right) $$\n奖励函数反求形式 由上式反推奖励： 由 DPO 的最优策略定义：\n$$ \\pi^*(y|x) = \\frac{1}{Z(x)} \\pi_{\\text{ref}}(y|x) \\exp \\left(\\frac{1}{\\beta} r(x, y)\\right) $$\n可得：\n$$ \\exp\\left(\\frac{1}{\\beta} r(x, y)\\right) = \\frac{\\pi(y|x)}{\\pi_{\\text{ref}}(y|x)} Z(x) $$\n两边取对数并乘以 $\\beta$，得到：\n$$ r(x, y) = \\beta \\ln!\\left( \\frac{\\pi(y|x)}{\\pi_{\\text{ref}}(y|x)} Z(x) \\right) $$ 即：\n$$ r(x, y) = \\beta \\ln \\frac{\\pi(y|x)}{\\pi_{\\text{ref}}(y|x)} + \\beta \\ln Z(x) $$\n解释：\n奖励函数可以理解为模型输出相对参考模型的“对数优势”。 它衡量了当前模型相对参考模型在特定回答上的改进幅度\n与偏好建模的联系 在成对偏好比较中，假设有更优回答 $y_w$ 与较差回答 $y_l$，则：\n$$ -\\ln \\sigma(r(x, y_w) - r(x, y_l)) = - \\ln \\sigma!\\left( \\beta \\ln \\frac{\\pi(y_w|x)}{\\pi_{\\text{ref}}(y_w|x)} - \\beta \\ln \\frac{\\pi(y_l|x)}{\\pi_{\\text{ref}}(y_l|x)} \\right) $$\n这表明 DPO 的优化目标实际上是将 Reward 模型的比较学习 转换为对 策略分布比值 的直接优化。\n总结 DPO的核心洞察在于原始强化学习问题存在解析最优解，表明最优策略与奖励函数存在一一映射关系。DPO将此关系反解后代入Bradley-Terry偏好模型，将对奖励函数的似然最大化，等价地转化为直接对策略的似然最大化。因此，优化DPO损失函数即是在直接寻找那个能同时最大化人类偏好概率且满足最优解形式的策略，避免了先用偏好数据拟合奖励模型再进行强化学习过程寻找最优策略\n","title":"LLM-RL-DPO"},{"link":"/posts/ppo/","text":" 基础概念I 如果你不熟悉强化学习，学习ppo了解这些基础知识就足够了 Action Space: 可选择的动作，比如 {left, up, right}\nPolicy: 策略函数，输入 State，输出 Action 的概率分布。一般用 π 表示。\n$\\pi(left|s_t) = 0.1$,在状态$s_t$下，采取的动作为left的概率为0.1 $\\pi(up|s_t) = 0.2$ , $\\pi(right|s_t) = 0.7$, 这个状态下的 所有动作 概率之和 为1 Trajectory: 轨迹，用$τ$表示，一连串状态和动作的序列。又称Episode, Rollout。 ${s0,a0,s1,a1,…}$\n$s_{t+1}=f(st,at)$ 确定状态转移\n$s_{t+1}=P(⋅|st,at)$随机状态转移\nReturn: 回报，从当前时间点到游戏结束的 Reward 的累积和。\n期望：每个可能结果的概率与其结果值的乘积之和\n$\\mathbb{E}(x)_{x \\sim p(x)}=\\sum x \\cdotp(x)\\approx \\frac{1}{n} \\sum_{i=1}^n x, \\quad x \\sim p(x)$\n$\\mathbb{E}[f(\\tau)] \\approx \\frac{1}{N} \\sum_{n=1}^N f(\\tau^n)$\nNote 运用的是蒙特卡洛思想，从分布p(x)中随机采样n次求平均，n趋于无穷时样本平均会趋近于期望值/大数定律，期望可以用样本平均近似\n轨迹概率：一条轨迹 $\\tau = (s_0, a_0, s_1, a_1, \\dots, s_T, a_T)$的概率是： $P_\\theta(\\tau) = P(s_0, a_0, s_1, a_1, \\dots, s_T, a_T)$ 概率论基本公式： $P(x_1, x_2, \\dots, x_n) = \\prod_{i=1}^n P(x_i \\mid x_1, \\dots, x_{i-1})$ 把它用在轨迹上，就可以逐步展开： $P_\\theta(\\tau) = P(s_0) \\cdot P(a_0 \\mid s_0) \\cdot P(s_1 \\mid s_0, a_0) \\cdot P(a_1 \\mid s_1,a_0,s_0) \\cdot \\dots$ 在强化学习里，环境通常假设是 马尔可夫决策过程 (MDP)，即：\n下一状态只依赖于当前状态和动作： $P(s_{t+1} \\mid s_0, a_0, \\dots, s_t, a_t) = P(s_{t+1} \\mid s_t, a_t)$ 动作只依赖于当前状态： $P(a_t \\mid s_0, a_0, \\dots, s_t) = \\pi_\\theta(a_t \\mid s_t)$ 于是上面的展开可以简化为： $\\ P_\\theta(\\tau) = P(s_0) \\prod_{t=0}^T \\pi_\\theta(a_t \\mid s_t) , P(s_{t+1} \\mid s_t, a_t)$ $\\pi_\\theta(a_n^t | s_n^t)$：策略在状态 $s_n^t$下选择动作 $a_n^t$ 的概率（依赖参数 $\\theta$）。 $P(s_{n}^{t+1}|s_n^t, a_n^t)$：环境的转移概率（不依赖 $\\theta$）。 Important 目标： 训练一个Policy神经网络 $π_{\\theta}$ ,在所有的状态S下，给出相应的Action，得到的Return的期望最大 or 训练一个Policy神经网络 $\\pi_{\\theta}$ ,在所有Trajectory中，得到的Return的期望最大\nPPO原理-part1 我们将刚才的目标翻译成数学表达式，就是希望 $E(R(\\tau))_{\\tau \\sim P_\\theta(\\tau)} = \\sum_{\\tau} R(\\tau) P_\\theta(\\tau)$ 越大越好\n$P_\\theta(\\tau)$ :在策略$\\pi_\\theta$下，采样到轨迹 $\\tau$ 的概率。 $R(\\tau)$：轨迹 $\\tau$ 的总回报（Return）。 Note 为什么$R(\\tau)$不受$\\theta$影响？ $R(τ)=∑_{t=0}^{T−1}​γ^{t}r(s_t​,a_t​)$ 因为一旦一条轨迹被确定下来，这条轨迹的回报R是由每个状态下的每个动作的得分之和决定的，又因为越远的动作影响对当前状态影响越小，所以要乘上一个衰减因子$\\gamma$\n我们只能改变神经网络里的参数 θ ,不能改变Reward，所以对 θ求梯度\n$$ \\nabla E(R(\\tau))_{\\tau \\sim P_{\\theta}(\\tau)} $$\n$$ = \\nabla \\sum_{\\tau} R(\\tau) P_\\theta(\\tau)$$ $$= \\sum_{\\tau} R(\\tau) \\nabla P_\\theta(\\tau) \\ $$$$= \\sum_{\\tau} R(\\tau) \\nabla P_\\theta(\\tau) \\frac{P_\\theta(\\tau)}{P_\\theta(\\tau)} \\ $$ $$ = \\sum_{\\tau} P_\\theta(\\tau) R(\\tau) \\frac{\\nabla P_\\theta(\\tau)}{P_\\theta(\\tau)} $$ $$\\approx \\frac{1}{N} \\sum_{n=1}^N R(\\tau^n) \\frac{\\nabla P_\\theta(\\tau^n)}{P_\\theta(\\tau^n)} $$\nTip $\\tau^n$表示第n条轨迹（trajectory）;多次采样取平均\n$$ = \\frac{1}{N} \\sum_{n=1}^N R(\\tau^n) \\nabla \\log P_\\theta(\\tau^n) \\ \\ $$\nTip 因为$\\nabla \\log f(x) = \\frac{\\nabla f(x)}{f(x)}$\n$$= \\frac{1}{N} \\sum_{n=1}^N R(\\tau^n) \\nabla \\log \\prod_{t=1}^{T_n} P_\\theta(a_n^t | s_n^t)$$\nTip 这里是轨迹分解，因为对 $\\theta$ 求梯度只关注 $P_\\theta$ ,就是前面基础知识中的 $\\pi_\\theta$,环境转移概率不包含 $\\theta$ ,故当作常数，求梯度就没了\n$$=\\frac{1}{N} \\sum_{n=1}^N R(\\tau^n) \\sum_{t=1}^{T_n} \\nabla \\log P_\\theta(a_n^t | s_n^t)\\ = \\frac{1}{N} \\sum_{n=1}^N \\sum_{t=1}^{T_n} R(\\tau^n),\\nabla \\log P_\\theta(a_n^t | s_n^t)$$\n到这里，我们求出了对于所有可能的Trajectory，期望的最大梯度，用个梯度去更新神经网络参数，就是 Policy gradient梯度策略算法\nPPO原理-part2 $$ \\frac{1}{N} \\sum_{n=1}^N \\sum_{t=1}^{T_n} R(\\tau^n),\\nabla \\log P_\\theta(a_n^t | s_n^t) $$ 刚刚我们得到的这个式子，很明显有两点可以改进\n应该看在$S_n$状态下采取动作$a_n$之后的Reward，而不是整个Trajectory（轨迹）的Reward，因为动作action只能影响后面的 action是只会影响后面的动作，但是影响会逐步衰减 由此我们修改Reward公式为$$R(\\tau^n) \\to \\sum_{t\u0026rsquo;=t}^{T_n} \\gamma^{t\u0026rsquo;-t}r_{t\u0026rsquo;}^n = R_t^n$$ 表示从当前t时刻开始到最后的累积Reward， $\\gamma$ 是衰减因子，表示离当前动作越远，在当前状态下采取当前动作的Reward越小，核心就是想突出当前状态下采取当前动作对于Reward的影响.\n另一个值得注意的点是局势也会影响算法的稳定性，比如说在好的局势下，采取什么动作都会得分（顺风局当赢的感觉），但是这样就偏离我们的初衷——我们想知道在某种状态下采取哪些动作更好，模型需要明确区分哪个动作“更好”\n因此我们希望让相对好的action的Reward得分增加，相对差的action的Reward减少，这样会加快训练速度。\n由此我们减去一个Baseline\n$$ \\frac{1}{N} \\sum_{n=1}^N \\sum_{t=1}^{T_n} \\bigl(R_t^n - B(s_n^t)\\bigr) \\nabla \\log P_\\theta(a_n^t \\mid s_n^t)$$\n基础概念II 强化学习中有几个重要的定义，可以简化上述式子\n$\\textbf{Action-Value Function}(动作价值函数)$ $Q_\\theta(s,a) 在 state \\ \\ s 下，做出 Action \\ \\ a的回报的期望$\n$\\textbf{State-Value Function} (状态价值函数)$ $V_\\theta(s) 在 state \\ \\ s 下，回报的期望$。\n$\\textbf{Advantage Function}(优势函数)$ $A_\\theta(s,a) = Q_\\theta(s,a) - V_\\theta(s) \\quad$ $在 state \\ \\ s 下，做出 Action \\ \\ a，比其他动作能带来多少优势。$于是原式被转化为： $$\\frac{1}{N} \\sum_{n=1}^{N} \\sum_{t=1}^{T_n} A_\\theta(s_n^t, a_n^t) \\nabla \\log P_\\theta(a_n^t | s_n^t)$$ 现在我们得到了理论上表达式，但是如何表示实际采样的呢？\n一次采样：\n$Q_\\theta(s_t, a) = r_t + \\gamma \\cdot V_\\theta(s_{t+1})$\n$A_\\theta(s_t, a) = r_t + \\gamma \\cdot V_\\theta(s_{t+1}) - V_\\theta(s_t)$\n$V_\\theta(s_{t+1}) \\approx r_{t+1} + \\gamma \\cdot V_\\theta(s_{t+2})$\nTip 上面我们分别对动作价值函数和状态价值函数进行了一次采样，对于动作价值函数Q来说，采用的 动作是固定的，因此可以用等号来表示，对于状态价值函数，采取的动作a还没有固定，因此用约等于表示一次采样\n多次采样，虽然会增大方差，但是会减少偏差 $$ A_\\theta^1(s_t, a) = r_t + \\gamma V_\\theta(s_{t+1}) - V_\\theta(s_t) $$\n$$ A_\\theta^2(s_t, a) = r_t + \\gamma r_{t+1} + \\gamma^2 V_\\theta(s_{t+2}) - V_\\theta(s_t) $$\n$$ A_\\theta^3(s_t, a) = r_t + \\gamma r_{t+1} + \\gamma^2 r_{t+2} + \\gamma^3 V_\\theta(s_{t+3}) - V_\\theta(s_t) $$\n$$ \\vdots $$\n$$ A_\\theta^T(s_t, a) = r_t + \\gamma r_{t+1} + \\gamma^2 r_{t+2} + \\gamma^3 r_{t+3} + \\cdots + \\gamma^T r_T - V_\\theta(s_t) $$ 定义新函数，简化表示 $$ \\delta_t^V = r_t + \\gamma V_\\theta(s_{t+1}) - V_\\theta(s_t) $$\n$$ \\delta_{t+1}^V = r_{t+1} + \\gamma V_\\theta(s_{t+2}) - V_\\theta(s_{t+1}) $$\n$$ A_\\theta^1(s_t, a) = \\delta_t^V $$\n$$ A_\\theta^2(s_t, a) = \\delta_t^V + \\gamma \\delta_{t+1}^V $$\n$$ A_\\theta^3(s_t, a) = \\delta_t^V + \\gamma \\delta_{t+1}^V + \\gamma^2 \\delta_{t+2}^V $$\n$$ \\vdots $$ 所以算法（比如 GAE）会综合使用多个步长 k 的估计，构建出一个平衡版本（平衡方差和偏差）： 多次采样，并乘上对应衰减权重 $$ A_\\theta^{GAE}(s_t, a) = (1 - \\lambda)\\big(A_\\theta^1 + \\lambda A_\\theta^2 + \\lambda^2 A_\\theta^3 + \\cdots \\big) $$\n$$ \\lambda = 0.9: \\quad A_\\theta^{GAE} = 0.1 A_\\theta^1 + 0.09 A_\\theta^2 + 0.081 A_\\theta^3 + \\cdots $$\n$$ = (1 - \\lambda)\\big(\\delta_t^V + \\lambda(\\delta_t^V + \\gamma \\delta_{t+1}^V) + \\lambda^2(\\delta_t^V + \\gamma \\delta_{t+1}^V + \\gamma^2 \\delta_{t+2}^V) + \\cdots \\big) $$\n$$ = (1 - \\lambda)\\big(\\delta_t^V(1 + \\lambda + \\lambda^2 + \\cdots) + \\gamma \\delta_{t+1}^V (\\lambda + \\lambda^2 + \\cdots) + \\cdots \\big) $$\n$$ = (1 - \\lambda)\\big(\\delta_t^V \\tfrac{1}{1-\\lambda} + \\gamma \\delta_{t+1}^V \\tfrac{\\lambda}{1-\\lambda} + \\cdots \\big) $$\n$$ = \\sum_{b=0}^{\\infty} (\\gamma \\lambda)^b , \\delta_{t+b}^V $$ 最后得到这仨表达式 $$\\delta_t^V = r_t + \\gamma V_\\theta(s_{t+1}) - V_\\theta(s_t) $$$$A_\\theta^{GAE}(s_t, a) = \\sum_{b=0}^{\\infty} (\\gamma \\lambda)^b , \\delta_{t+b}^V $$ $$\\frac{1}{N} \\sum_{n=1}^N \\sum_{t=1}^{T_n}A_\\theta^{GAE}(s_n^t, a_n^t) \\nabla \\log P_\\theta(a_n^t \\mid s_n^t)$$\nPPO原理-part3 通过运行模型来采集数据，这样就会导致采集数据时间过长，这也是ppo需要解决的问题,对于此我们采用重要性采样来解决\n重要性采样 $$ \\mathbb{E}(f(x))_{x \\sim p(x)} = \\sum_x f(x) p(x) $$\n$$ = \\sum_x f(x) p(x) \\frac{q(x)}{q(x)} $$\n$$ = \\sum_x f(x) \\frac{p(x)}{q(x)} q(x) $$\n$$ = \\mathbb{E}!\\left(f(x) \\frac{p(x)}{q(x)}\\right)_{x \\sim q(x)} $$\n$$ \\approx \\frac{1}{N} \\sum_{n=1}^N f(x) \\frac{p(x)}{q(x)}, \\quad x \\sim q(x) $$\n由此我们可以变换我们的公式，由On-Policy 转向Off-Policy\n$$ \\frac{1}{N} \\sum_{n=1}^N \\sum_{t=1}^{T_n} A_{\\theta}^{GAE}(s_n^t, a_n^t) \\nabla \\log P_\\theta(a_n^t \\mid s_n^t) $$\n$$ = \\frac{1}{N} \\sum_{n=1}^N \\sum_{t=1}^{T_n} A_{\\theta’}^{GAE}(s_n^t, a_n^t) \\frac{P_\\theta(a_n^t \\mid s_n^t)}{P_{\\theta’}(a_n^t \\mid s_n^t)} \\nabla \\log P_\\theta(a_n^t \\mid s_n^t) $$\n$$ = \\frac{1}{N} \\sum_{n=1}^N \\sum_{t=1}^{T_n} A_{\\theta’}^{GAE}(s_n^t, a_n^t) \\frac{\\nabla P_\\theta(a_n^t \\mid s_n^t)}{P_{\\theta’}(a_n^t \\mid s_n^t)} $$\n这里我们的做法是用旧策略(off-policy)的优势函数乘一个比例系数去模拟新的策略函数(on-policy)\n最后去掉梯度我们可以得到最终的loss函数（中间对$\\theta$求梯度是为了消去不影响$theta$的状态转移概率） $$\\text{Loss} = -\\frac{1}{N} \\sum_{n=1}^N \\sum_{t=1}^{T_n} A_{\\theta’}^{GAE}(s_n^t, a_n^t) \\frac{P_\\theta(a_n^t \\mid s_n^t)}{P_{\\theta’}(a_n^t \\mid s_n^t)}$$ 注意：旧策略与新策略之间的分布不能差距过大，否则很难学到有用的经验\n可以采用KL散度进行约束或者clip函数（限定那个新旧两种策略之间的比例在1左右，不能相差过大）\n$$\\text{Loss}_{ppo_1} = -\\frac{1}{N} \\sum_{n=1}^N \\sum_{t=1}^{T_n} A_{\\theta\u0026rsquo;}^{GAE}(s_n^t, a_n^t) \\frac{P_\\theta(a_n^t \\mid s_n^t)}{P_{\\theta\u0026rsquo;}(a_n^t \\mid s_n^t)} + \\beta , KL(P_\\theta, P_{\\theta\u0026rsquo;})$$\n$$\\text{Loss}_{ppo_2} = -\\frac{1}{N} \\sum_{n=1}^N \\sum_{t=1}^{T_n} \\min \\Bigg( A_{\\theta\u0026rsquo;}^{GAE}(s_n^t, a_n^t) \\frac{P_\\theta(a_n^t \\mid s_n^t)}{P_{\\theta\u0026rsquo;}(a_n^t \\mid s_n^t)} , \\text{clip}\\left( \\frac{P_\\theta(a_n^t \\mid s_n^t)}{P_{\\theta\u0026rsquo;}(a_n^t \\mid s_n^t)}, 1-\\epsilon,1+\\epsilon \\right) A_{\\theta\u0026rsquo;}^{GAE}(s_n^t, a_n^t) \\Bigg)$$\n","title":"LLM-RL-PPO"},{"link":"/posts/test-alert/","text":"GitHub Style Alert Testing This article is used to test the new GitHub-style Alert feature and folding functionality.\nAlert Syntax Note Alert Note This is a note alert box. Used to display useful information that users should be aware of, even when quickly browsing the content.\nTip Alert Tip This is a tip alert box. Provides suggestions that help complete tasks better or more easily.\nImportant Alert Important This is an important alert box. Displays critical information users need to know to achieve their goals.\nWarning Alert Warning This is a warning box. Urgent information that requires immediate user attention to avoid problems.\nCaution Alert Caution This is a caution alert box. Advises users to be aware of the risks or negative consequences of certain behaviors.\nExtended Syntax - Custom Titles Note with Custom Title Custom Title This is a note alert box with a custom title.\nWarning with Custom Title Radiation Hazard Do not approach or handle without protective equipment.\nFolding Feature Expanded Foldable Alert by Default Click to Collapse This is an expanded foldable alert box by default. Click the title to collapse the content.\nSupports multi-line content:\nList item 1 List item 2 List item 3 Collapsed Alert by Default Important Information (Collapsed by Default) This is an important information box collapsed by default. Click the title to expand and view the content.\nCan include:\nOrdered list Bold text Italic text Code snippet Foldable Alert with Complex Content Complex Content Example This foldable box contains complex Markdown content:\nSubheading This is a paragraph containing a link and other formatting.\nJAVASCRIPT Collapse Copy // Code block example function hello() { console.log(\u0026#34;Hello, World!\u0026#34;); } Click to expand and view more Table Example Row1 Data1 Row2 Data2 Regular Blockquote This is a regular blockquote, not an Alert:\nThis is a standard blockquote. It won\u0026rsquo;t be rendered as an Alert but will use the standard blockquote styling.\nSupports multi-line content and formatted text.\nMultilingual Support Alerts support multiple languages, and titles will automatically display in the current language:\nNote In a Chinese environment, this title will display as \u0026ldquo;注意\u0026rdquo; (Note).\nTip In a Chinese environment, this title will display as \u0026ldquo;提示\u0026rdquo; (Tip).\nNested Content Test Nested Content Test This Alert contains nested content:\nThis is a nested blockquote\nList item Nested list item Another nested item Ordered list Nested ordered list Another nested item ","title":"GitHub Style Alert Test"},{"link":"/posts/katex-and-mermaid-test/","text":"KaTeX and Mermaid Test This article is used to test the KaTeX and Mermaid features.\nConfiguration Frontmatter Configuration YAML Collapse Copy --- katex: true mermaid: true --- Click to expand and view more Global Configuration YAML Collapse Copy # hugo.yaml katex: enabled: true delimiters: - left: \u0026#34;$$\u0026#34; right: \u0026#34;$$\u0026#34; display: true - left: \u0026#34;$\u0026#34; right: \u0026#34;$\u0026#34; display: false mermaid: enabled: true Click to expand and view more KaTeX Test Inline Formula This is an inline formula: $E = mc^2$, Einstein\u0026rsquo;s mass-energy equivalence formula.\nAnother example：When $a \\neq 0$, the solutions to the quadratic equation $ax^2 + bx + c = 0$ are $x = \\frac{-b \\pm \\sqrt{b^2-4ac}}{2a}$.\nBlock Formula Quadratic Formula $$x = \\frac{-b \\pm \\sqrt{b^2-4ac}}{2a}$$\nEuler\u0026rsquo;s Formula $$e^{i\\pi} + 1 = 0$$\nIntegral Formula $$\\int_{-\\infty}^{\\infty} e^{-x^2} dx = \\sqrt{\\pi}$$\nMatrix Representation $$\\begin{pmatrix} a \u0026amp; b \\\\ c \u0026amp; d \\end{pmatrix} \\begin{pmatrix} x \\\\ y \\end{pmatrix} = \\begin{pmatrix} ax + by \\\\ cx + dy \\end{pmatrix}$$\nSummation Formula $$\\sum_{n=1}^{\\infty} \\frac{1}{n^2} = \\frac{\\pi^2}{6}$$\nCommon Mathematical Symbols Test Using predefined macros: $\\RR$, $\\NN$, $\\ZZ$, $\\QQ$, $\\CC$\n","title":"KaTeX and Mermaid Test"},{"link":"/posts/image-rendering-test/","text":"Image Rendering Test The Demo comes from LightGallery All images are from Unsplash Single Image Photo by - Daniel Leone Justified Gallery Photo by - Tobias Rademacher Photo by - Massimiliano Morosinotto Photo by - Sascha Bosshard Photo by - Yusuf Evli Photo by - Jay Mantri Photo by - Florian van Duyn Photo by - Juan Davila Photo by - David Marcu Masonry by shortcodes ","title":"Image Rendering Test"},{"link":"/posts/code-highlighting-test/","text":"Code Highlighting Test This article is used to test the new code highlighting feature, including syntax highlighting, copy button, language display, etc.\nJavaScript JAVASCRIPT Collapse Copy function fibonacci(n) { if (n \u0026lt;= 1) return n; return fibonacci(n - 1) + fibonacci(n - 2); } const result = fibonacci(10); console.log(`The 10th Fibonacci number is: ${result}`); // Async/Await const asyncFunction = async () =\u0026gt; { try { const response = await fetch(\u0026#39;/api/data\u0026#39;); const data = await response.json(); return data; } catch (error) { console.error(\u0026#39;Error fetching data:\u0026#39;, error); } }; Click to expand and view more Codeblock with Line Numbers PYTHON Collapse Copy 1# Python with line numbers 2import asyncio 3from typing import List, Optional 4 5class DataProcessor: 6 def __init__(self, data: List[dict]): 7 self.data = data 8 9 def process(self) -\u0026gt; Optional[dict]: 10 \u0026#34;\u0026#34;\u0026#34;Process the data and return the result\u0026#34;\u0026#34;\u0026#34; 11 if not self.data: 12 return None 13 14 result = { 15 \u0026#39;total\u0026#39;: len(self.data), 16 \u0026#39;processed\u0026#39;: [] 17 } 18 19 for item in self.data: 20 if self.validate_item(item): 21 result[\u0026#39;processed\u0026#39;].append(item) 22 23 return result Click to expand and view more Highlighting Specific Lines GO Collapse Copy 1package main 2 3import \u0026#34;fmt\u0026#34; // This line will be highlighted 4 5func main() { 6 message := \u0026#34;Hello, World!\u0026#34; // This line will also be highlighted 7 8 fmt.Println(message) // This line will also be highlighted 9 10 for i := 0; i \u0026lt; 3; i++ { 11 fmt.Printf(\u0026#34;Count: %d\\n\u0026#34;, i) 12 } 13} Click to expand and view more Codeblock with Filename api.ts Collapse Copy // TypeScript API interface ApiResponse\u0026lt;T\u0026gt; { data: T; status: number; message: string; } interface User { id: number; name: string; email: string; avatar?: string; } class ApiClient { private baseURL: string; private headers: Record\u0026lt;string, string\u0026gt;; constructor(baseURL: string, apiKey?: string) { this.baseURL = baseURL; this.headers = { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, ...(apiKey \u0026amp;\u0026amp; { \u0026#39;Authorization\u0026#39;: `Bearer ${apiKey}` }) }; } async get\u0026lt;T\u0026gt;(endpoint: string): Promise\u0026lt;ApiResponse\u0026lt;T\u0026gt;\u0026gt; { const response = await fetch(`${this.baseURL}${endpoint}`, { method: \u0026#39;GET\u0026#39;, headers: this.headers, }); if (!response.ok) { throw new Error(`HTTP error! status: ${response.status}`); } return response.json(); } async post\u0026lt;T\u0026gt;(endpoint: string, data: any): Promise\u0026lt;ApiResponse\u0026lt;T\u0026gt;\u0026gt; { const response = await fetch(`${this.baseURL}${endpoint}`, { method: \u0026#39;POST\u0026#39;, headers: this.headers, body: JSON.stringify(data), }); return response.json(); } } const client = new ApiClient(\u0026#39;https://api.example.com\u0026#39;, \u0026#39;your-api-key\u0026#39;); async function getUsers(): Promise\u0026lt;User[]\u0026gt; { try { const response = await client.get\u0026lt;User[]\u0026gt;(\u0026#39;/users\u0026#39;); return response.data; } catch (error) { console.error(\u0026#39;Error fetching users:\u0026#39;, error); return []; } } Click to expand and view more Plain Text Codeblock PLAINTEXT Collapse Copy This is a plain text codeblock. It should not have syntax highlighting. You can test the copy functionality here. function test() { console.log(\u0026#34;This is a test.\u0026#34;); } Click to expand and view more Inline Code This is an inline code example：const x = 42; and npm install and git commit -m \u0026quot;update\u0026quot;.\n","title":"Code Highlighting Test"},{"link":"/posts/markdown-syntax-test-document/","text":"Heading 1 This is a paragraph under a level 1 heading.\nHeading 2 This is a paragraph under a level 2 heading.\nHeading 3 This is a paragraph under a level 3 heading.\nHeading 4 This is a paragraph under a level 4 heading.\nHeading 5 This is a paragraph under a level 5 heading.\nHeading 6 This is a paragraph under a level 6 heading.\nParagraphs and Text Formatting This is a normal paragraph. It can contain bold text, italic text, bold italic text, strikethrough, inline code, and link text .\nThis is another paragraph to test spacing between paragraphs.\nBlockquotes This is a simple blockquote.\nBlockquotes can contain multiple paragraphs.\nThis is an example of a nested blockquote:\nThis is nested quote content.\nMultiple levels of nesting are possible.\nLists Unordered List First item Second item Nested item 1 Nested item 2 Even deeper nested item Third item Ordered List First item Second item Nested ordered item 1 Nested ordered item 2 Even deeper nested item Third item Task List (Checkbox) Completed task Incomplete task Another completed task Nested task list Subtask 1 (done) Subtask 2 (not done) Subtask 3 (done) Definition List Term 1 This is the definition for term 1. Term 2 This is the definition for term 2. Terms can have multiple definitions. Code Inline Code This is a paragraph with console.log('Hello World') inside.\nCode Blocks JAVASCRIPT Collapse Copy function greet(name) { console.log(`Hello, ${name}!`); } greet(\u0026#39;World\u0026#39;); Click to expand and view more PYTHON Collapse Copy def fibonacci(n): if n \u0026lt;= 1: return n return fibonacci(n-1) + fibonacci(n-2) print(fibonacci(10)) Click to expand and view more CSS Collapse Copy .prose { max-width: none; color: var(--tw-prose-body); } .prose h1 { font-size: 2.25rem; font-weight: 700; } Click to expand and view more Tables Left Align Center Align Right Align Content 1 Content 2 Content 3 Longer content Medium Short Data A Data B Data C Horizontal Rule Images Sample Image Links This is a regular link .\nThis is a link with title .\nThis is a reference-style link: Reference Link Footnotes This is a paragraph with a footnote1.\nHere is another footnote2.\nHighlighted Text This is a paragraph with highlighted text.\nSuperscript and Subscript H~2~O is the chemical formula for water.\nE = mc^2^ is Einstein\u0026rsquo;s mass-energy equation.\nKeyboard Keys Press Ctrl + C to copy text.\nAbbreviations HTML is the abbreviation for HyperText Markup Language.\n*[HTML]: HyperText Markup Language\nMath Formula (if KaTeX supported) Inline formula: $E = mc^2$\nBlock formula:\n$$ \\int_{-\\infty}^{\\infty} e^{-x^2} dx = \\sqrt{\\pi} $$\nAdmonitions (if supported) Note This is a note.\nTip This is a tip.\nImportant This is important information.\nWarning This is a warning.\nCaution This is a caution.\nDetails (if supported) Click to expand details This is the collapsed detailed content.\nYou can include any Markdown syntax here:\nList item Bold text Code Mixed Content Test This paragraph contains multiple formats: bold, italic, code, link , strikethrough, highlight.\nComplex List First item with bold text Nested item with code Another nested item with link Second item with italic text Ordered nested item Another ordered nested item Third item with strikethrough text Complex Table Feature Status Description Bold ✅ Supports bold text Italic ✅ Supports italic Code ✅ Supports inline code Link ✅ Supports links Strikethrough ❌ Needs testing This test document covers most common Markdown syntax and can be used to verify the completeness and aesthetics of prose styles.\nThis is the content of the first footnote.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nThis is the content of a named footnote.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","title":"Markdown Syntax Test Document"}],"tags":[{"link":"/tags/alert/","name":"Alert","slug":"Alert"},{"link":"/tags/code/","name":"Code","slug":"Code"},{"link":"/tags/image/","name":"Image","slug":"Image"},{"link":"/tags/markdown/","name":"Markdown","slug":"Markdown"},{"link":"/tags/math/","name":"Math","slug":"Math"},{"link":"/tags/mermaid/","name":"Mermaid","slug":"Mermaid"},{"link":"/tags/prose/","name":"Prose","slug":"Prose"},{"link":"/tags/rendering/","name":"Rendering","slug":"Rendering"},{"link":"/tags/style/","name":"Style","slug":"Style"},{"link":"/tags/syntax-highlighting/","name":"Syntax-Highlighting","slug":"Syntax-Highlighting"},{"link":"/tags/test/","name":"Test","slug":"Test"},{"link":"/tags/%E5%A4%A7%E6%A8%A1%E5%9E%8B/","name":"大模型","slug":"大模型"},{"link":"/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/","name":"强化学习","slug":"强化学习"},{"link":"/tags/%E7%94%9F%E6%88%90%E5%BC%8F%E6%8E%A8%E8%8D%90/","name":"生成式推荐","slug":"生成式推荐"},{"link":"/tags/%E7%94%9F%E6%88%90%E5%BC%8F%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/","name":"生成式推荐系统","slug":"生成式推荐系统"}]}