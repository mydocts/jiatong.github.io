<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Home on Jiatong Blog</title><link>https://mydocts.github.io/jiatong.github.io/</link><description>Recent content in Home on Jiatong Blog</description><generator>Hugo</generator><language>en-US</language><lastBuildDate>Fri, 26 Dec 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://mydocts.github.io/jiatong.github.io/index.xml" rel="self" type="application/rss+xml"/><item><title>NLP-Bookmark-Framework</title><link>https://mydocts.github.io/jiatong.github.io/posts/bookmark-framework/</link><pubDate>Fri, 26 Dec 2025 00:00:00 +0000</pubDate><guid>https://mydocts.github.io/jiatong.github.io/posts/bookmark-framework/</guid><description>&lt;h2 id="1-项目背景与动机"&gt;1. 项目背景与动机&lt;/h2&gt;
&lt;h3 id="核心问题"&gt;核心问题&lt;/h3&gt;
&lt;p&gt;在测试时扩展（Test-Time Scaling）场景下，大模型需要进行深度、多步骤推理（如数学竞赛题 AIME）。然而，现有方法面临两大挑战：&lt;/p&gt;</description></item><item><title>Translate-Paper</title><link>https://mydocts.github.io/jiatong.github.io/posts/translate/</link><pubDate>Tue, 23 Dec 2025 00:00:00 +0000</pubDate><guid>https://mydocts.github.io/jiatong.github.io/posts/translate/</guid><description>&lt;h3 id="项目背景"&gt;项目背景&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;目标&lt;/strong&gt;：构建一个专注于将英文论文翻译为流畅中文的垂直领域小模型。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;痛点分析&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;现有工具问题&lt;/strong&gt;：传统的翻译工具（如 Google Translate）往往过于直译，缺乏学术语境的润色，读起来拗口。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;大模型限制&lt;/strong&gt;：SOTA 大模型（如 GPT-4）效果好但 API 成本高，且并发受限，不适合大规模文档处理。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;项目方案&lt;/strong&gt;：&lt;/p&gt;</description></item><item><title>LLM-RL-GRPO-and its Variants</title><link>https://mydocts.github.io/jiatong.github.io/posts/grpo-variants/</link><pubDate>Sun, 14 Dec 2025 00:00:00 +0000</pubDate><guid>https://mydocts.github.io/jiatong.github.io/posts/grpo-variants/</guid><description>&lt;p&gt;&lt;strong&gt;GRPO是针对LLM的一种改进PPO算法&lt;/strong&gt;&lt;/p&gt;
&lt;h1 id="回顾前置知识"&gt;回顾前置知识：&lt;/h1&gt;
&lt;h2 id="policy-gradient"&gt;Policy Gradient&lt;/h2&gt;
$$
\nabla_\theta J(\theta)
\approx 
\frac{1}{N} 
\sum_{n=1}^{N} 
\sum_{t=1}^{T_n} 
R(\tau^{(n)}) 
\nabla_\theta 
\log P_\theta(a_t^{(n)} \mid s_t^{(n)})
$$&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;符号解释&lt;/strong&gt;&lt;/p&gt;
&lt;table&gt;
 &lt;thead&gt;
 &lt;tr&gt;
 &lt;th&gt;符号&lt;/th&gt;
 &lt;th&gt;含义&lt;/th&gt;
 &lt;/tr&gt;
 &lt;/thead&gt;
 &lt;tbody&gt;
 &lt;tr&gt;
 &lt;td&gt;$\nabla_\theta J(\theta)$&lt;/td&gt;
 &lt;td&gt;&lt;strong&gt;目标函数（期望回报）关于参数$\theta$的梯度&lt;/strong&gt;，即策略更新的方向。&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;$N$&lt;/td&gt;
 &lt;td&gt;&lt;strong&gt;采样的轨迹数量（episodes）&lt;/strong&gt;，即从环境中采样的完整回合数。&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;$T_n$&lt;/td&gt;
 &lt;td&gt;第$n$条轨迹的时间步数（episode的长度）。&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;$\tau^{(n)}$&lt;/td&gt;
 &lt;td&gt;第$n$条&lt;strong&gt;轨迹（trajectory）&lt;/strong&gt;：&lt;code&gt;&amp;lt;br&amp;gt;&lt;/code&gt; $\tau^{(n)} = (s_1^{(n)}, a_1^{(n)}, r_1^{(n)}, \dots, s_{T_n}^{(n)}, a_{T_n}^{(n)}, r_{T_n}^{(n)})$。&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;$R(\tau^{(n)})$&lt;/td&gt;
 &lt;td&gt;&lt;strong&gt;整条轨迹的总回报（return）&lt;/strong&gt;，通常为折扣累计奖励：&lt;code&gt;&amp;lt;br&amp;gt;&lt;/code&gt; $R(\tau^{(n)}) = \sum_{t=1}^{T_n}\gamma^{t-1}r_t^{(n)}$。&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;$\nabla_\theta \log P_\theta(a_t^{(n)}\mid s_t^{(n)})$&lt;/td&gt;
 &lt;td&gt;&lt;strong&gt;策略梯度项&lt;/strong&gt;，表示策略在状态$s_t^{(n)}$下选择动作$a_t^{(n)}$的对数概率的梯度，用于指导参数更新。&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;$P_\theta(a_t^{(n)}\mid s_t^{(n)})$&lt;/td&gt;
 &lt;td&gt;&lt;strong&gt;参数化策略（policy）&lt;/strong&gt;，给定状态$s_t^{(n)}$时采取动作$a_t^{(n)}$的概率，由参数$\theta$控制。&lt;/td&gt;
 &lt;/tr&gt;
 &lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;直观理解&lt;/strong&gt;
这个公式的含义是：如果一条轨迹带来的总奖励$R(\tau)$很高，那么模型应该调整参数$\theta$，让在这条轨迹中采取的动作$a_t$的概率更高；反之则降低这些动作的概率。&lt;/p&gt;</description></item><item><title>LLM-RL-DPO</title><link>https://mydocts.github.io/jiatong.github.io/posts/dpo/</link><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate><guid>https://mydocts.github.io/jiatong.github.io/posts/dpo/</guid><description>&lt;figure&gt;&lt;img src="https://mydocts.github.io/jiatong.github.io/images/ppo_full.png"
 alt="ppo" width="720"&gt;
&lt;/figure&gt;

&lt;figure&gt;&lt;img src="https://mydocts.github.io/jiatong.github.io/images/dpo_full.png"
 alt="dpo_full" width="720"&gt;
&lt;/figure&gt;

&lt;h3 id="kl-散度kullbackleibler-divergence"&gt;KL 散度（Kullback–Leibler Divergence）&lt;/h3&gt;
$$
KL(P \parallel Q) = \sum_{x \in X} P(x) \log \frac{P(x)}{Q(x)}
$$&lt;p&gt;&lt;strong&gt;含义&lt;/strong&gt;
P 分布相对于 Q 分布的相似程度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;性质&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;KL 散度的值 &lt;strong&gt;大于等于 0&lt;/strong&gt;。&lt;/li&gt;
&lt;li&gt;P 和 Q 越相似，KL 散度越接近 0。&lt;/li&gt;
&lt;li&gt;如果 P 和 Q 分布完全一致，则 &lt;strong&gt;KL 散度 = 0&lt;/strong&gt;。&lt;/li&gt;
&lt;li&gt;注意：
$$
 KL(P \parallel Q) \neq KL(Q \parallel P)
 $$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;KL 散度大于等于 0 的直观理解&lt;/strong&gt;
&lt;strong&gt;直观理解&lt;/strong&gt;：KL 散度是一个非负数，因为我们在比较两个分布时，
只有在完全一致时，它们之间的“差异”才为 0。&lt;/p&gt;</description></item><item><title>LLM-RL-PPO</title><link>https://mydocts.github.io/jiatong.github.io/posts/ppo/</link><pubDate>Sat, 06 Dec 2025 00:00:00 +0000</pubDate><guid>https://mydocts.github.io/jiatong.github.io/posts/ppo/</guid><description>&lt;figure&gt;&lt;img src="https://mydocts.github.io/jiatong.github.io/images/ppo_1.png"
 alt="前置基础" width="720"&gt;
&lt;/figure&gt;

&lt;h2 id="基础概念i"&gt;基础概念I&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;如果你不熟悉强化学习，学习ppo了解这些基础知识就足够了&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Action Space&lt;/strong&gt;: 可选择的动作，比如 {left, up, right}&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Policy&lt;/strong&gt;: 策略函数，输入 State，输出 Action 的概率分布。一般用 π 表示。&lt;/p&gt;</description></item><item><title>Some Obervations and Experience</title><link>https://mydocts.github.io/jiatong.github.io/posts/ai-thinking/</link><pubDate>Tue, 02 Dec 2025 00:00:00 +0000</pubDate><guid>https://mydocts.github.io/jiatong.github.io/posts/ai-thinking/</guid><description>&lt;h2 id="从工程实践看ai的本质"&gt;从工程实践看AI的本质&lt;/h2&gt;
&lt;p&gt;很多突破性的AI技术，其实都在模仿自然界已经存在了数百万年的机制。&lt;/p&gt;
&lt;p&gt;比如卷积神经网络的层级结构，本质上是在模拟视觉皮层的工作方式——从简单的边缘检测逐步抽象到复杂的物体识别。Transformer的自注意力机制，某种程度上也像是群体智能的数学建模，每个token都能&amp;quot;看到&amp;quot;全局，但各司其职。&lt;/p&gt;</description></item><item><title>Paper Reading 2</title><link>https://mydocts.github.io/jiatong.github.io/posts/paper2/</link><pubDate>Sat, 15 Nov 2025 10:00:00 +0800</pubDate><guid>https://mydocts.github.io/jiatong.github.io/posts/paper2/</guid><description>&lt;h2 id="lost-in-the-middle"&gt;Lost In the Middle&lt;/h2&gt;
&lt;p&gt;我们在这项工作中观察到的 U 形曲线，与心理学中的一个现象有关——&lt;strong&gt;系列位置效应（serial-position effect）&lt;/strong&gt;（Ebbinghaus，1913；Murdock Jr，1962）。这一效应指出：在让人们自由回忆一串列表元素的实验中，人类往往&lt;strong&gt;最容易记住列表中的第一个和最后一个元素&lt;/strong&gt;。&lt;/p&gt;</description></item><item><title>Paper Reading</title><link>https://mydocts.github.io/jiatong.github.io/posts/paper/</link><pubDate>Tue, 11 Nov 2025 10:00:00 +0800</pubDate><guid>https://mydocts.github.io/jiatong.github.io/posts/paper/</guid><description>&lt;h4 id="基于历史信息生成时对不同位置信息的关注不同"&gt;基于历史信息生成时，对不同位置信息的关注不同&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Lost In the Middle&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id="注意力层面校准差分选择性注意削弱首尾优势"&gt;注意力层面：校准/差分/选择性注意，削弱“首尾优势”；&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Found In the Middle&lt;/li&gt;
&lt;li&gt;DIFF&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id="训练数据位置无关信息密集型合成任务让模型学会任何位置都可能关键"&gt;&lt;strong&gt;训练数据&lt;/strong&gt;：位置无关/信息密集型合成任务，让模型学会“任何位置都可能关键”；&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Never Lost in the Model&lt;/li&gt;
&lt;li&gt;Fill in the Model&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id="提示工程预处理压缩与重排把关键信息放到模型更易注意的位置或在对话中状态化总结与提醒"&gt;&lt;strong&gt;提示工程/预处理&lt;/strong&gt;：压缩与&lt;strong&gt;重排&lt;/strong&gt;把关键信息放到模型更易注意的位置，或在对话中&lt;strong&gt;状态化总结与提醒&lt;/strong&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;LongLLMLingua&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="lost-in-the-middle"&gt;Lost In the Middle&lt;/h2&gt;
&lt;p&gt;目标信息的位置会显著影响模型推理的表现，当目标信息在上下文的开头和结尾时，模型表现好；在中间位置时，模型表现较差。&lt;/p&gt;</description></item><item><title>HSTU</title><link>https://mydocts.github.io/jiatong.github.io/posts/hstu/</link><pubDate>Fri, 24 Oct 2025 00:00:00 +0000</pubDate><guid>https://mydocts.github.io/jiatong.github.io/posts/hstu/</guid><description>&lt;h2 id="dlrm深度学习推荐系统"&gt;DLRM（深度学习推荐系统）：&lt;/h2&gt;
&lt;p&gt;一个经典的，标准的，符合工业界趋势的深度学习推荐系统模型应该如何构建？&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Bottom MLP：处理连续数值特征。&lt;/li&gt;
&lt;li&gt;EmbeddingLookup：将离散类别特征映射到向量空间。&lt;/li&gt;
&lt;li&gt;Feature Interaction Layer: 融合 dense 与 sparse 特征。&lt;/li&gt;
&lt;li&gt;Top MLP：进行最终预测（如点击率）。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;DLRM 成为推荐系统工业界（如 Meta、Tencent、ByteDance）广泛采用的标准架构之一，因为它兼顾：&lt;/p&gt;</description></item><item><title>TIGER &amp; Semantic ID</title><link>https://mydocts.github.io/jiatong.github.io/posts/tiger/</link><pubDate>Mon, 20 Oct 2025 00:00:00 +0000</pubDate><guid>https://mydocts.github.io/jiatong.github.io/posts/tiger/</guid><description>&lt;h1 id="tiger"&gt;TIGER&lt;/h1&gt;
&lt;hr&gt;
&lt;h2 id="推荐系统深度学习基本流程"&gt;推荐系统深度学习基本流程&lt;/h2&gt;
&lt;p&gt;现有深度学习架构：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;输入特征 → 文本嵌入 → 隐藏层（RNN，深度学习网络）&lt;br&gt;
得到更精准的预测表征 → 预测层（通过查询与物品的相似性度量、点击，对相关物品进行排序，得到预测）&lt;/li&gt;
&lt;li&gt;检索 / 召回一系列可行的候选对象，然后用排序模型对其进行排序。&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h4 id="检索阶段"&gt;检索阶段&lt;/h4&gt;
&lt;p&gt;通过矩阵分解，在同一空间学习查询和候选的嵌入。&lt;br&gt;
为了更好地捕获数据中的非线性关系，近年来采用 &lt;strong&gt;内积查询和候选嵌入到同一空间的双塔编码架构&lt;/strong&gt;（一个塔用于查询(用户)，另一个塔用于候选(物品)）成为主流。&lt;/p&gt;</description></item><item><title>GRID</title><link>https://mydocts.github.io/jiatong.github.io/posts/grid/</link><pubDate>Wed, 01 Oct 2025 10:00:00 +0800</pubDate><guid>https://mydocts.github.io/jiatong.github.io/posts/grid/</guid><description>&lt;h2 id="1前言"&gt;1.前言&lt;/h2&gt;
&lt;p&gt;GR 利用生成模型的进步，实现两种主要方式：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;直接生成用户感兴趣物品的文本内容&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;或者从预训练模型中提取语义表示（semantic representations），以编码开放世界的知识&lt;/strong&gt;，本文中的模型是后者。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="2语义idsemantic-ids"&gt;2.语义ID（Semantic IDs）&lt;/h2&gt;
&lt;h4 id="21语义id是什么"&gt;2.1语义ID是什么&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;促成 &lt;strong&gt;GR（Generative Recommender）&lt;/strong&gt; 成功的关键因素之一是 &lt;strong&gt;语义 ID（SID, Semantic ID）&lt;/strong&gt;。它将连续的语义表示（例如来自大型语言模型的向量表示）&lt;strong&gt;转换为离散的 ID 序列&lt;/strong&gt;。&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id="22传统id与语义id对比"&gt;2.2传统ID与语义ID对比&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;传统的ID特征是为用户/物品分配独一无二，但不提供信息的ID，这些ID被映射到嵌入向量中，主要用于捕获协同过滤信号&lt;/li&gt;
&lt;li&gt;语义ID将连续的语义表示(例如，来自大模型语言)转换为离散的ID序列；这样能够同时利用预训练基础模型中编码的语义知识以及用户-物品交互历史中的编码的协同信号。&lt;/li&gt;
&lt;li&gt;当两个物品的 SID 有重叠时，这种重叠在原理上反映了它们的&lt;strong&gt;语义相似性&lt;/strong&gt;；同时，下一步的监督学习（next-item supervision）使模型能够学习&lt;strong&gt;跨 SID 的协同过滤信号&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id="23怎么得到语义id"&gt;2.3怎么得到语义ID&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;首先通过模态编码器（modality encoder，例如大型语言模型 LLMs）提取语义表示；&lt;/li&gt;
&lt;li&gt;然后使用量化器（quantizer）将连续嵌入压缩为离散稀疏 ID。&lt;/li&gt;
&lt;li&gt;常见的基于量化的标记器包括：&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;RQ-VAE&lt;/strong&gt; ，&lt;strong&gt;RVQ&lt;/strong&gt; ，&lt;strong&gt;Residual K-Means&lt;/strong&gt; 。&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id="24如何将语义id用于生成式推荐"&gt;2.4如何将语义ID用于生成式推荐&lt;/h4&gt;
&lt;p&gt;TIGER 首次将 Transformer 应用于推荐任务，
用于预测物品的语义 ID（SID），并借鉴了文档检索（document retrieval）中生成式推荐的思想 。后续研究在多个方向上改进了 SID 的训练：&lt;/p&gt;</description></item><item><title>Transformer</title><link>https://mydocts.github.io/jiatong.github.io/posts/transformer/</link><pubDate>Wed, 17 Sep 2025 00:00:00 +0000</pubDate><guid>https://mydocts.github.io/jiatong.github.io/posts/transformer/</guid><description>&lt;h1 id="transformer"&gt;Transformer&lt;/h1&gt;
&lt;figure&gt;&lt;img src="https://mydocts.github.io/jiatong.github.io/images/transformer/architecture.png" width="720"&gt;
&lt;/figure&gt;

&lt;h2 id="embedding"&gt;Embedding&lt;/h2&gt;
&lt;p&gt;下面的示例实现一个最简词嵌入层，附带缩放确保数值稳定。&lt;/p&gt;
&lt;div
 class="code-block-container border-border bg-card my-6 overflow-hidden rounded-xl border shadow-sm transition-all duration-200 ease-out hover:-translate-y-0.5 hover:shadow-md"&gt;
 
 &lt;div
 class="code-block-header bg-muted/30 border-border flex items-center justify-between border-b px-4 py-3"&gt;
 
 &lt;div class="flex items-center gap-2"&gt;
 &lt;div class="text-muted-foreground flex-shrink-0"&gt;
 &lt;svg class="h-4 w-4" fill="none" stroke="currentColor" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"&gt;
 &lt;path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 20l4-16m4 4l4 4-4 4M6 16l-4-4 4-4" /&gt;
&lt;/svg&gt;
 &lt;/div&gt;
 &lt;span class="text-muted-foreground text-sm font-medium"&gt;
 PYTHON
 &lt;/span&gt;
 &lt;/div&gt;

 
 &lt;div class="flex items-center gap-2"&gt;
 &lt;button
 class="collapse-code-btn text-muted-foreground hover:text-primary hover:bg-primary/10 focus:ring-primary/20 flex items-center gap-1.5 rounded-md px-2 py-1 text-xs font-medium transition-all duration-200 ease-out focus:ring-2 focus:outline-none"
 data-code-id="code-0"
 data-default-state="expanded"
 data-collapsed="false"
 data-auto-collapse-lines="30"
 data-auto-collapse-height="400"
 data-collapsed-height="120"
 title="Collapse"
 aria-label="Collapse"&gt;
 &lt;span class="collapse-icon"&gt;
 &lt;svg class="h-3 w-3" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"&gt;&lt;path fill="currentColor" d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6l-6 6z"/&gt;&lt;/svg&gt;
 &lt;/span&gt;
 &lt;span class="collapse-text hidden sm:inline"
 &gt;Collapse&lt;/span
 &gt;
 &lt;/button&gt;
 &lt;button
 class="copy-code-btn text-muted-foreground hover:text-primary hover:bg-primary/10 focus:ring-primary/20 flex items-center gap-1.5 rounded-md px-2 py-1 text-xs font-medium transition-all duration-200 ease-out focus:ring-2 focus:outline-none"
 data-code-id="code-0"
 title="Copy"
 aria-label="Copy"&gt;
 &lt;span class="copy-icon"&gt;
 &lt;svg class="h-3 w-3" fill="none" stroke="currentColor" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"&gt;
 &lt;path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M8 16H6a2 2 0 01-2-2V6a2 2 0 012-2h8a2 2 0 012 2v2m-6 12h8a2 2 0 002-2v-8a2 2 0 00-2-2h-8a2 2 0 00-2 2v8a2 2 0 002 2z" /&gt;
&lt;/svg&gt;
 &lt;/span&gt;
 &lt;span class="copy-text hidden sm:inline"
 &gt;Copy&lt;/span
 &gt;
 &lt;/button&gt;
 &lt;/div&gt;
 &lt;/div&gt;

 
 &lt;div class="code-block-content relative" id="code-0"&gt;
 &lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-python" data-lang="python"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;torch&lt;/span&gt; &lt;span class="c1"&gt;# 引入 PyTorch 用于张量运算&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;torch.nn&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;nn&lt;/span&gt; &lt;span class="c1"&gt;# 导入神经网络模块方便搭建组件&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;SimpleEmbedding&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Module&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt; &lt;span class="c1"&gt;# 定义词嵌入层类&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;vocab_size&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;d_model&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt; &lt;span class="c1"&gt;# 接收词表大小和向量维度&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="nb"&gt;super&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="c1"&gt;# 初始化父类确保模块注册&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;embed&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Embedding&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;vocab_size&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;d_model&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="c1"&gt;# 创建可学习的词嵌入矩阵&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;scale&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;d_model&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt; &lt;span class="mf"&gt;0.5&lt;/span&gt; &lt;span class="c1"&gt;# 保存缩放系数防止数值过小&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;forward&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;tokens&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Tensor&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Tensor&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="c1"&gt;# 定义前向传播&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;embed&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tokens&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;scale&lt;/span&gt; &lt;span class="c1"&gt;# 查表并放大嵌入向量&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
 
 &lt;div
 class="collapse-overlay to-card/90 pointer-events-none absolute inset-0 bg-gradient-to-b from-transparent via-transparent opacity-0 transition-opacity duration-300"&gt;
 &lt;div
 class="text-muted-foreground bg-card/80 border-border/50 hover:bg-primary/10 hover:text-primary hover:border-primary/30 absolute bottom-4 left-1/2 -translate-x-1/2 cursor-pointer rounded-full border px-3 py-1.5 text-xs backdrop-blur-sm transition-all duration-200"&gt;
 Click to expand and view more
 &lt;/div&gt;
 &lt;/div&gt;
 &lt;/div&gt;
&lt;/div&gt;


&lt;script&gt;
(function() {
 const codeId = 'code-0';
 const copyBtn = document.querySelector('.copy-code-btn[data-code-id="' + codeId + '"]');
 const collapseBtn = document.querySelector('.collapse-code-btn[data-code-id="' + codeId + '"]');
 const codeContainer = document.getElementById(codeId);

 if (!codeContainer) return;

 
 if (copyBtn) {
 const copyIcon = copyBtn.querySelector('.copy-icon');
 const copyText = copyBtn.querySelector('.copy-text');

 copyBtn.addEventListener('click', async function() {
 try {
 
 let codeText = '';

 
 const codeTableCell = codeContainer.querySelector('.lntd:last-child code');
 if (codeTableCell) {
 codeText = codeTableCell.textContent || codeTableCell.innerText;
 } else {
 
 const codeElement = codeContainer.querySelector('code');
 if (codeElement) {
 
 const hasInlineLineNumbers = codeElement.querySelector('.ln');
 if (hasInlineLineNumbers) {
 
 const codeLines = codeElement.querySelectorAll('.cl');
 if (codeLines.length &gt; 0) {
 codeText = Array.from(codeLines)
 .map(line =&gt; {
 const text = line.textContent || line.innerText;
 
 return text.replace(/\n+$/, '');
 })
 .join('\n')
 .replace(/\n+$/, ''); 
 } else {
 
 const allText = codeElement.textContent || codeElement.innerText;
 codeText = allText.replace(/^\d+/gm, '').replace(/^\s+/gm, '');
 }
 } else {
 
 codeText = codeElement.textContent || codeElement.innerText;
 }
 } else {
 
 codeText = codeContainer.textContent || codeContainer.innerText;
 }
 }

 
 codeText = codeText.trim();

 
 await navigator.clipboard.writeText(codeText);

 
 copyIcon.innerHTML = `\u003csvg class=\u0022h-3 w-3\u0022 fill=\u0022none\u0022 stroke=\u0022currentColor\u0022 viewBox=\u00220 0 24 24\u0022 xmlns=\u0022http:\/\/www.w3.org\/2000\/svg\u0022\u003e\n \u003cpath stroke-linecap=\u0022round\u0022 stroke-linejoin=\u0022round\u0022 stroke-width=\u00222\u0022 d=\u0022M5 13l4 4L19 7\u0022 \/\u003e\n\u003c\/svg\u003e`;
 if (copyText) {
 copyText.textContent = 'Copied';
 }
 copyBtn.classList.add('text-green-600');

 
 setTimeout(() =&gt; {
 copyIcon.innerHTML = `\u003csvg class=\u0022h-3 w-3\u0022 fill=\u0022none\u0022 stroke=\u0022currentColor\u0022 viewBox=\u00220 0 24 24\u0022 xmlns=\u0022http:\/\/www.w3.org\/2000\/svg\u0022\u003e\n \u003cpath stroke-linecap=\u0022round\u0022 stroke-linejoin=\u0022round\u0022 stroke-width=\u00222\u0022 d=\u0022M8 16H6a2 2 0 01-2-2V6a2 2 0 012-2h8a2 2 0 012 2v2m-6 12h8a2 2 0 002-2v-8a2 2 0 00-2-2h-8a2 2 0 00-2 2v8a2 2 0 002 2z\u0022 \/\u003e\n\u003c\/svg\u003e`;
 if (copyText) {
 copyText.textContent = 'Copy';
 }
 copyBtn.classList.remove('text-green-600');
 }, 2000);

 } catch (err) {
 console.error('复制失败:', err);

 
 const range = document.createRange();
 const codeElement = codeContainer.querySelector('code') || codeContainer;
 range.selectNodeContents(codeElement);
 const selection = window.getSelection();
 selection.removeAllRanges();
 selection.addRange(range);

 
 if (copyText) {
 copyText.textContent = 'Selected';
 }

 setTimeout(() =&gt; {
 if (copyText) {
 copyText.textContent = 'Copy';
 }
 selection.removeAllRanges();
 }, 2000);
 }
 });
 }

 
 if (collapseBtn) {
 const collapseIcon = collapseBtn.querySelector('.collapse-icon');
 const collapseText = collapseBtn.querySelector('.collapse-text');
 const collapseOverlay = codeContainer.querySelector('.collapse-overlay');

 
 let codeElement = codeContainer.querySelector('pre.chroma');
 if (!codeElement) {
 codeElement = codeContainer.querySelector('pre');
 }

 const defaultState = collapseBtn.dataset.defaultState || 'expanded';
 const isCollapsedAttr = collapseBtn.dataset.collapsed === 'true';
 const autoCollapseLines = parseInt(collapseBtn.dataset.autoCollapseLines) || 30;
 const autoCollapseHeight = parseInt(collapseBtn.dataset.autoCollapseHeight) || 400;
 const collapsedHeight = parseInt(collapseBtn.dataset.collapsedHeight) || 120;

 let isCollapsed = false;

 
 function initCollapse() {
 
 const shouldCollapse = isCollapsedAttr ||
 defaultState === 'collapsed' ||
 shouldAutoCollapse();

 if (shouldCollapse) {
 setCollapsed(true, false); 
 }
 }

 function shouldAutoCollapse() {
 
 if (codeElement) {
 const lines = codeElement.querySelectorAll('.line, .cl');
 const height = codeElement.offsetHeight;
 return lines.length &gt; autoCollapseLines || height &gt; autoCollapseHeight;
 }

 
 const containerHeight = codeContainer.offsetHeight;
 if (containerHeight &gt; autoCollapseHeight) {
 return true;
 }

 
 const textContent = codeContainer.textContent || codeContainer.innerText || '';
 const estimatedLines = textContent.split('\n').length;
 return estimatedLines &gt; autoCollapseLines;
 }

 function setCollapsed(collapsed, animate = true) {
 if (!collapseOverlay) return;

 isCollapsed = collapsed;

 if (collapsed) {
 
 codeContainer.style.maxHeight = collapsedHeight + 'px';
 codeContainer.style.overflow = 'hidden';
 collapseOverlay.style.opacity = '1';
 collapseOverlay.style.pointerEvents = 'auto';

 
 collapseIcon.innerHTML = `\u003csvg class=\u0022h-3 w-3\u0022 fill=\u0022none\u0022 stroke=\u0022currentColor\u0022 viewBox=\u00220 0 24 24\u0022 xmlns=\u0022http:\/\/www.w3.org\/2000\/svg\u0022\u003e\n \u003cpath stroke-linecap=\u0022round\u0022 stroke-linejoin=\u0022round\u0022 stroke-width=\u00222\u0022 d=\u0022M19 9l-7 7-7-7\u0022 \/\u003e\n\u003c\/svg\u003e`;
 if (collapseText) {
 collapseText.textContent = 'Expand';
 }
 collapseBtn.title = 'Expand';

 } else {
 
 codeContainer.style.maxHeight = '';
 codeContainer.style.overflow = '';
 collapseOverlay.style.opacity = '0';
 collapseOverlay.style.pointerEvents = 'none';

 
 collapseIcon.innerHTML = `\u003csvg class=\u0022h-3 w-3\u0022 xmlns=\u0022http:\/\/www.w3.org\/2000\/svg\u0022 viewBox=\u00220 0 24 24\u0022\u003e\u003cpath fill=\u0022currentColor\u0022 d=\u0022M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6l-6 6z\u0022\/\u003e\u003c\/svg\u003e`;
 if (collapseText) {
 collapseText.textContent = 'Collapse';
 }
 collapseBtn.title = 'Collapse';
 }

 
 if (animate) {
 codeContainer.style.transition = 'max-height 0.3s ease-out';
 setTimeout(() =&gt; {
 codeContainer.style.transition = '';
 }, 300);
 }
 }

 function toggleCollapse() {
 setCollapsed(!isCollapsed, true);
 }

 
 collapseBtn.addEventListener('click', toggleCollapse);

 
 if (collapseOverlay) {
 collapseOverlay.addEventListener('click', () =&gt; {
 if (isCollapsed) {
 setCollapsed(false, true);
 }
 });
 }

 
 initCollapse();
 }
})();
&lt;/script&gt;
&lt;h2 id="muti_head-attention"&gt;Muti_Head Attention&lt;/h2&gt;
&lt;p&gt;下面的类展示如何把输入拆成多头并执行自注意力。&lt;/p&gt;</description></item><item><title>À Propos</title><link>https://mydocts.github.io/jiatong.github.io/about.fr/</link><pubDate>Mon, 01 Jan 2024 00:00:00 +0800</pubDate><guid>https://mydocts.github.io/jiatong.github.io/about.fr/</guid><description>Découvrez le thème Hugo Narrow et sa pile technologique</description></item><item><title>About</title><link>https://mydocts.github.io/jiatong.github.io/about/</link><pubDate>Mon, 01 Jan 2024 00:00:00 +0800</pubDate><guid>https://mydocts.github.io/jiatong.github.io/about/</guid><description>Learn about Hugo Narrow theme and its technology stack</description></item><item><title>について</title><link>https://mydocts.github.io/jiatong.github.io/about.ja/</link><pubDate>Mon, 01 Jan 2024 00:00:00 +0800</pubDate><guid>https://mydocts.github.io/jiatong.github.io/about.ja/</guid><description>Hugo Narrow テーマとその技術スタックについて</description></item><item><title>Accueil</title><link>https://mydocts.github.io/jiatong.github.io/_index.fr/</link><pubDate>Sun, 01 Jan 2023 08:00:00 -0700</pubDate><guid>https://mydocts.github.io/jiatong.github.io/_index.fr/</guid><description/></item><item><title>ホーム</title><link>https://mydocts.github.io/jiatong.github.io/_index.ja/</link><pubDate>Sun, 01 Jan 2023 08:00:00 -0700</pubDate><guid>https://mydocts.github.io/jiatong.github.io/_index.ja/</guid><description/></item><item><title/><link>https://mydocts.github.io/jiatong.github.io/code/core_algos_summary_zh/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://mydocts.github.io/jiatong.github.io/code/core_algos_summary_zh/</guid><description>&lt;h1 id="核心强化学习算法代码总结-core_algospy"&gt;核心强化学习算法代码总结 (&lt;code&gt;core_algos.py&lt;/code&gt;)&lt;/h1&gt;
&lt;p&gt;该文件是 &lt;strong&gt;verl&lt;/strong&gt; 框架中强化学习 (RL) 核心算法和工具函数的集合，主要针对大语言模型 (LLM) 对齐任务中的&lt;strong&gt;策略梯度 (Policy Gradient)&lt;/strong&gt; 方法（如 PPO, GRPO, RLOO 等）。&lt;/p&gt;
&lt;h2 id="1-概览"&gt;1. 概览&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;core_algos.py&lt;/code&gt; 提供了以下功能的实现：&lt;/p&gt;</description></item><item><title/><link>https://mydocts.github.io/jiatong.github.io/code/ppo_vs_grpo_example/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://mydocts.github.io/jiatong.github.io/code/ppo_vs_grpo_example/</guid><description>&lt;h1 id="ppo-vs-grpo-优势值计算对比实例"&gt;PPO vs GRPO 优势值计算对比实例&lt;/h1&gt;
&lt;p&gt;为了清晰理解代码背后的数学逻辑，我们假设一个简单的场景：
&lt;strong&gt;Prompt:&lt;/strong&gt; &lt;code&gt;1 + 1 = ?&lt;/code&gt;
&lt;strong&gt;模型生成回答:&lt;/strong&gt; &lt;code&gt;The answer is 2.&lt;/code&gt; (假设为 5 个 Token)&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id="1-ppo-基于-gae-实例"&gt;1. PPO (基于 GAE) 实例&lt;/h2&gt;
&lt;p&gt;PPO 需要一个 &lt;strong&gt;Critic 网络&lt;/strong&gt; 来预测每一个状态的价值 ($V$)。&lt;/p&gt;</description></item><item><title>Archives</title><link>https://mydocts.github.io/jiatong.github.io/archives/_index.fr/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://mydocts.github.io/jiatong.github.io/archives/_index.fr/</guid><description/></item><item><title>アーカイブ</title><link>https://mydocts.github.io/jiatong.github.io/archives/_index.ja/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://mydocts.github.io/jiatong.github.io/archives/_index.ja/</guid><description/></item></channel></rss>