{"categories":[{"link":"/categories/%E5%8D%9A%E5%AE%A2/","name":"博客","slug":"博客"},{"link":"/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/","name":"论文阅读","slug":"论文阅读"}],"pages":[],"posts":[{"link":"/posts/bookmark-framework/","text":"1. 项目背景与动机 核心问题 在测试时扩展（Test-Time Scaling）场景下，大模型需要进行深度、多步骤推理（如数学竞赛题 AIME）。然而，现有方法面临两大挑战：\n上下文窗口限制：长链推理会超出模型的最大上下文长度。 噪声累积：历史推理步骤过多时，模型难以聚焦于关键信息（\u0026ldquo;Lost in the Middle\u0026rdquo; 现象）。 解决方案：Bookmark 框架 Bookmark 框架提出了一种动态上下文压缩与检索机制：\nSubstep Archiving：将推理过程分解为多个子步骤，每个子步骤完成后生成摘要并\u0026quot;归档\u0026quot;。 Retrieval-Augmented Reflection：当模型需要回顾历史时，通过检索机制从归档中提取相关信息，而非保留完整上下文。 这使得模型在推理过程中始终保持精简的活跃上下文，有效解决了上下文窗口和噪声累积问题。\n2. 方法论详解 推理流程 (Inference Pipeline) 如下图所示，Bookmark 框架的推理流程包含以下核心组件：\nSubstep Tagging：使用 \u0026lt;substep\u0026gt;...\u0026lt;/substep\u0026gt; 标签将推理过程切分为离散步骤。 Bookmark Generation：每个子步骤结束后，模型生成一个简洁的摘要（Bookmark），保存到 Archive。 Retrieval Trigger：当模型遇到需要回顾历史的节点时（如 \u0026lt;reflection\u0026gt;），触发检索机制 from Archive 中提取相关 Bookmark。 Context Refresh：用检索到的 Bookmark 替换原始长上下文，输入模型继续推理。 训练策略 (Training Pipeline) 如下图所示，训练分为两个阶段：\nStage 1: 监督微调 (SFT) 目标：让模型学会生成结构化标签（\u0026lt;substep\u0026gt;, \u0026lt;reflection\u0026gt;）和 Bookmark 摘要。 数据来源：使用 s1.1 和 LIMO 数据集，通过 Gemini-3.0-flash 对原始 CoT 进行结构化标注。 利用大模型的逻辑判断能力对推理过程进行逻辑划分，使得推理过程分解成一个个包含子节点（子步骤和反思步骤）的信息。 利用子节点信息和正则化匹配的方法 将原始推理过程 用 \u0026lt;substep\u0026gt;...\u0026lt;/substep\u0026gt;和\u0026lt;reflection\u0026gt;...\u0026lt;/reflection\u0026gt;分割 有了标识符之后再利用大模型对每一对标识符进行总结，供后续RL动态推理使用。 模型：Qwen3-8B Backbone。 损失函数：标准自回归损失 $\\mathcal{L}\\_{SFT} = -\\sum\\_{t} \\log P(x\\_t | x\\_{\\lt t})$。 Stage 2: 强化学习 (GRPO) 算法：Group Relative Policy Optimization (GRPO)，无需 Critic 网络，通过组内相对优势进行优化。\n奖励函数设计：\n$$ R_{total} = w_{ans} \\cdot \\mathbb{I}(\\text{Answer} == GT) + w_{fmt} \\cdot Score_{format} $$ 准确性奖励 ($w_{ans}=1.0$)：最终答案是否正确，占主导地位。 格式奖励 ($w_{fmt}=0.5$)：是否正确使用了 \u0026lt;substep\u0026gt; 等标签，权重较低 训练过程：在模型推理过程中\n当生成遇到完整的 \u0026lt;substep\u0026gt;...\u0026lt;/substep\u0026gt; 标识符对时，模型要将 \u0026lt;substep\u0026gt;...\u0026lt;/substep\u0026gt;之间的内容抽取出来，存储在Archive中， 形如 {\u0026lt;substep1\u0026gt;...\u0026lt;/substep1\u0026gt;: $Emb\\_{1}$, \u0026lt;substep2\u0026gt;...\u0026lt;/substep2\u0026gt;: $Emb_{2}$\u0026hellip;} 当生成遇到 \u0026lt;reflection\u0026gt; 的首标识符时, 这时采取的动作是 计算 后两句话的Embedding向量，然后去Archive中检索最相关的子步骤$S_i$,将$S_i$插入这两句话的后面，然模型继续推理，只是此时的前文内容是 summary1,summary2 \u0026hellip; + $S_i$ 最终效果 3. 实验设置 数据构建 来源：s1.1 + LIMO 数据集（约 700 条高质量标注样本）。 流程：原始 CoT → Gemini 分段 → 生成摘要 → 拼接标签。 训练配置 参数 值 设备 4 × NVIDIA A800 (80GB) 框架 verl (GRPO 实现) Group Size 8 KL 系数 0.001 $w_{ans}$ / $w_{fmt}$ 1.0 / 0.1 4. 实验结果 SFT 阶段性能 (Qwen3-8B) 指标 AIME 2025 AIME 2024 Overall Pass@1 41.67% 53.33% 完全正确 (100% Acc) 1/30 2/30 完全失败 (0% Acc) 8/30 3/30 平均 Token 数 15,702 13,911 分析：\n对于一个 8B 模型，在 AIME 上取得 40%+ 的 Pass@1 是非常有意义的结果。 高 Token 数（约 14k-15k）表明模型确实在进行深度、长链推理。 预期后续 GRPO 阶段将进一步提升准确率，通过惩罚噪声累积、奖励高密度推理步骤。 5. 未来工作 RL 训练扩展：在更多数据集上进行大规模 GRPO 训练，观察是否能涌现更高级的策略行为。 算法优化：探索 DAPO 等更稳定的对齐算法，优化准确性与格式奖励的权衡。 跨模型验证：在 Llama-3、32B 等更大模型上验证框架的通用性。 ","title":"NLP-Bookmark-Framework"},{"link":"/posts/translate/","text":"项目背景 目标：构建一个专注于将英文论文翻译为流畅中文的垂直领域小模型。\n痛点分析：\n现有工具问题：传统的翻译工具（如 Google Translate）往往过于直译，缺乏学术语境的润色，读起来拗口。 大模型限制：SOTA 大模型（如 GPT-4）效果好但 API 成本高，且并发受限，不适合大规模文档处理。 项目方案：\n核心策略：基于开源小模型（如 Qwen系列）进行指令微调（SFT）和偏好对齐（DPO）。 优势：实现低成本、高并发的本地化部署，同时通过垂直数据训练保证专业术语准确和表达流畅。 数据工程 数据集构建策略 Teacher-Student 蒸馏：利用 SOTA 模型生成高质量翻译作为“金标准”。 Prompt 技巧：要求模型“先直译，再意译，最后润色”，或者使用 \u0026ldquo;Chain of Thought\u0026rdquo; 让模型解释术语。 多模型采样：使用 Qwen不同参数量的模型生成候选结果，通过人工或模型（LLM-as-a-Judge）筛选最佳答案。 坏例修正 (Self-Correction)：针对 Bad Case 修改 Prompt 重新生成，加入微调数据集。 SFT数据集: 收集原文: 近一年arxiv论文-\u0026gt;随机筛选2w篇-\u0026gt;每篇随机筛选一页-\u0026gt;dot.ocr提取文本提出reference后的页数-\u0026gt;每页按段落切分文档，选择字数小于512单词的数据 构造翻译prompt :尝试不同格式提示词（直接翻译，cot分析翻译，think模式翻译）以及不同的模型（qwen3-0.6B，qwen3-1.7B，qwen3-8B，qwen3-32B）；挑选AI领域段落（个人比较熟悉），人工gsb逐轮次迭代prompt要求（eg：见下面的翻译规范要求） 构造样本: 利用得到的最好的prompt和相对较大的模型（qwen3-32B）去生成sft数据 翻译规范要求 信息无损：严禁漏译核心观点。 格式保留：Markdown 格式、LaTeX 公式、引用链接必须原样保留。 术语一致：建立专有名词词表（Glossary），确保 \u0026ldquo;Attention\u0026rdquo; 翻译为 \u0026ldquo;注意力\u0026rdquo; 而不是 \u0026ldquo;关注\u0026rdquo;。 DPO 数据筛选与构建 (Pair 构造) Chosen (参考翻译)：利用得到的最好的prompt和相对较大的模型（qwen3-32B）去生成sft数据 Rejected (候选翻译)：SFT后的模型进行翻译（约2w条）-\u0026gt;过滤bleu\u0026lt;0.85的样本(约1.8w条)) -\u0026gt; DPO训练 经过SFT后的输出大部分在0.5上下，我们这里过滤设置为0.85就是认为小于0.85的都算作rejected样本，目的是想让模型尽可能向参考翻译学习 如果bleu设置小一些，例如0.5。模型本来生成的分数就在这个分数段，没有太大意义。我们想让模型区分什么是好样本，什么是坏样本。所谓的好样本，应该是和参考翻译很贴近。 评估 最终结果评估 BLEU：基础语义对齐评估。 为什么不用大模型评估？ 核心是与人对齐。大模型无法解决“拗口\u0026quot;的问题，有时候大模型给一段话平分很高，但是人读起来比较费解。 GSB (Good/Same/Bad)：人工或模型对比评估。 过程评估 幻觉率评估 (Hallucination Rate Evaluation)： 大模型评估 大部分的翻译问题都可以由prompt来解决，幻觉问题更多要从训练数据层面入手 SFT 灾难性遗忘:数据量较小或偏离原模型分布，模型可能会强行拟合新任务，从而覆盖掉原来的语言能力或推理能力 混合数据训练 降低学习率，采取Warmup等操作 Lora微调 损失函数，KL散度 本质上就是减少参数更新幅度，减少更新范围，减少分布偏移。\n参数选择：\n不同规模模型的推荐学习率参考 (LoRA/QLoRA)\n以下表格整理了在不同模型规模下，SFT（指令微调）与 DPO（偏好优化）的推荐学习率区间。\n模型规模 (Model Scale) SFT 推荐 LR (LoRA) DPO 推荐 LR 备注 (Notes) 0.5B - 1.8B 2e-4 — 5e-4 5e-6 — 1e-5 模型较小，相对“坚韧”，可以使用较大的步长快速收敛。 7B - 8B 5e-5 — 1e-4 1e-6 — 3e-6 主流模型规模（如 Llama 3, Qwen 2.5）。5e-7 通常是下限，过低会导致收敛极慢。 14B - 32B 1e-5 — 5e-5 5e-7 — 1e-6 参数量较大，即使是 LoRA 也需要非常小心，LR 过大容易导致灾难性遗忘。 70B+ 5e-6 — 1e-5 1e-7 — 5e-7 极小的步长，通常配合极大的 Batch Size 进行训练。 关键指标参考：\nGRPO: 学习率通常建议 ≤ 1e-5 (Full) 或 5e-5 (LoRA)，Beta 推荐 0.001。 DPO 监控: 重点关注 rewards/chosen、rewards/rejected 和 rewards/margins。 DPO 训练过程中重点关注的指标 指标 含义与关注点 异常情况 Training Loss 模型拟合数据的程度。 若不下降，可能是学习率太小或数据质量差。 Validation Loss 泛化能力。 若 Train 下降但 Val 上升，说明过拟合。 Accuracy Chosen 的 Reward \u0026gt; Rejected 的 Reward 的比例。 理想应稳步上升至 0.6~0.8。若接近 1.0 可能过拟合。 Reward Margin $R_{chosen} - R_{rejected}$ 的差值。 应逐渐变大。若 Margin 极小，说明模型分不清好坏。 DPO数据构造出现样本不均衡的问题怎么办？ 样本不均衡通常指不同类型的 prompt 数量差异大，或者 chosen/rejected 长度、质量分布不一致。\n分层采样 (Stratified Sampling)：按任务类型（如翻译、代码、逻辑）对数据进行分类，训练时保证每个 batch 内各类别比例均衡。 加权 Loss (Weighted Loss)：对稀缺样本类别赋予更高的 Loss 权重，强迫模型关注少样本任务。 数据增强 (Data Augmentation)：利用 LLM 改写少样本类别的 prompt，或者通过 Self-Instruct 合成更多同类数据。 调整 Rejected 采样策略：如果某些 chosen 回复很难找到好的 rejected（或者 rejected 太强），尝试生成更多样化的负样本（如不同温度采样），确保 margin 有区分度。 论文翻译任务中遇到的幻觉以及是如何解决的？ 在论文翻译这种对“精确度”要求极高的场景下，幻觉往往表现为术语乱译、公式篡改或逻辑扭曲。以下是几个典型案例及其解决方案：\n1：术语一致性幻觉 (Terminology Inconsistency) 具体问题：模型将同一术语在上下文中翻译得不一致，或者出现低级错误（如将 \u0026ldquo;Transformer\u0026rdquo; 翻译成 \u0026ldquo;变压器\u0026rdquo;）。 解决过程： 问题定位：发现模型在没有外部约束时，倾向于根据局部语境选择最常见的词汇，而非专业学术词汇。 解决方案： Glossary Injection (提示工程)：在 Prompt 中动态注入领域术语表（如：{ \u0026quot;Attention Mechanism\u0026quot;: \u0026quot;注意力机制\u0026quot; }），强制约束。 SFT 阶段约束：在训练数据中加入大量带术语保护的样本，让模型学会尊重特定实体。 2：公式的“降维打击” (Equation/LaTeX Hallucination) 具体问题：在处理 LaTeX 公式时，模型会自作聪明地修改下标、变量名，甚至因为 Markdown 渲染问题丢失反斜杠。 解决过程： 问题定位：Tokenizer 对 LaTeX 特殊字符（如 \\、_、^）的切分导致模型对公式结构的语义理解出现偏差。 解决方案： 正则占位符保护：翻译前用正则匹配公式并替换为占位符（如 \u0026lt;MATH_0\u0026gt;），翻译完成后再进行原位回填。 代码感知微调：在微调数据中刻意保留复杂的 LaTeX 结构，确保模型将其识别为“不可触碰”的特殊 Toekn。 3：引用标签丢失 (Citation/Link Hallucination) 具体问题：原文中的引用（如 [24, 31]）在译文中消失，导致学术严谨性受损。 解决过程： 问题定位：模型认为引用标签是“噪声”，在生成流畅译文时将其忽略。 解决方案： Rule-based Reward (RL 阶段)：在使用 GRPO/PPO 训练时，设计一个\u0026quot;引用一致性奖励函数\u0026quot;。如果译文中的引用标签数量与原文不符，扣除大量 Reward。 数据清洗：剔除 SFT 数据集中任何丢失引用标签的 Bad Case。 DPO缓解幻觉时，反而引入了新的幻觉，可能是什么原因？ 在尝试通过 DPO 修复上述幻觉时，有时会发现模型出现了“越对齐越离谱”的情况，主要原因为：\n偏好数据中的“事实性错误”被误选： 现象：构建数据时，如果 Chosen 回复虽然流畅但包含由模型编造的错误事实（幻觉），而 Rejected 仅仅是因为语气不好主要事实是对的。 举例：Chosen 是“Transformer 是由 Google 在 2018 年提出的（此处年份错误，应为 2017）”，而 Rejected 是“Transformer 是 Google 2017 年提出的项目。”，但因为前者句式更漂亮被标为 Chosen。 后果：DPO 会强化模型去学习那个“流畅但错误”的陈述，导致新幻觉。 解决办法：引入“事实对齐”环节。在数据筛选阶段增加事实性校验（Fact-checking），若 Chosen 存在事实错误，则该 Pair 必须舍弃或修正，不能仅凭语气打分。 过度优化（Over-optimization / Reward Hacking）： 现象：模型发现“胡说八道”但在格式、长度或语气上极度讨好（Sycophancy）能获得更低的 Loss。 举例：偏好数据中 Chosen 往往比 Rejected 长，模型学到了“长=好”的捷径，于是生成了大量毫无意义的修饰词和废话，甚至为了凑字数开始编造细节。 后果：特别是 Beta 过小时，模型挣脱了 SFT 的约束，开始放飞自我。 解决办法：适当增大 Beta 参数（如从 0.05 调回 0.1），加强对参考模型的约束；同时在数据构造时平衡 Chosen 和 Rejected 的长度分布。 Chosen 和 Rejected 的分布差异过大（OOD）： 现象：例如 Chosen 是 GPT-4o生成的完美答案，而训练模型很弱。 举例：1.8B 的小模型被迫模仿 GPT-4o 的高深逻辑 and 复杂句式，就像小学生模仿博士写论文，虽然语气很像（使用了大量“综上所述”、“显而易见”），但中间的推导逻辑全是错的。 后果：模型在 SFT 阶段没见过这么高质量的数据，DPO 强行拉近模型与 32B 的分布，导致模型“不懂装懂”，产生逻辑幻觉。 解决办法：先进行高质量的 SFT 让模型“见过世面”，或者采用“由弱到强”的对齐策略，先用与模型能力更接近的优质数据做 DPO 演进。 负样本质量太差： 现象：如果 Rejected 样本太过于离谱（如乱码、完全不相关），模型很容易区分。 举例：Rejected 样本全是 asdfghjkl 或“我不明白你在说什么”。模型只需要学会“不发疯”就能赢过 Rejected，而无法学到“将词语 A 翻译成词语 B 更好”这种细粒度的真实性差异。 后果：这种情况下 DPO 训练变得太简单，模型学不到细粒度的“真实性”差异，反而因为梯度更新导致参数漂移。 解决办法：构造“困难负样本 (Hard Negatives)”。通过对同一 Prompt 进行多次采样，选择那些逻辑通顺但包含细微错误、或术语使用不当的样本作为 Rejected。 DPO 中的 Beta 参数详解 Beta ($\\beta$) 是 KL 散度（KL Divergence）的系数，用于控制新策略（Policy）与参考策略（Reference Model）之间的偏离程度。\nBeta 过大 (e.g., \u0026gt; 0.5)： 影响：对模型的约束极强，模型不敢轻易改变，输出会非常接近 SFT 模型。 结果：训练非常稳定，但 Optimization 效果差，模型学不到这一步偏好。 Beta 过小 (e.g., \u0026lt; 0.05)： 影响：约束太弱，模型为了最大化 Reward 可能会“放飞自我”。 结果：容易出现 Reward Hacking（例如：生成一堆乱码但长度符合要求，或者开始复读），导致语言能力崩坏，输出可读性下降。 推荐值：通常在 0.1 左右。\nGRPO GRPO vs PPO 核心区别： Critic 模型：PPO 需要训练一个额外的 Value Network (Critic) 来评估状态价值；GRPO 不需要 Critic，它通过同一 Prompt 生成的一组（Group）输出计算 Baseline（例如组内平均分）。 计算效率：GRPO 省去了 Critic 模型的显存占用 and 计算量，更适合大模型训练。 为什么 GRPO 更高效： Group Baseline：对于推理/数学问题，答案通常只有一个对错。GRPO 通过从群体中对比，能更直接地利用“相对优劣”作为信号，减少方差。 适合 Training-free 场景的进一步优化（如 DeepSeek-R1 的思路）。 GRPO奖励函数 (Reward Function) 设计 BLEU：得分越高奖励越大 后续可以考虑加入幻觉惩罚，用大模型评估\n训练经验 核心参数与模型规模对照表 参数维度 策略类型 小规模模型 (0.5B - 3B) 中等规模模型 (7B - 14B) 大规模模型 (32B - 70B+) 学习率 (LR) DPO 5e−6 ～ 1e−5 1e−6 ～ 3e−6 1e−7 ～ 1e−6 GRPO 1e−5 ～ 2e−5 5e−6 ～ 1e−5 1e−6 ～ 5e−6 核心系数 DPO ($\\beta$) 0.05 ～ 0.1 (较激进) 0.1 (标准) 0.1 ～ 0.3 (较保守) GRPO (KL) 0.001 ～ 0.005 0.001 0.001 ～ 0.01 Batch Size 共同趋势 较小即可收敛 中等 必须配合大 Global Batch Beta / KL 系数的影响指南 维度 $\\beta$ / KL 系数 较小 (例如 DPO 0.01) $\\beta$ / KL 系数 较大 (例如 DPO 0.5) 对齐强度 强力对齐：模型会竭尽全力迎合偏好数据。 保守对齐：模型更新缓慢，更倾向于保留 SFT 的能力。 模型风险 高风险：容易出现“奖励黑客”，输出可能变得怪异或复读。 低风险：训练稳定，但可能感觉模型“没怎么变”。 适用场景 偏好数据质量极高、需要模型大幅改变行为。 保护模型的基础对话能力、防止推理逻辑崩坏。 逻辑表现 可能为了符合格式而牺牲逻辑。 逻辑更稳健，但对格式的依从性提升较慢。 实战调参速查表：不同工况下的参数干预 当训练出现以下“症状”时，可参考以下“处方”进行参数调整：\n遇到的情况 (Scenario) 学习率 (LR) 调整 Beta / KL 调整 调控原理 (Reasoning) 模型收敛极慢 / Loss 下降不明显 ⬆️ 适当调大 ⬇️ 可微调小 模型更新动力不足，需要更大的步长；减小 Beta 可放松对参考模型的约束。 训练不稳定 / Loss 剧烈震荡 ⬇️ 减小 (0.5x - 0.1x) ⬆️ 调大 步长过大导致在最优解附近跳动；增大 Beta 可增强稳定性，防止模型参数剧烈变化。 出现幻觉 / 语言能力严重退化 \u0026lt;br\u0026gt;(灾难性遗忘) ⬇️ 减小 ⬆️ 显著调大 模型偏离了原始 SFT 分布。增大 Beta/KL 强迫模型保持在 SFT 模型附近，保护基础能力。 “奖励黑客” (Reward Hacking)\u0026lt;br\u0026gt;(分数很高但回复逻辑差/Trick多) ➖ 保持或微降 ⬆️ 调大 模型为了高分钻了空子。增大 Beta 相当于增加“正则化”惩罚，限制模型走捷径。 模型“太顽固” / 对齐效果不明显 ⬆️ 尝试调大 ⬇️ 调小 约束太强（Beta 过大）或更新太慢（LR 过小）。减小 Beta 允许模型探索与 SFT 差异更大的策略空间。 过拟合 (验证集 Loss 上升) ⬇️ 减小 ⬆️ 调大 模型开始死记硬背训练数据。增大 Beta 有助于泛化，防止对特定偏好样本过度优化。 BadCase 处理全流程 归因分析： 是 SFT 没学好（知识缺失）？ -\u0026gt; 补 SFT 数据。 是 DPO 跑偏了（幻觉/复读）？ -\u0026gt; 调整 Beta or 检查负样本质量。 是 Prompt 不懂指令？ -\u0026gt; 优化 System Prompt。 数据闭环： 将 BadCase 收集起来，人工修正得到 Correct Answer。 构造 (Question, Bad_Answer) 作为 Rejected，(Question, Correct_Answer) 作为 Chosen。 加入微调集进行下一轮迭代。 ","title":"Translate-Paper"},{"link":"/posts/grpo-variants/","text":"GRPO是针对LLM的一种改进PPO算法\n回顾前置知识： Policy Gradient $$ \\nabla_\\theta J(\\theta) \\approx \\frac{1}{N} \\sum_{n=1}^{N} \\sum_{t=1}^{T_n} R(\\tau^{(n)}) \\nabla_\\theta \\log P_\\theta(a_t^{(n)} \\mid s_t^{(n)}) $$ 符号解释\n符号 含义 $\\nabla_\\theta J(\\theta)$ 目标函数（期望回报）关于参数$\\theta$的梯度，即策略更新的方向。 $N$ 采样的轨迹数量（episodes），即从环境中采样的完整回合数。 $T_n$ 第$n$条轨迹的时间步数（episode的长度）。 $\\tau^{(n)}$ 第$n$条轨迹（trajectory）：\u0026lt;br\u0026gt; $\\tau^{(n)} = (s_1^{(n)}, a_1^{(n)}, r_1^{(n)}, \\dots, s_{T_n}^{(n)}, a_{T_n}^{(n)}, r_{T_n}^{(n)})$。 $R(\\tau^{(n)})$ 整条轨迹的总回报（return），通常为折扣累计奖励：\u0026lt;br\u0026gt; $R(\\tau^{(n)}) = \\sum_{t=1}^{T_n}\\gamma^{t-1}r_t^{(n)}$。 $\\nabla_\\theta \\log P_\\theta(a_t^{(n)}\\mid s_t^{(n)})$ 策略梯度项，表示策略在状态$s_t^{(n)}$下选择动作$a_t^{(n)}$的对数概率的梯度，用于指导参数更新。 $P_\\theta(a_t^{(n)}\\mid s_t^{(n)})$ 参数化策略（policy），给定状态$s_t^{(n)}$时采取动作$a_t^{(n)}$的概率，由参数$\\theta$控制。 直观理解 这个公式的含义是：如果一条轨迹带来的总奖励$R(\\tau)$很高，那么模型应该调整参数$\\theta$，让在这条轨迹中采取的动作$a_t$的概率更高；反之则降低这些动作的概率。\n考虑动作影响时效 Discounted Return（折扣回报）\n$$ R_t^{(n)} = \\sum_{t'=t}^{T_n} \\gamma^{\\,t'-t} r_{t'}^{(n)} $$符号解释\n符号 含义 $t$、$t'$ 时间步索引；$t'$ 是求和变量，从当前时间步 $t$ 开始一直到 $T_n$。 $\\gamma$ 折扣因子（discount factor），$0 \u003c \\gamma \\le 1$。它控制未来奖励的重要性：越小表示越“短视”。 $r_{t'}^{(n)}$ 第$n$ 条轨迹在时间步 $t'$ 获得的即时奖励（immediate reward）。 直观理解 这个公式计算的是从某个时间步 $t$ 开始，到轨迹结束的所有未来奖励的加权和。 靠近当前的奖励权重大（因为$\\gamma^{0}=1$），越往后的奖励会被折扣（因为$\\gamma^{t'-t}$会变小）。\n与策略梯度公式的关系 在策略梯度中，$R_t^{(n)}$ 通常替代 $R(\\tau^{(n)})$ 用作加权项，使得每个时间步的梯度更新都考虑该时刻之后的未来回报：\n$$ \\nabla_\\theta J(\\theta) \\approx \\frac{1}{N} \\sum_{n=1}^{N} \\sum_{t=1}^{T_n} R_t^{(n)} \\nabla_\\theta \\log P_\\theta(a_t^{(n)} \\mid s_t^{(n)}) $$考虑到动作的相对优势 Policy Gradient with Baseline\n$$ \\nabla_\\theta J(\\theta) \\approx \\frac{1}{N} \\sum_{n=1}^{N} \\sum_{t=1}^{T_n} \\big(R_t^{(n)} - B(s_t^{(n)})\\big) \\nabla_\\theta \\log P_\\theta(a_t^{(n)} \\mid s_t^{(n)}) $$ 符号解释\n符号 含义 $B(s_t^{(n)})$ 基线函数（baseline function），通常是状态价值函数$V_\\pi(s_t^{(n)})$，用于减小方差。 $\\nabla_\\theta \\log P_\\theta(a_t^{(n)}\\mid s_t^{(n)})$ 策略梯度项，表示在状态$s_t^{(n)}$下采取动作$a_t^{(n)}$的对数概率的梯度。 $P_\\theta(a_t^{(n)}\\mid s_t^{(n)})$ 参数化策略（policy），给定状态$s_t^{(n)}$时采取动作$a_t^{(n)}$的概率，由参数$\\theta$控制。 直观理解与原始 REINFORCE 相比，这个版本在计算梯度时引入了一个基线项 $B(s_t^{(n)})$，用于减少方差但不引入偏差。它衡量“当前动作的回报比期望高多少”：\n如果 $R_t^{(n)} \u003e B(s_t^{(n)})$，说明该动作比平均表现好 → 提高它的概率； 如果 $R_t^{(n)} \u003c B(s_t^{(n)})$，说明该动作表现差 → 降低它的概率。 备注 若将 $A_t^{(n)} = R_t^{(n)} - B(s_t^{(n)})$ 记作优势函数（advantage function）， 公式可简写为：\n$$ \\nabla_\\theta J(\\theta) \\approx \\frac{1}{N} \\sum_{n=1}^{N} \\sum_{t=1}^{T_n} A_t^{(n)} \\nabla_\\theta \\log P_\\theta(a_t^{(n)} \\mid s_t^{(n)}) $$引入优势函数 优势函数定义 $$ A_\\theta(s, a) = Q_\\theta(s, a) - V_\\theta(s) $$在状态 $s$ 下，执行动作 $a$，相比平均水平（状态价值）能带来多少额外优势。\n各符号解释\n符号 含义 $A_\\theta(s, a)$ 优势函数（Advantage Function），衡量在状态 $s$ 下执行动作 $a$ 比平均表现（$V_\\theta(s)$）好多少。 $Q_\\theta(s, a)$ 动作价值函数（Action-Value Function）：在状态 $s$ 下执行动作 $a$ 后，期望获得的累计回报。 $V_\\theta(s)$ 状态价值函数（State-Value Function）：在状态 $s$ 下，依据当前策略期望的回报。 $\\theta$ 策略参数，控制策略$\\pi_\\theta$ 或价值函数的参数化形式。 直观理解\n$Q_\\theta(s,a)$ 衡量“执行某动作后能得到的回报”； $V_\\theta(s)$ 衡量“在该状态下平均能得到的回报”； 二者之差 $A_\\theta(s,a)$ 衡量“这个动作比平均动作好多少”。 当 $A_\\theta(s,a) \u003e 0$ 时，该动作优于平均水平，应提升其概率； 当 $A_\\theta(s,a) \u003c 0$ 时，该动作劣于平均，应降低其概率。\nAdvantage Policy Gradient 公式 $$ \\nabla_\\theta J(\\theta) \\approx \\frac{1}{N} \\sum_{n=1}^{N} \\sum_{t=1}^{T_n} A_\\theta(s_t^{(n)}, a_t^{(n)}) \\nabla_\\theta \\log P_\\theta(a_t^{(n)} \\mid s_t^{(n)}) $$ 符号解释\n符号 含义 $\\nabla_\\theta J(\\theta)$ 期望回报关于参数$\\theta$ 的梯度，即策略改进方向。 $A_\\theta(s_t^{(n)}, a_t^{(n)})$ 第$n$ 条轨迹第 $t$ 步的优势值，衡量该动作相对于平均水平的好坏。 $\\log P_\\theta(a_t^{(n)} \\mid s_t^{(n)})$ 策略函数在状态$s_t^{(n)}$ 下选择动作 $a_t^{(n)}$ 的对数概率。 $N$ 采样轨迹数量（episodes）。 $T_n$ 第$n$ 条轨迹的时间步数。 GAE 优势函数（Generalized Advantage Estimation） $$ A_\\theta(s_t, a_t) = Q_\\theta(s_t, a_t) - V_\\theta(s_t) $$$$ Q_\\theta(s_t, a_t) = r_t + \\gamma V_\\theta(s_{t+1}) $$$$ A_\\theta(s_t, a_t) = r_t + \\gamma V_\\theta(s_{t+1}) - V_\\theta(s_t) $$$$ V_\\theta(s_{t+1}) \\approx r_{t+1} + \\gamma V_\\theta(s_{t+2}) $$ $$ \\begin{aligned} A_\\theta^{1}(s_t, a_t) \u0026= r_t + \\gamma V_\\theta(s_{t+1}) - V_\\theta(s_t) \\\\ A_\\theta^{2}(s_t, a_t) \u0026= r_t + \\gamma r_{t+1} + \\gamma^2 V_\\theta(s_{t+2}) - V_\\theta(s_t) \\\\ A_\\theta^{3}(s_t, a_t) \u0026= r_t + \\gamma r_{t+1} + \\gamma^2 r_{t+2} + \\gamma^3 V_\\theta(s_{t+3}) - V_\\theta(s_t) \\\\ A_\\theta^{T}(s_t, a_t) \u0026= r_t + \\gamma r_{t+1} + \\gamma^2 r_{t+2} + \\gamma^3 r_{t+3} + \\cdots + \\gamma^{T} r_T - V_\\theta(s_t) \\end{aligned} $$ $$ A_\\theta^{GAE}(s_t, a_t) = (1 - \\lambda)(A_\\theta^{1} + \\lambda A_\\theta^{2} + \\lambda^2 A_\\theta^{3} + \\cdots) $$GRPO GRPO算法计算每个Token的Advantage GRPO：Group Relative Policy Optimization 中文名称：群体相对策略优化\n示例输入（Prompt及生成结果） 同一个问题，模型会输出三个结果\n什么 是 数据库 ? → 数据库 用于 存储 数据 。 什么 是 数据库 ? → 数据库 是 一个 有组织的 数据集合 什么 是 数据库 ? → 数据库 是 用来 高效 存取 数据 的 软件 奖励（Reward） 对每个句子打一个分\n$$ r_1 = 3.8, \\quad r_2 = 5.2, \\quad r_3 = 6.1 $$标准化奖励（Standardized Reward）\n$$ \\tilde{r}_i = \\frac{r_i - \\text{mean}(r)}{\\text{std}(r)} $$$$ \\tilde{r}_1 = -1.06, \\quad \\tilde{r}_2 = 0.14, \\quad \\tilde{r}_3 = 0.92 $$每个Token的Advantage表\nToken序列 Advantage值 数据库 用于 存储 数据 。 -1.06 数据库 是 一个 有组织的 数据集合 0.14 数据库 是 用来 高效 存取 数据 的 软件 0.92 这里代表着每一个token都是Advantage值，例如‘数据库’ ’用于‘ ’存储‘ ’数据‘ 的token都是 -1.\n目标函数 $$ J_{GRPO}(\\theta) = \\frac{1}{G} \\sum_{i=1}^G \\frac{1}{T_i} \\sum_{t=1}^{T_i} \\left[ \\min \\left( \\frac{P_\\theta(a_{i,t}|s_{i,t})}{P_{\\theta'}(a_{i,t}|s_{i,t})} A_i, \\text{clip}\\left(\\frac{P_\\theta(a_{i,t}|s_{i,t})}{P_{\\theta'}(a_{i,t}|s_{i,t})}, 1-\\varepsilon, 1+\\varepsilon\\right) A_i \\right) - \\beta D_{KL}(P_\\theta \\| P_{ref}) \\right] $$其中，$A_i$ 为第 $i$ 个样本的优势值（通常为组内奖励的标准化值），$D_{KL}$ 通常采用无偏估计量：\n$$ D_{KL}(P_\\theta \\| P_{ref}) = \\frac{P_{ref}(a_t|s_t)}{P_\\theta(a_t|s_t)} - \\log \\frac{P_{ref}(a_t|s_t)}{P_\\theta(a_t|s_t)} - 1 $$点评 PPO在计算token的优势的时候，Reward模型只把得分到最后一个token里，其他得分都为0，然后用KL散度乘以一个系数加上刚才的得分 得到Reward对整体句子中每一个token的打分，但是对于大模型回答来说，我们评判的是整体的回答，而不是像游戏中那样在某个场景下只关注当下的动作，因此用KL散度乘系数再加上Reward对最后一个token打分这种计算方式，对于评估大模型生成的回答质量实在有些牵强\n简洁来说，PPO只关注最后一个token的优势值，而我们想关注的是一句话作为整体的每个token的优势值。\nGRPO提供了一种不需要训练状态价值网络，就可以估算每个token优势值的方法，而且这个方法更适合大模型生成强化学习这个场景\n实际代码中，Clip和KL散度用一个就行了。\n其他变体 GRPO - DeepSeekMath (February 2024) DAPO - Decoupled Clip and Dynamic Sampling Policy Optimization (March 2025) GSPO - Group Sequence Policy Optimization (July 2025) GFPO - Group Filtered Policy Optimization (August 2025) DRPO - Decoupled Reward Policy Optimization (October 2025) Training-Free GRPO - Tencent: Training-Free Group Relative Policy Optimization (October 2025) 附录：不同规模模型的推荐学习率参考 以下表格整理了在不同模型规模下，SFT（指令微调）与 DPO（偏好优化）的推荐学习率区间。数据主要基于 LoRA/QLoRA 微调场景。\n模型规模 (Model Scale) SFT 推荐 LR (LoRA) DPO 推荐 LR 备注 (Notes) 0.5B - 1.8B 2e-4 — 5e-4 5e-6 — 1e-5 模型较小，相对“坚韧”，可以使用较大的步长快速收敛。 7B - 8B 5e-5 — 1e-4 1e-6 — 3e-6 主流模型规模（如 Llama 3, Qwen 2.5）。5e-7 通常是下限，过低会导致收敛极慢。 14B - 32B 1e-5 — 5e-5 5e-7 — 1e-6 参数量较大，即使是 LoRA 也需要非常小心，LR 过大容易导致灾难性遗忘。 70B+ 5e-6 — 1e-5 1e-7 — 5e-7 极小的步长，通常配合极大的 Batch Size 进行训练。 关键指标参考：\nGRPO: 学习率通常建议 ≤ 1e-5 (Full) 或 5e-5 (LoRA)，Beta 推荐 0.001。 DPO 监控: 重点关注 rewards/chosen、rewards/rejected 和 rewards/margins。 ","title":"LLM-RL-GRPO-and its Variants"},{"link":"/posts/dpo/","text":" KL 散度（Kullback–Leibler Divergence） $$ KL(P \\parallel Q) = \\sum_{x \\in X} P(x) \\log \\frac{P(x)}{Q(x)} $$含义 P 分布相对于 Q 分布的相似程度。\n性质\nKL 散度的值 大于等于 0。 P 和 Q 越相似，KL 散度越接近 0。 如果 P 和 Q 分布完全一致，则 KL 散度 = 0。 注意： $$ KL(P \\parallel Q) \\neq KL(Q \\parallel P) $$ KL 散度大于等于 0 的直观理解 直观理解：KL 散度是一个非负数，因为我们在比较两个分布时， 只有在完全一致时，它们之间的“差异”才为 0。\n$$ KL(P \\parallel Q) = \\sum_{x \\in X} P(x) \\log \\frac{P(x)}{Q(x)} $$示例\n变量 P(x) Q(x) x = 0 0.2 0.8 x = 1 0.8 0.2 计算过程 $$ KL(P \\parallel Q) = 0.2 \\times \\log \\frac{0.2}{0.8} + 0.8 \\times \\log \\frac{0.8}{0.2} $$直观理解\nKL 散度衡量 P 分布相对于 Q 分布的差异程度。 由于对数函数的凸性，KL 散度始终 大于等于 0。 当 P 和 Q 分布完全一致时，KL 散度 等于 0。 若 P 与 Q 差异越大，KL 散度值越大。 Bradley–Terry 模型（成对比较模型） 基本思想 用于描述多个选手（或元素）之间的成对胜负概率。 假设每个元素 $i$ 都有一个“实力参数” $\\alpha_i \u003e 0$。\n定义：\n$$ P(i \u003e j) = \\frac{\\alpha_i}{\\alpha_i + \\alpha_j} $$其中：\n$\\alpha_i$：第 $i$ 个元素的真实实力； $P(i \u003e j)$：第 $i$ 个元素战胜第 $j$ 个元素的概率。 对数似然函数（Maximum Likelihood Estimation） 模型给出每次对战的概率：\n$$ P(i\u003ej)= \\frac{\\alpha_i}{\\alpha_i + \\alpha_j} $$我们可以计算出“在当前参数 $\\alpha$下，所有比赛结果同时发生的概率：\n$$ L(\\boldsymbol{\\alpha}) = \\prod_{\\text{所有比赛}} P(\\text{胜方} \u003e \\text{负方}) $$这就是所谓的 似然函数（Likelihood Function）。它衡量了：\n给定参数 $\\alpha$，观测到这些比赛结果的可能性有多大”。\n根据观测数据构建似然函数并取对数（假设观测到的对战结果如下表）：\n对战双方 胜方 败方 胜场数 A vs B A B 8 A vs B B A 4 A vs C A C 3 A vs C C A 5 $$ \\ln L = 8 \\ln\\left(\\frac{\\alpha_A}{\\alpha_A + \\alpha_B}\\right)+ 4 \\ln\\left(\\frac{\\alpha_B}{\\alpha_A + \\alpha_B}\\right)+ 3 \\ln\\left(\\frac{\\alpha_A}{\\alpha_A + \\alpha_C}\\right)+ 5 \\ln\\left(\\frac{\\alpha_C}{\\alpha_A + \\alpha_C}\\right) $$通过最大化对数似然，可以求得各选手的相对实力参数。\n参数估计结果 $$ \\alpha_A = 1, \\quad \\alpha_B = \\frac{1}{2}, \\quad \\alpha_C = \\frac{5}{3} $$指定$\\alpha_A$ 的值，可的得到剩下两个的值\n推导新的对战概率 例如计算 $P(B \u003e C)$：\n$$ P(B \u003e C) = \\frac{\\alpha_B}{\\alpha_B + \\alpha_C} = \\frac{1/2}{1/2 + 5/3} \\approx 0.23 $$直观理解：\n模型通过比较胜负记录估计出每个选手的“潜在实力”； 实力越大，战胜其他人的概率越高； 可推广用于比赛预测、排序、推荐系统等任务。 一般的 Loss 函数（Bradley–Terry 模型） 模型目标是最大化对数似然，对应的最小化损失函数（负对数似然）为：\n$$ Loss = - \\mathbb{E}_{(a_x, a_y) \\sim D} \\left[ \\ln \\frac{a_x}{a_x + a_y} \\right] $$强化学习中的比较建模（Pairwise Preference in RLHF） 在强化学习中：\n大模型的输入是 prompt，记作 $x$； 模型的输出（回答）是 response，记作 $y$； 回答 $y$ 的好坏（即“得分”或“实力”）由 Reward 模型 $r(x, y)$ 进行评估。 比较两个回答的优劣概率 对于同一个输入 $x$，有两个不同回答 $y_1$ 和 $y_2$。\n原始形式为：\n$$ P(y_1 \\succ y_2) = \\frac{r(x, y_1)}{r(x, y_1) + r(x, y_2)} $$由于 $r(x, y)$ 可能返回负数，因此引入指数函数，使概率始终为正：\n$$ P(y_1 \\succ y_2) = \\frac{\\exp(r(x, y_1))}{\\exp(r(x, y_1)) + \\exp(r(x, y_2))} $$说明\n$r(x, y)$：Reward 模型给出的得分（越大表示回答越好）； $P(y_1 \\succ y_2)$：模型预测回答 $y_1$ 优于 $y_2$ 的概率； 该形式本质上是 Bradley–Terry 模型 在强化学习（RLHF）中的应用。 偏好概率建模\n对于同一输入 $x$，Reward 模型给出两个回答 $y_w$（获胜回答）和 $y_l$（失败回答）的得分：\n$$ P(y_w \\succ y_l) = \\frac{\\exp(r(x, y_w))}{\\exp(r(x, y_w)) + \\exp(r(x, y_l))} $$Sigmoid 函数定义为：\n$$ \\sigma(x) = \\frac{1}{1 + \\exp(-x)} $$损失函数（Loss Function） Reward 模型的目标是最小化负对数似然（Negative Log-Likelihood）：\n$$ Loss = -\\mathbb{E}_{(x, y_w, y_l) \\sim D} \\left[ \\ln \\frac{\\exp(r(x, y_w))}{\\exp(r(x, y_w)) + \\exp(r(x, y_l))} \\right] $$等价地：\n$$ Loss = - \\mathbb{E}_{(x, y_w, y_l) \\sim D} \\left[ \\ln \\frac{1}{1 + \\exp(r(x, y_l) - r(x, y_w))} \\right] $$最终可写为简洁的 Sigmoid 形式：\n$$ Loss = - \\mathbb{E}_{(x, y_w, y_l) \\sim D} \\left[ \\ln \\sigma(r(x, y_w) - r(x, y_l)) \\right] $$DPO 的训练目标（Direct Preference Optimization） 基本定义 奖励函数：$r(x, y)$，其中 $x$ 为 prompt，$y$ 为 response 基准模型（reference model）：$\\pi_{\\text{ref}}(y|x)$ 训练模型（policy model）：$\\pi(y|x)$ DPO 的目标是在获得高奖励的同时，使新模型不偏离参考模型：\n$$ \\max_{\\pi} \\mathbb{E}_{x \\sim D, y \\sim \\pi(y|x)}[r(x,y)] - \\beta \\mathbb{D}_{\\mathrm{KL}}\\big(\\pi(y|x) \\parallel \\pi_{\\mathrm{ref}}(y|x)\\big) $$ 第一项：希望得到尽可能多的奖励； 第二项：限制新模型与基准模型的分布差距； $\\beta$：超参数，用于平衡两者。 目标函数的等价形式推导 将 KL 散度展开：\n$$ \\max_{\\pi} \\mathbb{E}_{x, y \\sim \\pi} [r(x, y)] - \\beta \\mathbb{E}_{x, y \\sim \\pi} \\left[\\log \\frac{\\pi(y|x)}{\\pi_{\\text{ref}}(y|x)} \\right] $$等价地（加负号取最小值且两项同时除以$\\beta$）：\n$$ \\min_{\\pi} \\mathbb{E}_{x, y \\sim \\pi} \\left[\\log \\frac{\\pi(y|x)}{\\pi_{\\text{ref}}(y|x)} - \\frac{1}{\\beta} r(x, y)\\right] $$将减法项写成对数形式\n$$ = \\min_{\\pi} \\; \\mathbb{E}_{x, y \\sim \\pi} \\left[ \\log \\frac{\\pi(y|x)}{\\pi_{\\text{ref}}(y|x)} - \\log \\exp\\left(\\frac{1}{\\beta} r(x, y)\\right) \\right] $$合并为单个对数项\n$$ = \\min_{\\pi} \\; \\mathbb{E}_{x, y \\sim \\pi} \\left[ \\log \\frac{\\pi(y|x)} {\\pi_{\\text{ref}}(y|x) \\exp\\left(\\frac{1}{\\beta} r(x, y)\\right)} \\right] $$引入归一化常数 $Z(x)$\n$$ = \\min_{\\pi} \\; \\mathbb{E}_{x, y \\sim \\pi} \\left[ \\log \\frac{\\pi(y|x)} {\\pi_{\\text{ref}}(y|x) \\exp\\left(\\frac{1}{\\beta} r(x, y)\\right) \\frac{1}{Z(x)} Z(x)} \\right] $$拆出常数项\n$$ =\\min_{\\pi} \\; \\mathbb{E}_{x, y \\sim \\pi} \\left[ \\log \\frac{\\pi(y|x)}{\\frac{1}{Z(x)} \\pi_{\\text{ref}}(y|x) \\exp\\left(\\frac{1}{\\beta} r(x, y)\\right)} -\\log Z(x) \\right] $$求解最优策略分布 定义：\n$$ \\pi^*(y|x) = \\frac{1}{Z(x)} \\, \\pi_{\\text{ref}}(y|x) \\exp\\left(\\frac{1}{\\beta} r(x, y)\\right) $$其中：\n$$ Z(x) = \\sum_y \\pi_{\\text{ref}}(y|x) \\exp\\left(\\frac{1}{\\beta} r(x, y)\\right) $$表示归一化常数，是我们自己定义的，这么定义的原因是为了能和后面的式子消去，简化式子。\n解释：最优策略 $\\pi^*(y|x)$ 是在参考模型分布上，通过奖励函数加权后的归一化分布。\n代入 $\\pi^*(y|x)$，目标可写为：\n$$ \\min_{\\pi} \\mathbb{E}_{x \\sim D} \\left[ D_{KL}(\\pi(y|x) \\parallel \\pi^*(y|x)) \\right] $$因此最优时：\n$$ \\pi(y|x) = \\pi^*(y|x) = \\frac{1}{Z(x)} \\pi_{\\text{ref}}(y|x) \\exp\\left(\\frac{1}{\\beta} r(x, y)\\right) $$奖励函数反求形式 由上式反推奖励： 由 DPO 的最优策略定义：\n$$ \\pi^*(y|x) = \\frac{1}{Z(x)} \\pi_{\\text{ref}}(y|x) \\exp \\left(\\frac{1}{\\beta} r(x, y)\\right) $$可得：\n$$ \\exp\\left(\\frac{1}{\\beta} r(x, y)\\right) = \\frac{\\pi(y|x)}{\\pi_{\\text{ref}}(y|x)} Z(x) $$两边取对数并乘以 $\\beta$，得到：\n$$ r(x, y) = \\beta \\ln\\left( \\frac{\\pi(y|x)}{\\pi_{\\text{ref}}(y|x)} Z(x) \\right) $$即：\n$$ r(x, y) = \\beta \\ln \\frac{\\pi(y|x)}{\\pi_{\\text{ref}}(y|x)} + \\beta \\ln Z(x) $$ 解释： 奖励函数可以理解为模型输出相对参考模型的“对数优势”。 它衡量了当前模型相对参考模型在特定回答上的改进幅度\n与偏好建模的联系 在成对偏好比较中，假设有更优回答 $y_w$ 与较差回答 $y_l$，则最终的损失函数为：\n$$ -\\ln \\sigma(r(x, y_w) - r(x, y_l)) = - \\ln \\sigma\\left( \\beta \\ln \\frac{\\pi(y_w|x)}{\\pi_{\\text{ref}}(y_w|x)} - \\beta \\ln \\frac{\\pi(y_l|x)}{\\pi_{\\text{ref}}(y_l|x)} \\right) $$ 这表明 DPO 的优化目标实际上是将 Reward 模型的比较学习 转换为对 策略分布比值 的直接优化。\n总结 DPO的核心洞察在于原始强化学习问题存在解析最优解，表明最优策略与奖励函数存在一一映射关系。DPO将此关系反解后代入Bradley-Terry偏好模型，将对奖励函数的似然最大化，等价地转化为直接对策略的似然最大化。因此，优化DPO损失函数即是在直接寻找那个能同时最大化人类偏好概率且满足最优解形式的策略，避免了先用偏好数据拟合奖励模型再进行强化学习过程寻找最优策略\n参考论文 DPO - Direct Preference Optimization: Your Language Model is Secretly a Reward Model (Rafailov et al., 2023) ","title":"LLM-RL-DPO"},{"link":"/posts/ppo/","text":" 基础概念I 如果你不熟悉强化学习，学习ppo了解这些基础知识就足够了 Action Space: 可选择的动作，比如 {left, up, right}\nPolicy: 策略函数，输入 State，输出 Action 的概率分布。一般用 π 表示。\n$\\pi(left|s_t) = 0.1$,在状态$s_t$下，采取的动作为left的概率为0.1 $\\pi(up|s_t) = 0.2$ , $\\pi(right|s_t) = 0.7$, 这个状态下的 所有动作 概率之和 为1 Trajectory: 轨迹，用$τ$表示，一连串状态和动作的序列。又称Episode, Rollout。 ${s0,a0,s1,a1,…}$\n$s_{t+1}=f(st,at)$ 确定状态转移\n$s_{t+1}=P(⋅|st,at)$随机状态转移\nReturn: 回报，从当前时间点到游戏结束的 Reward 的累积和。\n期望：每个可能结果的概率与其结果值的乘积之和\n$\\mathbb{E}(x)_{x \\sim p(x)} = \\sum x \\cdot p(x) \\approx \\frac{1}{n} \\sum_{i=1}^n x, \\quad x \\sim p(x)$\n$\\mathbb{E}[f(\\tau)] \\approx \\frac{1}{N} \\sum_{n=1}^N f(\\tau^n)$\nNote 运用的是蒙特卡洛思想，从分布p(x)中随机采样n次求平均，n趋于无穷时样本平均会趋近于期望值/大数定律，期望可以用样本平均近似\n轨迹概率：一条轨迹 $\\tau = (s_0, a_0, s_1, a_1, \\dots, s_T, a_T)$的概率是： $P_\\theta(\\tau) = P(s_0, a_0, s_1, a_1, \\dots, s_T, a_T)$ 概率论基本公式： $P(x_1, x_2, \\dots, x_n) = \\prod_{i=1}^n P(x_i \\mid x_1, \\dots, x_{i-1})$ 把它用在轨迹上，就可以逐步展开： $P_\\theta(\\tau) = P(s_0) \\cdot P(a_0 \\mid s_0) \\cdot P(s_1 \\mid s_0, a_0) \\cdot P(a_1 \\mid s_1,a_0,s_0) \\cdot \\dots$ 在强化学习里，环境通常假设是 马尔可夫决策过程 (MDP)，即：\n下一状态只依赖于当前状态和动作： $P(s_{t+1} \\mid s_0, a_0, \\dots, s_t, a_t) = P(s_{t+1} \\mid s_t, a_t)$ 动作只依赖于当前状态： $P(a_t \\mid s_0, a_0, \\dots, s_t) = \\pi_\\theta(a_t \\mid s_t)$ 于是上面的展开可以简化为： $\\ P_\\theta(\\tau) = P(s_0) \\prod_{t=0}^T \\pi_\\theta(a_t \\mid s_t) \\, P(s_{t+1} \\mid s_t, a_t)$ $\\pi_\\theta(a_n^t | s_n^t)$：策略在状态 $s_n^t$下选择动作 $a_n^t$ 的概率（依赖参数 $\\theta$）。 $P(s_{n}^{t+1}|s_n^t, a_n^t)$：环境的转移概率（不依赖 $\\theta$）。 Important 目标： 训练一个Policy神经网络 $π_{\\theta}$ ,在所有的状态S下，给出相应的Action，得到的Return的期望最大 or 训练一个Policy神经网络 $\\pi_{\\theta}$ ,在所有Trajectory中，得到的Return的期望最大\nPPO原理-part1 我们将刚才的目标翻译成数学表达式，就是希望 $\\mathbb{E}[R(\\tau)]_{\\tau \\sim P_\\theta(\\tau)} = \\sum_{\\tau} R(\\tau) P_\\theta(\\tau)$ 越大越好\n$P_\\theta(\\tau)$ :在策略$\\pi_\\theta$下，采样到轨迹 $\\tau$ 的概率。 $R(\\tau)$：轨迹 $\\tau$ 的总回报（Return）。 Note 为什么$R(\\tau)$不受$\\theta$影响？ $R(τ)=∑_{t=0}^{T−1}γ^{t}r(s_t,a_t)$ 因为一旦一条轨迹被确定下来，这条轨迹的回报R是由每个状态下的每个动作的得分之和决定的，又因为越远的动作影响对当前状态影响越小，所以要乘上一个衰减因子$\\gamma$\n我们只能改变神经网络里的参数 θ ,不能改变Reward，所以对 θ求梯度\n$$ \\nabla \\mathbb{E}[R(\\tau)]_{\\tau \\sim P_{\\theta}(\\tau)} $$$$ = \\nabla \\sum_{\\tau} R(\\tau) P_\\theta(\\tau) $$$$ = \\sum_{\\tau} R(\\tau) \\nabla P_\\theta(\\tau) $$$$ = \\sum_{\\tau} R(\\tau) \\nabla P_\\theta(\\tau) \\frac{P_\\theta(\\tau)}{P_\\theta(\\tau)} $$$$ = \\sum_{\\tau} P_\\theta(\\tau) R(\\tau) \\frac{\\nabla P_\\theta(\\tau)}{P_\\theta(\\tau)} $$$$ \\approx \\frac{1}{N} \\sum_{n=1}^N R(\\tau^n) \\frac{\\nabla P_\\theta(\\tau^n)}{P_\\theta(\\tau^n)} $$ Tip $\\tau^n$表示第n条轨迹（trajectory）;多次采样取平均\n$$ = \\frac{1}{N} \\sum_{n=1}^N R(\\tau^n) \\nabla \\log P_\\theta(\\tau^n) $$ Tip 因为$\\nabla \\log f(x) = \\frac{\\nabla f(x)}{f(x)}$\n$$ = \\frac{1}{N} \\sum_{n=1}^N R(\\tau^n) \\nabla \\log \\prod_{t=1}^{T_n} P_\\theta(a_n^t | s_n^t) $$ Tip 这里是轨迹分解，因为对 $\\theta$ 求梯度只关注 $P_\\theta$ ,就是前面基础知识中的 $\\pi_\\theta$,环境转移概率不包含 $\\theta$ ,故当作常数，求梯度就没了\n$$ = \\frac{1}{N} \\sum_{n=1}^N R(\\tau^n) \\sum_{t=1}^{T_n} \\nabla \\log P_\\theta(a_n^t | s_n^t) $$$$ = \\frac{1}{N} \\sum_{n=1}^N \\sum_{t=1}^{T_n} R(\\tau^n)\\,\\nabla \\log P_\\theta(a_n^t | s_n^t) $$到这里，我们求出了对于所有可能的Trajectory，期望的最大梯度，用个梯度去更新神经网络参数，就是 Policy gradient梯度策略算法\nPPO原理-part2 $$ \\frac{1}{N} \\sum_{n=1}^N \\sum_{t=1}^{T_n} R(\\tau^n)\\,\\nabla \\log P_\\theta(a_n^t | s_n^t) $$刚刚我们得到的这个式子，很明显有两点可以改进\n应该看在$S_n$状态下采取动作$a_n$之后的Reward，而不是整个Trajectory（轨迹）的Reward，因为动作action只能影响后面的 action是只会影响后面的动作，但是影响会逐步衰减 由此我们修改Reward公式为\n$$ R(\\tau^n) \\to \\sum_{t'=t}^{T_n} \\gamma^{t'-t}r_{t'}^n = R_t^n $$表示从当前t时刻开始到最后的累积Reward，\n$\\gamma$\n是衰减因子，表示离当前动作越远，在当前状态下采取当前动作的Reward越小，核心就是想突出当前状态下采取当前动作对于Reward的影响.\n另一个值得注意的点是局势也会影响算法的稳定性，比如说在好的局势下，采取什么动作都会得分（顺风局当赢的感觉），但是这样就偏离我们的初衷——我们想知道在某种状态下采取哪些动作更好，模型需要明确区分哪个动作“更好”\n因此我们希望让相对好的action的Reward得分增加，相对差的action的Reward减少，这样会加快训练速度。\n由此我们减去一个Baseline\n$$ \\frac{1}{N} \\sum_{n=1}^N \\sum_{t=1}^{T_n} \\bigl(R_t^n - B(s_n^t)\\bigr) \\nabla \\log P_\\theta(a_n^t \\mid s_n^t) $$基础概念II 强化学习中有几个重要的定义，可以简化上述式子\n$\\textbf{Action-Value Function}(动作价值函数)$ $Q_\\theta(s,a) 在 state \\ \\ s 下，做出 Action \\ \\ a的回报的期望$\n$\\textbf{State-Value Function} (状态价值函数)$ $V_\\theta(s) 在 state \\ \\ s 下，回报的期望$。\n$\\textbf{Advantage Function}(优势函数)$ $A_\\theta(s,a) = Q_\\theta(s,a) - V_\\theta(s) \\quad$ $在 state \\ \\ s 下，做出 Action \\ \\ a，比其他动作能带来多少优势。$于是原式被转化为：\n$$ \\frac{1}{N} \\sum_{n=1}^{N} \\sum_{t=1}^{T_n} A_\\theta(s_n^t, a_n^t) \\nabla \\log P_\\theta(a_n^t | s_n^t) $$现在我们得到了理论上表达式，但是如何表示实际采样的呢？\n一次采样：\n$Q_\\theta(s_t, a) = r_t + \\gamma \\cdot V_\\theta(s_{t+1})$\n$A_\\theta(s_t, a) = r_t + \\gamma \\cdot V_\\theta(s_{t+1}) - V_\\theta(s_t)$\n$V_\\theta(s_{t+1}) \\approx r_{t+1} + \\gamma \\cdot V_\\theta(s_{t+2})$\nTip 上面我们分别对动作价值函数和状态价值函数进行了一次采样，对于动作价值函数Q来说，采用的 动作是固定的，因此可以用等号来表示，对于状态价值函数，采取的动作a还没有固定，因此用约等于表示一次采样\n多次采样，虽然会增大方差，但是会减少偏差\n$$ A_\\theta^1(s_t, a) = r_t + \\gamma V_\\theta(s_{t+1}) - V_\\theta(s_t) $$$$ A_\\theta^2(s_t, a) = r_t + \\gamma r_{t+1} + \\gamma^2 V_\\theta(s_{t+2}) - V_\\theta(s_t) $$$$ A_\\theta^3(s_t, a) = r_t + \\gamma r_{t+1} + \\gamma^2 r_{t+2} + \\gamma^3 V_\\theta(s_{t+3}) - V_\\theta(s_t) $$$$ \\vdots $$$$ A_\\theta^T(s_t, a) = r_t + \\gamma r_{t+1} + \\gamma^2 r_{t+2} + \\gamma^3 r_{t+3} + \\cdots + \\gamma^T r_T - V_\\theta(s_t) $$定义新函数，简化表示\n$$ \\delta_t^V = r_t + \\gamma V_\\theta(s_{t+1}) - V_\\theta(s_t) $$$$ \\delta_{t+1}^V = r_{t+1} + \\gamma V_\\theta(s_{t+2}) - V_\\theta(s_{t+1}) $$$$ A_\\theta^1(s_t, a) = \\delta_t^V $$$$ A_\\theta^2(s_t, a) = \\delta_t^V + \\gamma \\delta_{t+1}^V $$$$ A_\\theta^3(s_t, a) = \\delta_t^V + \\gamma \\delta_{t+1}^V + \\gamma^2 \\delta_{t+2}^V $$$$ \\vdots $$所以算法（比如 GAE）会综合使用多个步长 k 的估计，构建出一个平衡版本（平衡方差和偏差）： 多次采样，并乘上对应衰减权重\n$$ A_\\theta^{GAE}(s_t, a) = (1 - \\lambda)\\big(A_\\theta^1 + \\lambda A_\\theta^2 + \\lambda^2 A_\\theta^3 + \\cdots \\big) $$$$ \\lambda = 0.9: \\quad A_\\theta^{GAE} = 0.1 A_\\theta^1 + 0.09 A_\\theta^2 + 0.081 A_\\theta^3 + \\cdots $$$$ = (1 - \\lambda)\\big(\\delta_t^V + \\lambda(\\delta_t^V + \\gamma \\delta_{t+1}^V) + \\lambda^2(\\delta_t^V + \\gamma \\delta_{t+1}^V + \\gamma^2 \\delta_{t+2}^V) + \\cdots \\big) $$$$ = (1 - \\lambda)\\big(\\delta_t^V(1 + \\lambda + \\lambda^2 + \\cdots) + \\gamma \\delta_{t+1}^V (\\lambda + \\lambda^2 + \\cdots) + \\cdots \\big) $$$$ = (1 - \\lambda)\\big(\\delta_t^V \\tfrac{1}{1-\\lambda} + \\gamma \\delta_{t+1}^V \\tfrac{\\lambda}{1-\\lambda} + \\cdots \\big) $$$$ = \\sum_{b=0}^{\\infty} (\\gamma \\lambda)^b \\, \\delta_{t+b}^V $$最后得到这三个表达式：\n$$ \\delta_t^V = r_t + \\gamma V_\\theta(s_{t+1}) - V_\\theta(s_t) $$$$ A_\\theta^{GAE}(s_t, a) = \\sum_{b=0}^{\\infty} (\\gamma \\lambda)^b \\, \\delta_{t+b}^V $$$$ \\frac{1}{N} \\sum_{n=1}^N \\sum_{t=1}^{T_n} A_\\theta^{GAE}(s_n^t, a_n^t) \\nabla \\log P_\\theta(a_n^t \\mid s_n^t) $$PPO原理-part3 通过运行模型来采集数据，这样就会导致采集数据时间过长，这也是ppo需要解决的问题,对于此我们采用重要性采样来解决\n重要性采样 $$ \\mathbb{E}(f(x))_{x \\sim p(x)} = \\sum_x f(x) p(x) $$$$ = \\sum_x f(x) p(x) \\frac{q(x)}{q(x)} $$$$ = \\sum_x f(x) \\frac{p(x)}{q(x)} q(x) $$$$ = \\mathbb{E}\\!\\left(f(x) \\frac{p(x)}{q(x)}\\right)_{x \\sim q(x)} $$$$ \\approx \\frac{1}{N} \\sum_{n=1}^N f(x) \\frac{p(x)}{q(x)}, \\quad x \\sim q(x) $$由此我们可以变换我们的公式，由On-Policy 转向Off-Policy\n$$ \\frac{1}{N} \\sum_{n=1}^N \\sum_{t=1}^{T_n} A_{\\theta}^{GAE}(s_n^t, a_n^t) \\nabla \\log P_\\theta(a_n^t \\mid s_n^t) $$$$ = \\frac{1}{N} \\sum_{n=1}^N \\sum_{t=1}^{T_n} A_{\\theta'}^{GAE}(s_n^t, a_n^t) \\frac{P_\\theta(a_n^t \\mid s_n^t)}{P_{\\theta'}(a_n^t \\mid s_n^t)} \\nabla \\log P_\\theta(a_n^t \\mid s_n^t) $$$$ = \\frac{1}{N} \\sum_{n=1}^N \\sum_{t=1}^{T_n} A_{\\theta'}^{GAE}(s_n^t, a_n^t) \\frac{\\nabla P_\\theta(a_n^t \\mid s_n^t)}{P_{\\theta'}(a_n^t \\mid s_n^t)} $$这里我们的做法是用旧策略(off-policy)的优势函数乘一个比例系数去模拟新的策略函数(on-policy)\n最后去掉梯度我们可以得到最终的loss函数（中间对$\\theta$求梯度是为了消去不影响$theta$的状态转移概率）\n$$ \\text{Loss} = -\\frac{1}{N} \\sum_{n=1}^N \\sum_{t=1}^{T_n} A_{\\theta'}^{GAE}(s_n^t, a_n^t) \\frac{P_\\theta(a_n^t \\mid s_n^t)}{P_{\\theta'}(a_n^t \\mid s_n^t)} $$注意：旧策略与新策略之间的分布不能差距过大，否则很难学到有用的经验\n可以采用KL散度进行约束或者clip函数（限定那个新旧两种策略之间的比例在1左右，不能相差过大）\n$$ \\text{Loss}_{ppo_1} = -\\frac{1}{N} \\sum_{n=1}^N \\sum_{t=1}^{T_n} A_{\\theta'}^{GAE}(s_n^t, a_n^t) \\frac{P_\\theta(a_n^t \\mid s_n^t)}{P_{\\theta'}(a_n^t \\mid s_n^t)} + \\beta \\, KL(P_\\theta, P_{\\theta'}) $$$$ \\text{Loss}_{ppo_2} = -\\frac{1}{N} \\sum_{n=1}^N \\sum_{t=1}^{T_n} \\min \\Bigg( A_{\\theta'}^{GAE}(s_n^t, a_n^t) \\frac{P_\\theta(a_n^t \\mid s_n^t)}{P_{\\theta'}(a_n^t \\mid s_n^t)} , \\text{clip}\\left( \\frac{P_\\theta(a_n^t \\mid s_n^t)}{P_{\\theta'}(a_n^t \\mid s_n^t)}, 1-\\epsilon, 1+\\epsilon \\right) A_{\\theta'}^{GAE}(s_n^t, a_n^t) \\Bigg) $$ 参考论文 PPO - Proximal Policy Optimization Algorithms (Schulman et al., 2017) ","title":"LLM-RL-PPO"},{"link":"/posts/ai-thinking/","text":"从工程实践看AI的本质 很多突破性的AI技术，其实都在模仿自然界已经存在了数百万年的机制。\n比如卷积神经网络的层级结构，本质上是在模拟视觉皮层的工作方式——从简单的边缘检测逐步抽象到复杂的物体识别。Transformer的自注意力机制，某种程度上也像是群体智能的数学建模，每个token都能\u0026quot;看到\u0026quot;全局，但各司其职。\n这让我开始思考：AI的发展，到底是在创造全新的智能，还是在用数学语言重新诠释自然界早已验证过的机制？也许后者的成分更大一些。大自然用了几十亿年的进化，已经帮我们找到了很多最优解，我们只是在学习如何用代码去实现它。\nAI发展背后的代价 在为AI的能力感到惊叹的同时，我也开始关注另一个问题：这背后的资源消耗。\n训练一个大语言模型需要的计算资源是惊人的。我看到过一些数据，一个70B参数的模型训练一次可能消耗数百万度电。如果把实验室的服务器24小时跑满，一个月的电费账单会让人倒吸一口凉气。更不用说这些电力消耗对应的碳排放了。\n数据存储也是个问题。TB级别的训练数据从采集、预处理到存储，整个链路下来，硬盘空间和能源消耗都是巨大的。\n这让我想到一个矛盾：AI被寄希望于解决人类面临的很多问题，包括环境问题。但AI本身的发展，又在制造新的环境压力。这个矛盾如何解决？\n或许答案在于效率的提升——如何用更少的资源训练出更好的模型，如何让模型的推理更加节能。这可能是未来AI发展中一个重要的方向，也可能蕴含着巨大的商业机会。\nAI如何改变工作方式 深度体验使用agent处理任务后，我发现工作的性质正在发生变化。\n以前一个项目可能需要花80%的时间在编码实现上，20%在设计和测试。现在这个比例倒过来了——核心的20%是思考如何设计方案、如何让AI理解你的意图、如何优化结果；大约70%的实现工作AI可以协助完成；剩下10%是那些细节的打磨和边界情况的处理，这部分AI往往做得不够好，需要人工介入。\n这种转变带来了一些有意思的变化：\n首先是对人的要求不同了。以前可能更看重编码能力和具体技术栈的熟练度，现在更看重的是系统设计能力、问题分解能力，还有如何高效地与AI协作，以及审查各种情况都能力。\n其次是知识的时效性变强了。某个best practice可能几个月后就过时了。这要求我们保持持续学习的状态，同时也要学会抓住那些不变的本质——算法原理、系统设计思想这些更底层的东西。\n语言和交互方式的演进 在使用AI编程助手的过程中，我越来越感受到一个问题：自然语言和编程语言之间存在巨大的gap。\n自然语言太模糊，很难精确表达需求；代码又太细节，让人陷入具体实现而忽略整体设计。我们需要的可能是介于两者之间的某种表达方式——既能清晰表达意图，又不用陷入太多细节。\n也许未来会出现某种\u0026quot;伪代码+\u0026ldquo;的语言，专门用于人类和AI之间的交互。或者说，编程的范式会发生变化——我们不再是在写具体的代码，而是在表达意图、描述约束，然后由AI来生成具体实现。\n这不是说传统编程会消失，而是交互方式会更加灵活和多样化。\n保持开放但不焦虑 看到AI的快速发展，很容易产生焦虑——会不会被替代，要不要转行，需要学什么才能不被淘汰。\n但换个角度想，AI带来的更多是机会而不是威胁。它降低了很多领域的门槛，让更多人能够接触到以前需要专业知识才能做的事情。从这个意义上说，AI推动的是知识的平权。\n对于普通人来说，重要的可能不是去焦虑AI会带来什么，而是想清楚如何利用它。现在Claude、GPT、Gemini这些强大的工具，大部分功能都可以免费使用。与其花时间焦虑，不如花时间探索如何把这些工具用到自己的工作和学习中。\nAI可以给你的建议，可能比很多人给你的建议都更加客观和全面（虽然也需要critical thinking去判断）。这是一个从未有过的时代——你可以随时随地和一个知识渊博的\u0026quot;助手\u0026quot;对话，这在以前是不可想象的。\n一些老生常谈但是正确无比的facts AI发展很快，但不必过于焦虑。重要的是保持开放的心态，持续学习，同时学会利用这些工具。\n生产力的提升终究会让生活变得更好，也会创造新的机会。与其焦虑变化，不如拥抱变化。\nAI时代的核心竞争力，可能不再是掌握某项具体技能，而是学习能力、思考能力，以及如何与AI协作的能力。这些更底层的能力，才是长期有价值的。\n","title":"Some Obervations and Experience"},{"link":"/posts/paper2/","text":"Lost In the Middle 我们在这项工作中观察到的 U 形曲线，与心理学中的一个现象有关——系列位置效应（serial-position effect）（Ebbinghaus，1913；Murdock Jr，1962）。这一效应指出：在让人们自由回忆一串列表元素的实验中，人类往往最容易记住列表中的第一个和最后一个元素。\n系列位置效应在研究人类如何形成短期记忆和长期记忆方面具有重要意义。\n在语言模型中观察到类似系列位置效应的现象是有些令人惊讶的，因为 Transformer 语言模型底层的自注意力机制在理论上能够同等地从上下文中的任何位置检索信息。\nPositional Biases Shift as Inputs Approach Context Window 论文提出了一个关键变量：\nRelative Input Length (输入长度 ÷ 模型最大 context window)** $$ L_{rel}=\\frac{L_{input}}{L_{max}}$$衡量输入文本长度与上下文窗口大小比值\n并首次系统研究：\nLiM 何时出现 何时消失 消失后会出现什么新现象 检索与推理之间的关系（推理是否依赖检索？） 最终得到三个主要发现：\n主要发现 1：LiM 在所有模型中都存在（但只在 L_rel ≤ 0.5 时） 大量实验显示： 当输入占模型 Context Window 的 0%～50% 时：\nLiM 非常明显 开头与结尾准确率高 中间显著下降 当输入超过 50% Context Window（L_rel \u0026gt; 0.5）后：** LiM 开始变弱 最终完全消失 这解释了为什么：\n短输入研究 → 发现 LiM 超长 context LLM（100k tokens） → 看不到 LiM 它们研究的其实是不同 $L_{rel}$ 区间。\n主要发现 2：Primacy 急剧下降导致 LiM 消失 当输入接近 context window 上限时：\nRecency bias（尾部信息偏置）仍然稳定甚至增强 Primacy bias（开头的信息）急剧下降 最终变成：\n模型基本只擅长处理“接近结尾的位置” → 出现 Distance-based Bias\n即：\n距离结尾越近，表现越好；距离越远越差。\n这也是为什么：\n长文本 LLM 更依赖“把重要内容放在结尾”（如 RAG 系统会将 evidence 放在尾部）\n同时\n位置准确率排序变成：\nLast \u0026gt; Middle \u0026gt; First\n文章认为这是 context window 压力导致的记忆衰减模式，非常类似于 transformer 的 attention 衰减。\n主要发现 3：推理成功强烈依赖于“检索成功”（Retrieval → Reasoning） 作者首次构造 Retrieval–Reasoning Minimal Pairs：\n每个 reasoning 问题都有一个对应的 retrieval 问题， retrieval 问题的答案被包含在文中一个明确句子里。\n若检索失败 (RT=0)，推理仍正确的概率只有：35%–52% 若检索成功 (RT=1)，推理正确率直接升至：72%–82%\n也就是说：\n推理的位置信偏置几乎完全继承于检索偏置。\n只要检索成功，推理偏置就几乎消失。\nOn the Emergence of Position Bias in Transformers 背景: LIM 动机：分析原因 方法: 图论(graph-theoretic)框架，定量分析，不同掩码／位置编码方案进行了定性 或定量分析，看哪些情况下会出现LIM 结果:\n如果模型使用某些掩码模式（比如典型的自回归 Transformer 中的因果掩码／过去不能看未来），那么序列最开头或最结尾的 token 往往具有更强的“接收／聚合”信息能力（因为路径更短、更直接、注意力度更集中）。\n中间位置的 token 由于可能既要“接受”前面很多信息、又要“传递”给后面很多，反而在“影晌力”或“被关注程度”上处于劣势。换句话说，模型在结构上对中段位置天然更弱。\n位置编码和掩码设计是关键：如果设计得当，可以减弱这种偏向。但在很多标准模型设定下，这种偏向“自动”产生。也就是说，并不是训练数据本身唯一原因：模型结构／掩码本身也“内建”了偏向。\n该偏向在实际任务中可能带来问题：例如，当你希望模型关注长序列中的中段细节（如法律文档中间页、对话中的某次中间转折）时，这种结构偏向可能会使模型“忘记”中段内容，或者中段内容被弱化。报导中称：“研究已显示大型语言模型在长文档或会话中倾向于强调开头和结尾信息，而忽视中间。”\n总结: LIM是由Transformer的因果掩码机制天然造成的，系统误差\nLOST IN THE MIDDLE: AN EMERGENT PROPERTY FROM INFORMATION RETRIEVAL DEMANDS IN LLMS 作者指出：\n他们提出一个新的视角：\n“lost in the middle 是训练时的信息检索任务需求 + 模型架构共同导致的 适应性行为，而不是缺陷。”\n他们借鉴了心理学中的 human memory：\nFree recall → 前面更容易记 Running span → 后面更容易记 因此形成 U 型曲线。 论文的结论: Lost-in-the-middle arises because：\nLong-term retrieval demand → primacy（靠 attention sinks + causal mask） Short-term retrieval demand → recency Both demands coexist in pre-training → U-shaped behavior → 这不是 flaw，而是 optimal adaptation under constraints 大模型的表示能力更强 更强的网络能学会：\n更好地压缩长距离依赖\n更好地关联中间 token 与 query\n对中间内容的 embedding 表示质量更高\n为什么 next-token 预测会自然产生 primacy + recency？ 训练目标是：\n预测下一个 token（next-token prediction）。\n这个目标天然引入两个偏置： （1）Recency（短程）偏置来自语言本身\n下一个词最依赖最近几个词 模型学到“最近的信息最重要” → 强 Recency（末尾 recall 强） （2）Primacy（长程）偏置来自模型结构 因果 mask + sink 模型强化序列开头 模型在 long-context QA 时必须能检索任何远处 token 所以开头被当成“长期记忆 anchor” 因此： next-token prediction 自然生成 Primacy + Recency 并最终形成 U 形曲线（lost-in-the-middle）。\n如何通过架构或 sink 干预改变 LITM？ 论文及相关研究告诉我们几种方式能改变 lost-in-the-middle。\n(1）弱化或去掉 attention sink\n方法包括：\n给 BOS token 做 dropout（论文实验） 修改 embedding 初始化 删除 sink heads 添加 anti-sink loss（抵消 ingoing attention） 论文发现：\n去掉 sink → Primacy 消失，但 Recency 保留。\n（2）改变模型架构（从 autoregressive → bidirectional）\n如：\nT5 BERT Mamba（序列建模但无强因果 mask） 结果：\nbidirectional 模型不产生 lost-in-the-middle。\n(3) 改变位置编码或引入 RoPE scaling\n更弱的位置偏置 → 减弱 Primacy。 (4）改变训练任务（Free Recall vs RS 的比例）\n论文中的 combined task 说明：\n增加 RS → 强调结尾 → 减弱 Primacy\n增加 FR → 强调开头 → 增强 Primacy\n（5）使用 memory-augmented 架构**\nIdentifying and Evaluating Inactive Heads in Pretrained LLMs 一、核心问题 这篇论文研究了大型语言模型（LLM）中 “注意力头”（attention heads） 的非活跃（inactive／dormant）现象。具体来说：\n在 Transformer 架构中，多头注意力（multi-head attention）通过多个 “头（head）” 并行计算，让模型从不同角度关注输入序列。 然而，已有研究发现一些注意力头的行为并不像“真正起作用”那样：例如它们的大部分注意力集中到第一个 token 或者某些“sink”（汇聚点）token 上，而这些 token 在语义上却可能并不重要。即所谓 “attention sinks” 现象。 本文提出：这些表现 “像在但是没用” 的头，可以视为“非活跃注意力头”（dormant attention heads）或“非活跃头”（inactive heads）。研究的目标是：**如何识别这些头？它们确实对模型输出贡献很小吗？**如果是的话，是否可将它们移除或屏蔽，从而减少计算冗余。 主要发现 论文报道了多个有趣结果，以下为摘要：\n在所测试的模型上（多个预训练 LLM 家族），“非活跃头”占比并非极低：作者发现平均超过 12% 的注意力头可以被视为非活跃，而屏蔽它们后，模型在某些任务（如 MMLU）上的准确率仍可维持在原模型水平以内（例如下降不到 1%） 。 值得注意的是，用传统 “First Token” 方法（只看第一个 token 注意力聚焦）去识别非活跃头，会低估其比例：说白了，仅靠“看它把注意力投给第一个 token”这种指标，漏检很多非活跃头。作者指出，这种方法平均会漏掉 7% 以上的头。 [ 在不同模型规模之间、或者预训练 vs 微调之间，注意力头行为有显著差异：例如微调 (finetuning) 对注意力头是否活跃的改变很小，说明很多头的“非活跃状态”在预训练阶段就已形成。 输入文本特征也会影响头是否被判为非活跃：换句话说，一个头可能在某些输入上活跃、在其他输入上非活跃，状态有依赖于具体输入。 不同的情况，注意力头的活跃程度不同（对指定问题的解答能力不同）\nMOTIF: Modular Thinking via Reinforcement Fine‑tuning in LLMs 因为 LLM 有 context 长度限制，一次性写太长的思维会丢失注意力。\n论文目的就是：\n通过多轮，让模型相当于“超长思考，但分段执行”。 Table Paper Motivation Method Task Datasets Models Metrics Publication Date Baselines Detailed Methds Others Positional Biases Shift as Inputs Approach Context Window Limits LIM并未总是出现 引入$L_{rel}$ Non Reasoning and Reasoning MonoRel, PIR, RuleTaker, BoxTracker Llama-3.1-70B, Llama-3.3-70B, Llama-3-70B,Mistral-Small-24B, Qwen-2.5-32B, Gemma-2-27B Acc, 2025.7 LOST IN THE MIDDLE: AN EMERGENT PROPERTY FROM INFORMATION RETRIEVAL DEMANDS IN LLMS 并非缺陷，而是一种适应 Non Reasoning 人工构造 GPT-2 Small,GPT-2 large,Llama 3.2B-1B; RNN(autoreg), T5 (双向) Recall 2025.10 On the Emergence of Position Bias in Transformers Why LIM 图论(grah-theoretic)框架 Non Reasoning 自己合成的可控数据集 简化的自注意力网络+MLP 自定义：准确率差距 2025.2 MOTIF 推理长度受上下文限制 改进GRPO Reasoning GSM8K, MATH500, AIME2024 Qwen2.5‑3B‑Instruct pass@1 accuracy 2025.7 ","title":"Paper Reading 2"},{"link":"/posts/paper/","text":"基于历史信息生成时，对不同位置信息的关注不同 Lost In the Middle 注意力层面：校准/差分/选择性注意，削弱“首尾优势”； Found In the Middle DIFF 训练数据：位置无关/信息密集型合成任务，让模型学会“任何位置都可能关键”； Never Lost in the Model Fill in the Model 提示工程/预处理：压缩与重排把关键信息放到模型更易注意的位置，或在对话中状态化总结与提醒 LongLLMLingua Lost In the Middle 目标信息的位置会显著影响模型推理的表现，当目标信息在上下文的开头和结尾时，模型表现好；在中间位置时，模型表现较差。\n微调是否对这个现象有影响？-并没有\n越多上下文越好？ 并非，取决于下游任务,本文的实验就早早饱和\nFound in the Middle 三件事\n发现了造成lost-in-the-middle problem的原因 引入注意力矫正机制: found-in-the-middle 这个机制同时提升RAG能力 之前方法： 重排文档的相关性——需要额外监督信号或专门微调 且未从根本上解决\n全文假设: 位置注意力偏差可能导致模型过度依赖输入首尾内容（无论其实际相关性如何），从而引发该现象。\n理论上来说注意力机制应该对Gold doc的注意力更高，但是实际上对开头和结尾的注意力更高，所以提出了这个假设。本文通过引入注意力矫正机制（抑制对首尾的注意力），来进行实验，如果矫正后消除了偏差，得到了期望的答案，就说明两个事情 1.假设成立 2.让注意力不再依赖位置，而只取决于真实相关性。\n建模注意力 简化建模,rel()代表prompt和第k个文档的相关性，bias(k)代表第k个位置的偏差\n$$ \\operatorname {A t t n} \\left(x ^ {\\text {d o c}}, k\\right) = \\operatorname {r e l} \\left(x ^ {\\text {d o c}}\\right) + \\operatorname {b i a s} (k) + \\epsilon \\tag {2} $$引入一个“虚拟文档$x_{dum}$​ 以获得仅含位置偏差的基线注意力，然后通过相减去除偏差：\n$$Calibrated Attention=Attn(x_{doc},k)−Attn(x_{dum},k)$$ 再进行归一化 $$ \\operatorname {a t t n} _ {\\text {calibrated}} \\left(x _ {k, i} ^ {\\text {d o c}}\\right) = \\tag {5} \\frac {\\alpha_ {k}}{\\mathrm {A t t n} _ {\\mathrm {o r i g i n a l}} (x _ {k} ^ {\\mathrm {d o c}})} \\cdot \\mathrm {a t t n} _ {\\mathrm {o r i g i n a l}} (x _ {k, i} ^ {\\mathrm {d o c}}) \\cdot C, $$Never Lost in the Middle 背景：Lost In the Middle 方法: 作者提出一个特别的训练任务 PAM QA（Position-Agnostic Multi-step QA）\n让模型学会在长文档里精确定位目标信息，无论它在开头、中间还是结尾。 效果: 非常惊人：模型在“中间有答案”的测试中几乎不掉分，甚至能做到 中间、尾部都不再迷路。 Fill in the Model Make Your LLM Fully Utilize the Context\n背景： Lost In the Middle 方法：IN2训练 （Information-Intensive Training）\n用一句人话总结： 让模型在训练时反复做“从长文本的随机位置取关键信息”的任务。 让它知道：重要信息什么位置都有 ① 微粒度信息感知（Fine-grained Information Awareness） 做法：\n从文章中 随机裁剪一个 128-token 的小段（例如一小段描述、一句话等） 用 GPT-4 生成一个问题，其答案只在这个小段里 把这个小段和很多其他随机无关的段落混在一起，组成非常长的文本（4K–32K token） 让模型在这个长文本里找答案 通俗理解： 在 3 万字的文章里找“那一句话”。 这训练模型：\n✔ 任何位置都可能有关键句\n✔ 必须在噪音中找信号\n② 跨段落推理（多跳问题）（Integration \u0026amp; Reasoning） 步骤类似，但不同的是：\n不是一个小段，而是 两个或更多小段 问题需要结合多个段落的信息来回答 举例（类比）：\n“段落 A 说张三出生地，段落 C 说他的现居地，请问他从哪里搬到哪里？”\n→ 答案需要读两个不同位置的段落。\n目的是训练模型实现真正的“跨长文推理能力”。\n③ 合成长文本的方法 长文本长度从 4K → 32K 平均分布\n目的是让模型习惯不同长度的上下文。\n还保留了 10% 的短文本任务，以防它忘记短文本能力。\nLongLLMLingua 背景： Long context scenarios下，LLM有higher computational cost, performance reduction, and position bias 的问题 方法： Differential Transformer Disentangling Memory and Reasoning Ability in Large Language Models 背景: 推理阶段中没有明确区分memory和reasoning，会导致knowledge forgetting 方法：加入special token进行训练 实验：\nShuffle Token Ablation：随机打乱 token（检验 token 的必要性)——效果下降 Token 数量 Ablation：4/6/8 个 Token 哪个最好 Prompt Ablation：去掉 prompt 中对 memory/reason 的要求 ——效果下降 Table Paper Motivation Method Task Experiment Datasets Models Metrics Publication Date Baselines Detailed Methds Others Lost In the Middle How well LM use longer text Multi-document question answering; Key Value Retrival NaturalQuestions-Open+干扰文档(用 Contriever 检索系统（基于 MS-MARCO 微调))获取；人工合成数据集 MPT-30B-Instruct;GPT-3.5-Turbo, GPT-3.5-Turbo(16K),Claude-1.3 and Claude-1.3(100k);LongChat-13B(6k) Acc（as taken from annotations appear in the predicted output） 2023 Found In the Middle Improve the lost-in-the-middle problem Calibrate Attention NaturalQuestion；SynthWiki Vicuna-7b-v1.5-16k，Tulu‑2‑7Bss Recall 2024 Disentangling Memory and Reasoning Ability in LLM knowledge forgetting data-generationw with special token;train Ablation Study StrategyQA,CommonsenseQA ,TruthfulQA. LLaMA,Qwen,GPT-4o Acc 2025 zero shot,CoT,Lora, Lora+prompt Never Lost In the Middle Improve the lost-in-the-middle problem PAM-QA DuReader2.0,WebCPM Baichuan,longchat, Improve the lost-in-the-middle problem Acc 2024 Make Your LLM Fully Utilize the Context(Fill in the Middle) Improve the lost-in-the-middle problem IN2 (Information-Intensive Training） NarrativeQA,GovReport\u0026hellip;. Mistral, Llama, GPT-4 Turbo, Claude, Gemini Acc, F1 score, 2024 LongLLMLingua Four Methods NarrativeQA, HotpotQA, TriviaQA, PassageRetrieval-en, Needle-in-a-Haystack (RULER), LooGLE, MuSiQue LLama,Mistral Perplexity, Exact Math,F1 score,Recall, Acc,Token Reduction, Latency, Cost 2024 DIFF subtract noise StableLM-3B-4E1T, Book corpus,Needle-In-A-Haystack, Multi-Needle Retrieval\u0026hellip;. DIFF LM- loss, Acc,activation outliers 2025 ","title":"Paper Reading"},{"link":"/posts/hstu/","text":"DLRM（深度学习推荐系统）： 一个经典的，标准的，符合工业界趋势的深度学习推荐系统模型应该如何构建？\nBottom MLP：处理连续数值特征。 EmbeddingLookup：将离散类别特征映射到向量空间。 Feature Interaction Layer: 融合 dense 与 sparse 特征。 Top MLP：进行最终预测（如点击率）。 DLRM 成为推荐系统工业界（如 Meta、Tencent、ByteDance）广泛采用的标准架构之一，因为它兼顾：\n高效的稀疏特征处理能力； 强大的特征交互建模能力； 在分布式训练与推理环境下的可扩展性。 过去的十年间，在DLRM框架内不断地迭代、发展新模型\nFeature interactions (FMs, DCN, Autolnt, DHEN/Wukong, MaskNet, \u0026hellip;) Multi-task learning (MMoE, ESMM, PLE, \u0026hellip;) Sequential (sub-)modules (one-stage DIN, BST, hybrid UBM, SIM, \u0026hellip;) Debiasing (off-policy correction / REINFORCE, IPW / CLRec, \u0026hellip;) Beyond two-tower settings (multi-interest / MIND, beam search / \u0026ldquo;generative retrieval\u0026rdquo; / TDM, OTM, DR, learned similarities / MoL, \u0026hellip;) . But 生成式模型的出现打破了传统的模型发展思路… Many explored use cases in RecSys:\nIn-context Learning (e.g., LLMRank, \u0026hellip;) Instruction Tuning (e.g., M6-Rec, TALLRec, \u0026hellip;) Transfer Learning utilizing World Knowledge (e.g., NoteLLM, \u0026hellip;) . 现有问题和挑战\n推荐系统中的特征缺乏明确的结构。 十亿级词汇动态系统 vs 100K级自然语言静态系统 计算成本是实现大规模序列模型的主要”卡脖子”问题。 GR-24已经直追LLaMa-2的运算规模。 DLRMs + Generative Models: How do we get the best of both worlds? Classical recommendation models - DLRMs - vs LLMs Pros of LLMs\nReplace feature engineering, to the extent capable by language; World knowledge benefits cold-start scenarios; Scale with compute. Pros of DLRMs\nLeverage vast number of human-engineered features; Concise representations — efficient and support very long context sizes; Scale with (in-domain recommendation) data. 解决方案一重新构建工业化的推荐系统\n- 将用户行为视为首要模态 将点击/跳过/停留/搜索等动作记为token;\n与图像/视频/文本/元数据融合而不丢失信息。\n- 规范化特征空间与编码 在sequence-to-sequence这种序列化视图下统一检索与排序\n- 消除架构中一些不好掌控的冗余 -工业级就绪性与可扩展性 随数据与计算资源扩展；\n对动态词汇表[持续新增项]及长历史数据具备泛化能力和鲁邦性。\n本文的核心贡献 统一的生成式推荐器 [GR]。部署在核心产品上；一个序列模型同时完成召回和排序，取代了长期以来依赖大量特征的推荐管线。 HSTU 编码器。针对高基数、非平稳数据流的新架构；在质量上超过 Transformer，并且在序列长度 L = 8192 时训练速度比 FlashAttention2 快 15.2 倍。 M-FALCON 算法。在成千上万候选项之间共享计算 [约 700 倍摊销]，即使模型大了 285 倍，也能实现约 2.48 倍的吞吐率提升。 公开数据集实验。在 MovieLens / Amazon Reviews 上，相比强序列模型基线（如 SASRec），NDCG@10 指标提升约 20% - 66% 线上A/B测试。端到端〔召回+排序〕提升最高 +18.6% [排序 +12.4%, 召回 +6.2%]。 大规模训练。训练了 1.5 万亿参数 [Trillion-Parameter] 的 GR；年计算量提升约 1000 倍，可与 GPT-3/LLaMA-2 相媲美；首次在推荐系统中观察到类似 LLM 的 scaling law。 DLRMs + Generative Models =\u0026gt; Generative Recommenders 从DLRM到GR 用户和商品的交互序列当作主线\n地理位置为辅助序列1，慢变化属性，辅助变量\n用户加入的购物社区2，慢变化属性，辅助变量\n像点击率这种快速变化,不能真实地反应用户兴趣，不再显示输入，而是让模型自己从历史时序序列中[生成]相同的信息\n作者将所有的正负反馈行为组织成序列，这个序列中既包含item_id, user_id,也包含稀疏特征，交互行为类型等，而摒弃了数值型特征，构造了生成式建模所需要的统一输入格式，这也印证了actions speak louder\n​\n简单说：GR里面，排序就是一个预测动作，检索就是预测内容，而这两个都是通过同一个序列模型来完成，在检索中，输入是动作和内容的交互序列 检索: 如果动作是正反馈，输出的候选内容加入候选集；如果是负反馈，则输出为空，所以检索的本质是根据用户的历史行为生成新的候选内容 排序：输入的是历史的内容和用户的动作序列，最后一位是内容，叫做内容位，模型在内容位预测用户会采取什么动作，比如点击，喜欢等等。所以排序的本质是在已有的候选内容上预测用户的排序分布 监督策略：只对正反馈监督，负反馈标记为空，只保留有意义的交互，减少冗余\nGR在同一条交互里序列完成检索，也就是动作位生成内容，再加上排序，也就是内容位生成内容，并且保持最新的动作和内容能够与历史快速交互，GR并不是每次曝光就发送一条样本，而是在会话末或者关键发送一条样本，在样本减少的情况下还能保存丰富的信息量。在传统的DLRM，每曝光一个item，就会生成一个独立的训练样本，这样的问题是样本特别多，而且很多计算是重复，因为用户的历史行为在不同样本会被反复算一遍。GR不会在每次曝光时就立刻生成样本，而是在一个关键时刻，比如一次推荐对话时间结束，把前面的交互序列打包成一个训练样本，这样在减少算力消耗的同时，每个样本的监督信号更密集，更干净， 同时通过更密集/更精确的每轮监督以及候选模型与历史数据间的目标感知交互，保持甚至提升质量。由公式可以看到，计算量下降了一个量级 $$O(N^3 d + N^2 d^2) t \\ \\ -\u003e \\ \\ O(N^{2}d + Nd^{2})$$HSTU(Hierarchical Sequential Transduction Unit) 为了让GR模型在工业界大规模推荐系统中实现高可扩展性处理海量非稳态的词表和数据\nDLRM中的三个主要阶段：特征提取、特征交互作用以及表示的转换。 HSTU每个层包含三个主要的子层：\npointwise投影层：信息压缩与重构 Pointwise Projection Layer 在传统的 Q, K, V 投影之外引入了额外的 U 投影，用于建模用户长期历史（long-term user representation）。 它将用户的长序列行为压缩成较低维的语义表示，从而在与目标（target item）交互时，能更有效地筛选和增强关键信息。 pointwise空间聚合层: 局部信息聚合 Pointwise Aggregation Layer 使用“聚合注意力（aggregation attention）”取代传统 softmax 注意力，不再进行概率归一化，从而避免了 softmax 的稀释效应。 这种改进能更充分地保留输入信息，使模型在面对动态变化或长尾分布的行为词表时更加稳定。 pointwise转换层: 非线性变换与偏置调整 Pointwise Transformation Layer 在非线性变换（如 ReLU/GELU）中引入了偏置项 $r_{ab}$，以对 token 之间的关系进行细粒度建模。 该层增强了模型的表达灵活性，使其能更好地捕捉行为序列中的复杂交互模式。 工程优化 稀疏性优化 高效注意力内核 [GPU内核] 将注意力机制的计算重构为一组分组矩阵乘法（Grouped GEMM），并通过融合内核（Fused Kernel）实现一次 GPU 调用完成所有步骤（QKᵀ、Softmax、乘 V），从而减少显存访问和内核启动开销，使模型在 GPU 上的计算吞吐量提升约 2–5 倍。\n算法增强稀疏性：随机长度 [SL] 用户行为在多时间尺度上呈现重复性\n通过从完整历史记录中提取子序列进行训练，人工增强数据稀疏性 ​\n内存是工业级scaling非常重要的瓶颈! 精简层级与融合操作： HSTU将非注意力线性层从6层削减至2层\n使每层内存消耗降至约14d [bf16]\n该设计可构建深度提升2倍以上的模型。\n嵌入/优化器内存： 针对十亿级词汇表，采用行向AdamW算法，使嵌入参数的 HBM占用从约12B缩减至约2B。\n对于十亿级 embedding 表，普通 AdamW 会因维护元素级动量状态导致显存占用极高。 采用“行向 AdamW”后，将状态按行聚合，大幅压缩优化器状态存储，使嵌入参数在 GPU 高速显存（HBM）中的占用从约 12B 降至 2B，提高了训练可扩展性和内存效率 并行优化 通过成本摊销实现推理扩展 在排序阶段，我们面临数万个候选项目。作者提出名为M-FALCON[微批量快速注意力缓存操作]的算法，可在输入序列长度为n时，对m个候选项执行单次推理。\nM-FALCON 的共享计算原理：通过将用户历史序列的中间特征（K,V）缓存起来，让所有候选 item 复用这份表示，仅计算目标 item 的交叉注意力，并采用微批并行方式统一处理，从而实现“计算一次，多次使用”的推理加速。 他们成功部署了复杂度提升285倍的目标感知交叉注意力模型，在相同推理计算能力下实现了1.5倍的吞吐量提升。\nMicrobatched-Fast Attention Leveraging Cachable Operations 数据集 MovieLens\n该非商业性数据集由明尼苏达大学计算机科学与工程学院的GroupLens项目团队创建，旨在支持研究工作。数据集包含多个电影评分数据子集，已成为推荐系统领域的经典数据集。\nAmazon Book Reviews\n亚马逊提供的产品数据集包含商品评论及元数据，涵盖1996年5月至2014年7月期间的1.428亿条评论。\n工程优化的有效性 随机序列长度[SL]的有效性：长度为4096的序列可以减少80%的token。即使 $\\alpha$ 变大，可有效减少的幅度也不会下降很多。同时NE指标并没有因此变差，变差幅度不超过0.2%\n相比FlashAttention优化的 Transformers，HSTU在训练和推理阶段效率能分别提升15.2x、5.6x倍。\n此外，由于内存的各种优化和节省，相比于Transformers，HSTU可以再叠深2层网络。\nNE指标下降明显，意味着线上指标效果提升明显 Generative Recommenders vs DLRMs 传统DLRM里用了大量的特征，如果把DLRM的特征做消融，仅仅保留GR里用到的那些，那么模型效果会大打折扣，看离线指标降了很多。一方面说明传统DLRM没有特征不行，另一方面表明了新架构的优势。\n在GR中只考虑item的属性(content-based)，召回效果奇差，比baseline也低不少，说明用户行为中蕴含的高基数、高阶信息建模的重要性。\n即使模型复杂度比基线高出285倍，当候选序列为1024时，GR仍能实现1.5倍吞吐量；选序列为16384时，吞吐量提升至2.48倍。\n在大型工业环境中，DLRM模型在特定计算和参数配置条件下会达到质量饱和点。 而GRs体现出来了更强劲的、在大模型领域出现过的scaling能力。\n推荐系统中的幂律扩展趋势则无法显现算力越大，效果越好这种趋势\n总结： 作者提出一种统一的生成式推荐模型 [GR]，以实践证明“行动胜于言辞”。性能表现：在传统公开测试集和行业真实流数据集上均取得显著提升，其NDCG@10指标较经典SASRec模型提升 20.3% - 65.8% 。相较于经多次迭代优化的DLRMs基准模型，该模型在线召回率与精确度实现 18.6% 的相对提升。首次在核心产品线中取代传统深度推荐模型——而这些基于海量异构特征的模型曾主导推荐领域近十年。\n长远的意义\n通过降低推荐、搜索和广告系统对海量异构特征的依赖，这些系统既能提升用户体验，又能更注重隐私保护。 传统推荐系统常基于用户短期行为和偏好生成推荐，这可能导致用户接触到与其长期目标不符的内容。 该方法还使平台激励机制与用户价值更趋一致，能够真正识别用户潜在需求而非平台推广导向。 ","title":"HSTU"},{"link":"/posts/tiger/","text":"TIGER 推荐系统深度学习基本流程 现有深度学习架构：\n输入特征 → 文本嵌入 → 隐藏层（RNN，深度学习网络）\n得到更精准的预测表征 → 预测层（通过查询与物品的相似性度量、点击，对相关物品进行排序，得到预测） 检索 / 召回一系列可行的候选对象，然后用排序模型对其进行排序。 检索阶段 通过矩阵分解，在同一空间学习查询和候选的嵌入。\n为了更好地捕获数据中的非线性关系，近年来采用 内积查询和候选嵌入到同一空间的双塔编码架构（一个塔用于查询(用户)，另一个塔用于候选(物品)）成为主流。\n为了在推理期间使用这些模型，使用候选塔创建一个存储所有物品嵌入的索引。\n对于给定查询，通过在同一个空间内嵌入查询和候选项来执行大规模检索，\n然后使用**近似最邻近搜索（ANN）**来选择给定查询嵌入的最佳候选项。 然后筛选到的物品进行内积计算排序\n推荐系统存在的局限与挑战 召回 → 排序 → 重排，流程复杂，需要分别优化。 对召回阶段的依赖（庞大候选集快速筛选相关物品） 如果召回都没召回到用户感兴趣的东西，排序更不可能找到。 对排序阶段的局限（基于学习的排序模型 / 神经网络模型 / 对候选物品排序）。 对反馈循环的影响：基于用户的历史交互行为进行预测，会产生反馈循环。 冷启动推荐：新商品被反馈循环影响。 推荐的“长尾”现象：热门更热，冷门更冷。 本文工作：提出全新的推荐系统框架 TIGER 基于生成式检索范式并应用到序列推荐中：\n创建语义上有意义的 token 元组，作为每个物品的语义 ID。\n通过生成的用户交互序列中物品的语义 ID，训练基于 Transformer 的序列到序列模型来“生成”用户将与之交互的下一个物品的语义 ID。 提出 TIGER 推荐模型在各种数据集上的性能显著优于当前 SOTA 模型。 展现两个关键能力： 冷启动推荐能力：能够推荐此前从未出现过的物品； 推荐多样性控制能力：可通过调节生成参数实现推荐内容的丰富性。 生成式推荐新范式，为构建更具泛化能力、可解释性和灵活性的推荐系统开辟新方向。 TIGER 的生成式推荐范式 提出一种构建序列推荐生成式检索模型的新范式。\n与传统的检索-排序方法不同，\n我们的方法使用 直接预测候选物品 ID 的端到端生成模型。\n我们提出利用 Transformer 内存（参数）作为推荐系统中检索的端到端索引引擎。\n我们将提出的方法称为 Transformer Index for GEnerative Recommenders（TIGER）。\n图 2 展示了 TIGER 框架的整体工作流程，说明了如何将序列推荐任务转化为一个生成式检索任务。\n语义 ID（Semantic ID）简介 语义 ID 示例： (2, 3, 55)，每一个数字都代表着一个稠密向量。\n将物品 (item) 表示为语义 ID 序列具有许多优点：\n语义共享与泛化\n在具有语义意义的数据上训练 Transformer 允许在相似的物品之间共享知识，\n避免在推荐模型中使用原子、随机的物品 ID 作为特征，\n从而支持知识迁移和泛化。\n缓解反馈循环\n使用物品的语义 ID 能缓解推荐系统固有的反馈循环，\n减少系统对热门物品的依赖，\n并允许模型泛化到语料库中的新物品，更好地支持新物品推荐。\n存储与扩展性\n通常物品数量可达数十亿级。\n使用有限数量的代码词组合生成语义 ID 来表示物品，\n可显著降低物品表示的存储成本和参数规模，\n提升系统的可扩展性。\n本工作的主要贡献 提出新的基于生成检索的推荐框架 TIGER：\n为每个物品分配语义 ID，并训练模型预测生成给定用户可能交互的物品语义 ID。 通过召回率和 NDCG 指标，证明 TIGER 在多个数据集上优于现有 SOTA 推荐系统。 发现生成式检索新范式在序列推荐系统中带来了两个额外能力： 推荐新的和冷门的物品，改进冷启动； 可用可调参数生成多样的推荐结果。 语义ID详解 定义与构成 语义ID是长度为\\(m\\)的token元组，每个token来自不同的codebook（代码簿），可唯一表示的项数等于每层codebook大小的乘积。 示例：语义ID\\([10,21,35]\\)与\\([10,21,40]\\)的相似度高于\\([10,23,32]\\)，因前两个token重叠，语义更接近。 关键属性 相似性：相似项（内容特征相似或语义嵌入接近）需具有重叠的语义ID。 离散化语义表示：不依赖具体物品ID，基于内容语义生成，而非物品实体本身。 可控可复用：组成单位（codewords）来自固定大小的codebook，大小可控且可重复利用。 组合式表达：如每个ID由3个token组成，每个token选自1000个候选，可表达\\(1000^3=10^9\\)个组合，能覆盖大规模推荐系统的物品空间。 与大语言模型Tokenizer的区别 对比维度 语义ID 大语言模型Tokenizer 依赖对象 不依赖具体物品ID，基于内容语义 依赖文本序列，与物品语义无直接关联 组成来源 固定大小codebook的codewords 文本语料库中的子词（如BPE分词结果） 表达能力 组合式，覆盖物品空间能力强 序列式，聚焦文本语义表达 Tip 通过语义ID的设计，使得推荐系统能够像语言模型那样“输出”目标物品，但又规避了推荐领域中 item ID/token 空间过大、实体变化频繁等根本难题。可以说，语义ID是 TIGER 成功的核心支柱之一。\n语义ID生成方法 核心方法：基于RQ-VAE（残差量化变分自动编码器） RQ-VAE 首先通过编码器 $\\mathcal{E}$ 对输入 $x$ 进行编码，以学习潜在表示：$z := \\mathcal{E}(x)$\n在第 0 个残差两化层（$d=0$）时，初始残差被定义为：$r_0 := z$ 在每一层 $d$，都有一个码本（codebook）：$C_d :=$ { $e_k$ } $_{k=1}^K$ 其中 $K$ 是码本的大小。（可以理解为有K个物品）\n接着，$r_0$ 被量化为该层码本中最近的嵌入向量。 距离最近的嵌入为 $e_{c_d}$，其索引为：$c_0 = \\arg\\min_k \\| r_0 - e_k \\|$ 这表示第 0 个 codeword。 在下一层（$d=1$）中，残差定义为：$r_1 := r_0 - e_{c_0}$ 然后与第 0 层类似，第 1 层的 codeword 通过在码本中寻找最接近 $r_1$ 的嵌入得到。 该过程递归重复 $m$ 次，以获得 $m$ 个 codeword 组成的元组： $$ (c_0, c_1, \\ldots, c_{m-1}) $$ 该元组即表示语义 ID（Semantic ID）。\n重构与损失函数 一旦得到语义 ID $(c_0, \\ldots, c_{m-1})$，量化后的潜在表示计算为： $$ \\hat{z} := \\sum_{d=0}^{m-1} e_{c_d} $$然后将 $\\hat{z}$ 传入解码器，以重建输入 $x$。\nRQ-VAE 的总损失定义为： $$ \\mathcal{L}(x) := \\mathcal{L}\\_{recon} + \\mathcal{L}_{rqvae} $$其中： $$ \\mathcal{L}\\_{recon} := \\|| x - \\hat{x} \\||^2, $$ $$ \\mathcal{L}\\_{rqvae} := \\sum_{d=0}^{m-1} \\big( \\| sg[r_i] - e_{c_i} \\|^2 + \\beta \\|| r_i - sg[e_{c_i}] \\||^2 \\big) $$这里 $\\hat{x}$ 是解码器输出，$sg$ 表示停止梯度（stop-gradient）操作。\n该损失函数联合训练编码器、解码器和码本。\n避免 Codebook Collapse 为了防止 RQ-VAE 出现 codebook collapse（即大部分输入仅映射到极少数码本向量），\n采用基于 k-means 聚类 的码本初始化方法。\n具体做法是在首个训练批次上应用 k-means 算法，并使用聚类中心作为初始码本向量。\n生成示例 若经 3 层量化得到嵌入索引 $e_{c_0}=7$、$e_{c_1}=1$、$e_{c_2}=4$， 则语义 ID 为 $(7, 1, 4)$。\n其他量化方法对比（消融实验证实RQ-VAE最优） 方法 特点 不足 LSH（局部敏感哈希） 离散化速度快 精度低，语义保留差 VQ-VAE（矢量量化变分自动编码器） 基础量化能力 无语义层次粒度，无法体现粗/细分类别 基于k-means层次聚类 可实现层次划分 丢失ID间的语义关系，相似项可能无重叠ID 冲突处理 附加标识位：增加1位token表示语义重复，区分相同语义ID的不同物品。 查找表：维护“语义ID→物品ID”映射表，避免模型误判无对应物品的语义ID；仅需训练后处理1次，不影响训练效率。 基于语义ID的生成式检索（TIGER框架核心） 核心思路 将推荐任务转化为“序列生成”任务：通过用户历史交互物品的语义ID序列，预测下一个物品的语义ID，实现生成式检索。 流程步骤 （1）用户序列构建 按时间顺序排序用户历史交互物品，形成物品序列$(item_1, ..., item_n)$。 （2）序列转换 设$(item_i)$的语义ID为$(c_{i,0}, ..., c_{i,m-1})$，将物品序列转换为语义ID序列：$(c_{1,0}, ..., c_{1,m-1}, c_{2,0}, ..., c_{2,m-1}, ..., c_{n,0}, ..., c_{n,m-1})$。 （3）模型训练与预测 模型结构： 任务目标：训练序列到序列模型，预测下一个物品的语义ID$(c_{n+1,0}, ..., c_{n+1,m-1})$。 特殊情况：解码器生成的语义ID可能与推荐语料库物品不匹配，但概率极低。 通过语义ID的设计，TIGER框架成功地将推荐系统引入了一个生成式检索的新范式，使得系统能够像语言模型那样直接“输出”目标物品。 实验设计与结果 1. 实验基础设置 基座模型与数据集 2. 核心实验结果 （1）序列推荐性能对比（TIGER最优） （2）消融实验：不同ID生成方式（RQ-VAE SID最优） （3）冷启动推荐性能（TIGER优于KNN） （4）推荐多样性（温度系数调控） （5）模型层数与用户信息影响 （6）Invalid Semantic ID 模型成本与优势 成本分析 （1）内存成本 查找表：维护“物品ID→语义ID”和“语义ID→物品ID”两个哈希表，每个语义ID为4整数元组，大小约64N位（N为物品数）。 嵌入表：仅存储codebook中每个codeword的嵌入，远小于传统推荐系统的“物品-嵌入”表（传统需为每个物品存储嵌入）。 （2）推理成本 挑战：自回归解码+beam search，推理成本高于基于ANN的模型。 优化方向：探索更小模型结构或高效推理方法。 核心优势 嵌入表规模可控：嵌入表基数不随物品空间线性增长，避免传统模型的大型嵌入表问题。 泛化能力强：支持冷启动推荐，可泛化到未见过的新物品。 生成式范式创新：将推荐从“检索匹配”转为“生成预测”，开辟新研究方向。 总结 模型核心思路 本文提出了一种新的推荐范式，称为 TIGER，使用生成检索模型在序列推荐中“生成”下一个可能的交互对象。 支撑该方法的是一种新的 物品语义 ID 表示，它在内容嵌入上使用多层量化器（RQ-VAE）来生成语义 ID。 基于 Transformer 的序列生成模型：将用户的历史行为表示为语义 ID 序列，使用 Encoder–Decoder 结构的 Transformer 模型学习用户偏好，并生成下一个物品的语义 ID。 嵌入表的基数不会随着物品空间的线性增长而增加，这使得训练过程中无需为每个单独物品建立大型嵌入表或索引系统，效率更高。 在三个数据集上的实验表明，该模型能达到与 SOTA 检索模型相当的性能，同时具备泛化能力，可生成全新的或未见过的物品。 不足 论文中的语义 ID 建模是静态的，没有与 user–item 交互相关联，也未融合协同信息。 后续的生成式模型在动态交互建模和个性化生成方面仍有较大改进空间。 ","title":"TIGER \u0026 Semantic ID"},{"link":"/posts/grid/","text":"1.前言 GR 利用生成模型的进步，实现两种主要方式：\n直接生成用户感兴趣物品的文本内容 或者从预训练模型中提取语义表示（semantic representations），以编码开放世界的知识，本文中的模型是后者。 2.语义ID（Semantic IDs） 2.1语义ID是什么 促成 GR（Generative Recommender） 成功的关键因素之一是 语义 ID（SID, Semantic ID）。它将连续的语义表示（例如来自大型语言模型的向量表示）转换为离散的 ID 序列。 2.2传统ID与语义ID对比 传统的ID特征是为用户/物品分配独一无二，但不提供信息的ID，这些ID被映射到嵌入向量中，主要用于捕获协同过滤信号 语义ID将连续的语义表示(例如，来自大模型语言)转换为离散的ID序列；这样能够同时利用预训练基础模型中编码的语义知识以及用户-物品交互历史中的编码的协同信号。 当两个物品的 SID 有重叠时，这种重叠在原理上反映了它们的语义相似性；同时，下一步的监督学习（next-item supervision）使模型能够学习跨 SID 的协同过滤信号 2.3怎么得到语义ID 首先通过模态编码器（modality encoder，例如大型语言模型 LLMs）提取语义表示； 然后使用量化器（quantizer）将连续嵌入压缩为离散稀疏 ID。 常见的基于量化的标记器包括： RQ-VAE ，RVQ ，Residual K-Means 。 2.4如何将语义ID用于生成式推荐 TIGER 首次将 Transformer 应用于推荐任务， 用于预测物品的语义 ID（SID），并借鉴了文档检索（document retrieval）中生成式推荐的思想 。后续研究在多个方向上改进了 SID 的训练：\n通过协同过滤信号增强学习 ； 引入分布式平衡机制（distributional balancing）； 使用更强大的大型语言模型或多模态编码器（multimodal encoders）。 3.GRID架构 我们考虑一个用户集合 $\\mathcal{U}$，每个用户与一个物品集合 $\\mathcal{I}$ 中的项目交互。每个物品 $i \\in \\mathcal{I}$ 都有相关的语义特征 $f_i$，包括但不限于文本和图像。\n每个用户 $u \\in \\mathcal{U}$ 都有一个交互序列，其长度为 $L_u$，记作 $S_u = [i_u^1, i_u^2, \\dots, i_u^{L_u}]$。\n一般性地，模态编码器（modality encoder，例如 LLM 或 VLM）将特征 $f_i$ 转换为 $d$ 维的表示 $h_i \\in \\mathbb{R}^d$。\n生成式推荐（GR）的目标是解决序列推荐问题：给定一个用户的交互序列 $S_u$，生成一个候选物品集合，使得这些物品是用户下一步最可能交互的内容（即 $i_u^{L_u+1}$）。\n3.1架构：先标记化（Tokenization）后生成（Generation）** GRID 将基于语义ID的生成式推荐划分为两个阶段：\n标记化阶段（Tokenization）\n将物品特征映射成嵌入向量（即 $h_i$），再量化为语义ID（SID）。\n生成阶段（Generation）\n使用所有物品的SID，探索不同的生成模型结构（例如 Transformer-based 模型），以生成下一个物品的SID。\nGRID 在每个阶段都提供了灵活的可配置实现。\n3.2语义ID标记化（Semantic ID Tokenization） SID 标记化首先通过预训练的模态编码器 $E(\\cdot)$ 将物品的语义特征映射到嵌入空间，然后通过分层结构将这些嵌入量化为稀疏的语义ID。\n这种层次化组织的SID可以通过不同层级的前缀（prefix）灵活控制粒度。\n形式化地，给定 $h_i$，量化器（Tokenizer）$Tokenizer(\\cdot): \\mathbb{R}^d \\to {0, 1, \\dots, V}^L$ 将嵌入 $h_i$ 映射为ID序列：\n$SID_i=Tokenizer(h_i)=[SID_i^1,SID_i^2,…,SID_i^L]$ 其中 $V$ 表示每层的词汇大小，$L$ 表示层数。\nGRID 提供了可插拔的模块化设计，方便用户自定义模型或直接使用 HuggingFace 上的模型。\n目前支持三种主要量化器：\nResidual Mini-Batch K-Means (RK-Means) Residual Vector Quantization (R-VQ) Residual Quantized Variational Autoencoder (RQ-VAE) 3.3下一项生成（Next Item Generation） 当所有物品都生成了SID后，针对每个用户序列，GR 框架会使用序列模型生成用户最可能交互的候选SID。\n在 GRID 中，支持多种生成结构：\nEncoder-Decoder 模型； Decoder-only（仅解码器） 模型； 可灵活配置的 Transformer 层数、头数、MoE 专家结构等。 用户可以导入 HuggingFace 上公开的架构或自定义自己的结构。\n默认训练目标是基于 下一token预测（Next-token prediction） 的生成任务，并采用 **滑动窗口增强（sliding window augmentation）。 推理阶段使用带 KV-cache 的 beam search，支持调节 beam width，确保生成的SID是合法的。\nGRID 还内置了多种技巧，比如：\n用户token机制（user token）； 去重（de-duplication）； 避免SID冲突。 4.GRID 实验结果总结 总体设置 数据集：Amazon 5-core（Beauty, Sports, Toys） 评估方式：最后一个交互用于测试，倒数第二个用于验证，其余用于训练。 模型：Flan-T5-Large / XL / XXL 提取语义嵌入。 标记化算法：RK-Means、R-VQ、RQ-VAE。 生成模型：采用 Transformer 架构（共8层，encoder 4层 + decoder 4层）。 评估指标：Recall@K 与 NDCG@K（K = 5, 10）。 4.1 语义ID标记化（Semantic ID Tokenization） 标记化算法（Tokenizer Algorithm） 比较 RQ-VAE、RK-Means、R-VQ 三者。 结果： -**RQ-VAE 虽常被采用，但需同时训练自编码器与量化器，复杂度高。 -RK-Means 与 R-VQ 表现更优，即使在计算量更低的情况下也能获得更好推荐结果。 - 说明复杂的 RQ-VAE 性价比不高。 语义编码器规模（Semantic Encoder Size）\n测试 Flan-T5-Large (780M) → XL (3B) → XXL (11B)。 结果： 模型参数扩大14倍，性能提升却非常有限。 结论： 当前 GR with SID 体系未充分利用更大LLM的语义知识，性能主要受其它模块影响。 标记器层数与维度（SID Tokenizer Dimension） 调整残差层数 L 与每层token数量 W。默认配置 (L, W) = (3, 256)。 结果： 默认配置效果最佳； 层数过多会降低性能——更多层虽可传递更多语义信息，但会造成SID序列不稳定。 存在“语义信息量 vs. 序列可学习性”的权衡。 4.2 生成式推荐（Generative Recommendation） 实验从五个角度研究了 GR with SID 的生成阶段设计。\n用户token数量（User Tokens） 每个用户的SID序列前可加入一个用户token。 结果： 更大的用户词汇表并不提升性能； 完全去掉用户token（即不加个性化标识）反而效果最佳。 结论： 当前 GR with SID 框架中的用户token设计未实现个性化目标。 模型架构：Encoder-Decoder vs. Decoder-only 对比 Transformer 编解码器与仅解码器架构。 结果： Decoder-only 明显性能更差。 Encoder-Decoder 模型更能捕捉用户长期行为的上下文信息。 说明 Encoder 层的上下文建模对生成式推荐至关重要。 数据增强（Data Augmentation） 使用滑动窗口（sliding window）生成多样化序列样本。 结果： 适当的数据增强显著提高泛化能力、减少过拟合； 模型能更好预测多样化和稀疏样本中的下一交互项。 结论： 数据增强是提升 GR 性能的关键手段。 SID去重（De-duplication） 比较两种去重策略： TIGER策略： 在SID末尾加数字； 随机选择策略： 当碰撞时随机选一个。 结果： 两种方式效果接近，TIGER略好但计算代价更高。 TIGER方法需全局SID分布信息，不适合大规模数据。 结论： 简单随机去重在实际场景更高效。 束搜索（Beam Search）策略 对比 约束式 与 非约束式 beam search。 结果： 性能差距极小； 非约束式搜索更快、计算更省； 说明SID生成任务本身的模式约束足以保证生成质量，无需显式约束。 5. 结论（Conclusion） 主要发现： 过去被认为“关键”的组件（如复杂量化器、RQ-VAE、自定义LLM、大量用户token）其实可以被更简单、高效的设计替代； 相反，一些被忽视的因素（如 Encoder-Decoder结构 与 数据增强）却是性能提升的关键； 这些结果为理解 GR with SID 的真正性能驱动因素提供了新的视角。 GRID 的价值： 通过开源的、统一的实验平台，系统揭示了 GR with SID 的核心设计权衡； GRID 框架 能作为标准基准（benchmark）和实验工具，加速后续研究与验证。 ","title":"GRID"},{"link":"/posts/transformer/","text":"Transformer Embedding 下面的示例实现一个最简词嵌入层，附带缩放确保数值稳定。\nPYTHON Collapse Copy import torch # 引入 PyTorch 用于张量运算 import torch.nn as nn # 导入神经网络模块方便搭建组件 class SimpleEmbedding(nn.Module): # 定义词嵌入层类 def __init__(self, vocab_size: int, d_model: int): # 接收词表大小和向量维度 super().__init__() # 初始化父类确保模块注册 self.embed = nn.Embedding(vocab_size, d_model) # 创建可学习的词嵌入矩阵 self.scale = d_model ** 0.5 # 保存缩放系数防止数值过小 def forward(self, tokens: torch.Tensor) -\u0026gt; torch.Tensor: # 定义前向传播 return self.embed(tokens) * self.scale # 查表并放大嵌入向量 Click to expand and view more Muti_Head Attention 下面的类展示如何把输入拆成多头并执行自注意力。\nPYTHON Collapse Copy import torch # 引入 PyTorch 支撑矩阵运算 import torch.nn as nn # 导入 nn 模块方便线性层创建 class SimpleMultiHeadAttention(nn.Module): # 定义多头注意力层 def __init__(self, d_model: int, num_heads: int): # 接收特征维度和头数 super().__init__() # 调用父类初始化模块 assert d_model % num_heads == 0 # 断言能均分每个头的维度 self.d_model = d_model # 保存特征总维度 self.num_heads = num_heads # 保存头数 self.head_dim = d_model // num_heads # 计算每个头的维度 self.q_proj = nn.Linear(d_model, d_model) # 定义查询映射层 self.k_proj = nn.Linear(d_model, d_model) # 定义键映射层 self.v_proj = nn.Linear(d_model, d_model) # 定义值映射层 self.o_proj = nn.Linear(d_model, d_model) # 定义输出映射层 def forward(self, x: torch.Tensor) -\u0026gt; torch.Tensor: # 前向函数执行自注意力 batch_size, seq_len, _ = x.shape # 读取批大小和序列长度 q = self.q_proj(x) # 线性映射生成查询 k = self.k_proj(x) # 线性映射生成键 v = self.v_proj(x) # 线性映射生成值 q = q.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2) # 重塑查询形状并交换维度 k = k.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2) # 重塑键形状并交换维度 v = v.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2) # 重塑值形状并交换维度 scores = torch.matmul(q, k.transpose(-2, -1)) / (self.head_dim ** 0.5) # 计算缩放点积注意力分数 weights = torch.softmax(scores, dim=-1) # 对最后一维做 softmax 获得注意力权重 context = torch.matmul(weights, v) # 加权求和值向量 context = context.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model) # 合并头并恢复原尺寸 return self.o_proj(context) # 输出线性层映射后的结果 Click to expand and view more Encoder 这里的编码器层由自注意力和前馈网络组成。\nPYTHON Collapse Copy import torch # 引入 PyTorch 支撑张量操作 import torch.nn as nn # 导入 nn 模块构建层 class SimpleEncoderLayer(nn.Module): # 定义基本的 Transformer 编码器层 def __init__(self, d_model: int, num_heads: int, d_ff: int): # 接收核心超参数 super().__init__() # 初始化父类 self.self_attn = SimpleMultiHeadAttention(d_model, num_heads) # 自注意力子层 self.ffn = nn.Sequential(nn.Linear(d_model, d_ff), nn.ReLU(), nn.Linear(d_ff, d_model)) # 前馈网络子层 self.norm1 = nn.LayerNorm(d_model) # 第一层归一化 self.norm2 = nn.LayerNorm(d_model) # 第二层归一化 def forward(self, x: torch.Tensor) -\u0026gt; torch.Tensor: # 定义编码器层前向传播 attn_out = self.self_attn(self.norm1(x)) # 对归一化后的输入做自注意力 x = x + attn_out # 残差连接叠加注意力输出 ffn_out = self.ffn(self.norm2(x)) # 对归一化后的结果做前馈网络 return x + ffn_out # 残差连接返回最终输出 Click to expand and view more Decoder 下面的解码器层包含自注意力和交叉注意力两个部分。\nPYTHON Collapse Copy import torch # 引入 PyTorch 以执行张量计算 import torch.nn as nn # 导入 nn 模块搭建子层 class SimpleDecoderLayer(nn.Module): # 定义基本的 Transformer 解码器层 def __init__(self, d_model: int, num_heads: int, d_ff: int): # 初始化解码器参数 super().__init__() # 初始化父类模块 self.self_attn = SimpleMultiHeadAttention(d_model, num_heads) # 自注意力处理目标序列 self.cross_attn = SimpleCrossAttention(d_model, num_heads) # 交叉注意力融合编码器信息 self.ffn = nn.Sequential(nn.Linear(d_model, d_ff), nn.ReLU(), nn.Linear(d_ff, d_model)) # 前馈网络细化特征 self.norm1 = nn.LayerNorm(d_model) # 自注意力前归一化 self.norm2 = nn.LayerNorm(d_model) # 交叉注意力前归一化 self.norm3 = nn.LayerNorm(d_model) # 前馈网络前归一化 def forward(self, x: torch.Tensor, memory: torch.Tensor) -\u0026gt; torch.Tensor: # 前向传播接收目标输入与编码器输出 attn_out = self.self_attn(self.norm1(x)) # 对归一化后的目标序列做自注意力 x = x + attn_out # 残差连接保留原信息 cross_out = self.cross_attn(self.norm2(x), memory) # 使用编码器记忆执行交叉注意力 x = x + cross_out # 残差连接融合上下文 ffn_out = self.ffn(self.norm3(x)) # 通过前馈网络细化结果 return x + ffn_out # 残差输出最终表示 Click to expand and view more Layer Norm 下例展示 LayerNorm 的手工实现，突出按特征维度标准化的思想。\nPYTHON Collapse Copy import torch # 引入 PyTorch 支撑自动微分 import torch.nn as nn # 导入 nn 模块定义模块接口 class SimpleLayerNorm(nn.Module): # 定义简单的层归一化 def __init__(self, features: int, eps: float = 1e-5): # 初始化特征数量和稳定常数 super().__init__() # 初始化父类 self.gamma = nn.Parameter(torch.ones(features)) # 可学习的缩放参数 self.beta = nn.Parameter(torch.zeros(features)) # 可学习的偏移参数 self.eps = eps # 保存防止除零的常数 def forward(self, x: torch.Tensor) -\u0026gt; torch.Tensor: # 执行层归一化 mean = x.mean(dim=-1, keepdim=True) # 计算最后一维的均值 var = x.var(dim=-1, keepdim=True, unbiased=False) # 计算最后一维的方差 normalized = (x - mean) / torch.sqrt(var + self.eps) # 对输入做标准化 return self.gamma * normalized + self.beta # 应用缩放和平移得到结果 Click to expand and view more Cross Attention 最后给出一个简化版的交叉注意力实现，用于解码器读取编码器输出。\nPYTHON Collapse Copy import torch # 引入 PyTorch 获得矩阵运算能力 import torch.nn as nn # 导入 nn 模块构建线性层 class SimpleCrossAttention(nn.Module): # 定义交叉注意力层 def __init__(self, d_model: int, num_heads: int): # 接收输入维度和头数 super().__init__() # 初始化父类 assert d_model % num_heads == 0 # 确保可以平均分配维度 self.d_model = d_model # 保存特征维度 self.num_heads = num_heads # 保存注意力头数 self.head_dim = d_model // num_heads # 计算单头维度 self.q_proj = nn.Linear(d_model, d_model) # 定义查询投影层 self.k_proj = nn.Linear(d_model, d_model) # 定义键投影层 self.v_proj = nn.Linear(d_model, d_model) # 定义值投影层 self.o_proj = nn.Linear(d_model, d_model) # 定义输出投影层 def forward(self, query: torch.Tensor, context: torch.Tensor) -\u0026gt; torch.Tensor: # 执行交叉注意力 batch_size, tgt_len, _ = query.shape # 获取目标序列形状 src_len = context.shape[1] # 获取源序列长度 q = self.q_proj(query).view(batch_size, tgt_len, self.num_heads, self.head_dim).transpose(1, 2) # 构建多头查询 k = self.k_proj(context).view(batch_size, src_len, self.num_heads, self.head_dim).transpose(1, 2) # 构建多头键 v = self.v_proj(context).view(batch_size, src_len, self.num_heads, self.head_dim).transpose(1, 2) # 构建多头值 scores = torch.matmul(q, k.transpose(-2, -1)) / (self.head_dim ** 0.5) # 计算缩放注意力得分 weights = torch.softmax(scores, dim=-1) # 对源位置求权重 context_out = torch.matmul(weights, v) # 根据权重汇总值向量 context_out = context_out.transpose(1, 2).contiguous().view(batch_size, tgt_len, self.d_model) # 合并各头结果 return self.o_proj(context_out) # 线性变换输出最终表示 Click to expand and view more ","title":"Transformer"}],"tags":[{"link":"/tags/ai/","name":"AI","slug":"AI"},{"link":"/tags/nlp/","name":"NLP","slug":"NLP"},{"link":"/tags/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/","name":"基础知识","slug":"基础知识"},{"link":"/tags/%E5%A4%A7%E6%A8%A1%E5%9E%8B/","name":"大模型","slug":"大模型"},{"link":"/tags/%E5%AE%9E%E8%B7%B5/","name":"实践","slug":"实践"},{"link":"/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/","name":"强化学习","slug":"强化学习"},{"link":"/tags/%E6%80%9D%E8%80%83/","name":"思考","slug":"思考"},{"link":"/tags/%E7%94%9F%E6%88%90%E5%BC%8F%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/","name":"生成式推荐系统","slug":"生成式推荐系统"},{"link":"/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/","name":"论文阅读","slug":"论文阅读"}]}